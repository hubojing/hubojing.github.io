<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>GPT on 靖待</title>
        <link>https://hubojing.github.io/tags/gpt/</link>
        <description>Recent content in GPT on 靖待</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>靖待</copyright>
        <lastBuildDate>Tue, 14 Mar 2023 21:05:47 +0000</lastBuildDate><atom:link href="https://hubojing.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>InstructGPT笔记</title>
        <link>https://hubojing.github.io/kte2ngfx/</link>
        <pubDate>Tue, 14 Mar 2023 21:05:47 +0000</pubDate>
        
        <guid>https://hubojing.github.io/kte2ngfx/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;https://hubojing.github.io/images/假装有图片.jpg&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;简记。&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;论文概况&#34;&gt;论文概况
&lt;/h1&gt;&lt;p&gt;Training language models to follow instructions with human feedback&lt;/p&gt;
&lt;p&gt;根据人类反馈指示来训练语言模型&lt;/p&gt;
&lt;p&gt;OpenAI 2022&lt;/p&gt;
&lt;h1 id=&#34;核心&#34;&gt;核心
&lt;/h1&gt;&lt;p&gt;该系统包含主要三个步骤实现：&lt;/p&gt;
&lt;p&gt;1、使用一组广泛分布的互联网数据对GPT-3模型进行预训练。然后，针对典型的一组human prompts，让laber写下正确的答案并用这组12,725的监督数据对模型进行精调；&lt;/p&gt;
&lt;p&gt;2、随机选择一组human prompts，并用模型对每个prompt产生多个输出的答案。让labeler对这些回答进行排序，并根据排序训练一个奖励模型 （reward model）。这组用来训练reward model的数据包含有33,207个prompts以及在不同回答组合下产生的10倍于此的答案；&lt;/p&gt;
&lt;p&gt;3、再次随机采样human prompts，并基于PPO的强化学习算法（Proximal Policy Optimization Algorithm）对监督训练后精调过的模型进行再次fine-tune。每个采样的prompt输入PPO模型，并用reward model给出的奖励信号用31,144个prompts对模型进行训练。&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
