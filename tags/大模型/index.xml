<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>大模型 on 靖待</title>
        <link>https://hubojing.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
        <description>Recent content in 大模型 on 靖待</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>靖待</copyright>
        <lastBuildDate>Wed, 24 Apr 2024 00:15:21 +0000</lastBuildDate><atom:link href="https://hubojing.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>【大模型系列】指令微调</title>
        <link>https://hubojing.github.io/na527pxe/</link>
        <pubDate>Wed, 24 Apr 2024 00:15:21 +0000</pubDate>
        
        <guid>https://hubojing.github.io/na527pxe/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;\images\大模型.jpg&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;大模型系列之指令微调笔记。&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;概述&#34;&gt;概述
&lt;/h1&gt;&lt;p&gt;　　指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，22年谷歌ICLR论文中提出这个概念。在其它文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）或多任务提示训练（Multitask Prompted Training）。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。经过 NLP 指令数据微调后，大语言模型可以学习到指令遵循（Instruction Following）的能力，进而能够解决其他未见过的 NLP 任务。&lt;/p&gt;
&lt;h1 id=&#34;指令数据构建&#34;&gt;指令数据构建
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;基于现有NLP任务数据集构建&lt;/li&gt;
&lt;li&gt;基于日常对话数据构建&lt;/li&gt;
&lt;li&gt;基于合成数据构建
　　以下是几种合成数据构建的方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-instruct&#34;&gt;Self-Instruct
&lt;/h2&gt;&lt;p&gt;　　代表性工作Self-Instruct：借助大语言模型（例如 ChatGPT）所具备的数据合成能力，通过迭代的方法高效地生成大量的指令微调数据。作为初始任务池，该方法首先构建了 175 条高质量且多样的指令数据，之后经由两个主要步骤生成指令微调数据。
　　（1）指令数据生成：从任务池中随机选取少量指令数据作为示例，并针对 ChatGPT 设计精细指令来提示模型生成新的微调数据。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;你被要求提供 10 个多样化的任务指令。这些任务指令将被提供给 GPT 模型。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;以下是你提供指令需要满足的要求：
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1. 尽量不要在每个指令中重复动词，要最大化指令的多样性。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2. 使用指令的语气也应该多样化。例如，将问题与祈使句结合起来。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;……（省略后续要求）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;下面是 10 个任务指令的列表：
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 指令：将 85 华氏度转换为摄氏度。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 输出：85 华氏度等于 29.44 摄氏度。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 指令：是否有科学无法解释的事情？
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 输出：有很多科学无法解释的事情，比如生命的起源、意识的存在……
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;……（省略上下文示例）
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;　　（2）过滤与后处理：剔除低质量或者重复的生成实例，从而保证指令数据的多样性与有效性。常见的过滤方法包括：去除与任务池中指令相似度过高的指令、语言模型难以生成回复的指令、过长或过短的指令以及输入或输出存在重复的实例。&lt;/p&gt;
&lt;h2 id=&#34;evol-instruct&#34;&gt;Evol-Instruct
&lt;/h2&gt;&lt;p&gt;　　Self-Instruct 生成的实例可能过于简单或缺乏多样性，提出一种改进的指令数据合成方法 Evol-Instruct。该方法通过基于深度和广度的演化来提高实例的复杂性和多样性。
　　（1）指令演化：分为深度演化和广度演化。深度演化通过五种特定类型的提示（添加约束、深化、具体化、增加推理步骤以及使输入复杂化）使得指令变得更加复杂与困难；而广度演化旨在扩充指令主题范围、指令涉及的能力范围以及整体数据集的多样性。
　　深度演化（添加约束）指令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;我希望您充当指令重写器。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您的目标是将给定的提示重写为更复杂的版本，使得著名的 AI 系统（例如 ChatGPT 和 GPT-4）更难处理。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;但重写的提示必须是合理的，且必须是人类能够理解和响应的。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您的重写不能省略 # 给定提示 # 中表格和代码等非文本部分。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您应该使用以下方法使给定的提示复杂化：
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;请在 # 给定提示 # 中添加一项约束或要求。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;你应该尽量不要让 # 重写提示 # 变得冗长，# 重写提示 # 只能在 # 给定提示 # 中
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;添加 10 到 20 个单词。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 重写提示 # 中不允许出现“# 给定提示 #”、“# 重写提示 #”字段。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 给定提示 #: {需要重写的指令}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 重写提示 #
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;　　广度演化指令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;我希望你充当指令创造器。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您的目标是从 # 给定提示 # 中汲取灵感来创建全新的提示。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;此新提示应与 # 给定提示 # 属于同一领域，但更为少见。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 # 的长度和复杂性应与 # 给定提示 # 类似。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 # 必须合理，并且必须能够被人类理解和响应。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 # 中不允许出现“# 给定提示 #”、“# 创造提示 #”字段。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 给定提示 #: {需要重写的指令}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 #:
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;　　（2）数据后处理：主要使用了如下的规则进行处理：使用 ChatGPT 比较演化前后的指令，移除 ChatGPT 认为差异很小的指令；移除大模型难以响应的指令，如响应中包含“sorry”或响应长度过短；移除仅包含标点符号和连词的指令或回复。&lt;/p&gt;
&lt;p&gt;　　其它方法：
　　Self-Align设计了多种基于人类对齐原则的合成数据过滤技术，该方法通过上下文提示让 ChatGPT 能够筛选出高质量的实例数据来训练新的模型，并进一步让新训练的模型产生更多与人类对齐的指令微调数据。
　　指令回译技术：直接使用现有的文本（例如维基网页数据）作为输出，然后利用上下文学习来逆向合成相应的输入指令。由于输出内容都是人工撰写的，该方法能够让模型学习生成准确且流畅的文本。&lt;/p&gt;
&lt;h2 id=&#34;指令数据构建的提升方法&#34;&gt;指令数据构建的提升方法
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;指令格式设计
　　训练过程中同时使用了带示例的指令数据（即少样本）和不带示例的指令数据（即零样本）、在指令微调数据集中引入思维链数据、在指令微调时同时引入了包含 CoT 和不包含 CoT 的实例，使用混合指令数据
指令数据并不是包含信息越多越好，添加某些看似有用的信息（例如需要避免的事项、原因或建议）到指令中，可能不会带来明显的效果提升，甚至可能产生不利影响&lt;/li&gt;
&lt;li&gt;扩展指令数量
　　对于NLP任务，FLAN-T5通过逐步将指令数量扩展至 0.18M、5.55M、7.2M 以及17.26M，模型性能呈持续上升的趋势。然而，当指令数量达到 7.2M后，模型性能的提升变得非常缓慢。FLAN-T5 中采用的指令可能仅对传统 NLP 任务适用，而对于日常对话这一至关重要的能力并未带来明显的提升。
　　对于一个较好的预训练基座模型，越来越多的研究工作表明一定数
量的高质量指令就可以激活大语言模型的对话能力。Alpaca [42] 使用了 52K 条合成数据来指令微调 LLaMA (7B)，在 179 条日常对话数据的评测中到达了接近text-davinci-003 的效果。进一步，LIMA [203] 仅使用了一千条人工标注的高质量指令数据来微调 LLaMA (65B)，就在 300 条日常对话测试中取得了较好的模型表现。
但是，仅依靠少量指令数据难以兼顾 NLP 任务和对话场景任务。&lt;/li&gt;
&lt;li&gt;指令重写与筛选
　　YuLan-Chat-3提出了“主题多样性”增强方法，预先从知乎收集了 293 种常见主题标签（例如，“教育”，“体育”），然后随机选择一种并使用 ChatGPT 对指令进行重写来适配到相应的主题（例如使用提示：“请帮我把以下指令重写为教育主题”），最后进行质量筛选来获取高质量的多样性指令数据。它提出了“平衡指令难度”策略，其用大模型的困
惑度分数来估算指令数据的难度水平，删除过于简单或过于困难的指令数据，从而缓解大模型训练不稳定或者过拟合的现象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;指令微调的训练策略&#34;&gt;指令微调的训练策略
&lt;/h1&gt;&lt;p&gt;　　在训练方式上，指令微调与预训练较为相似，很多设置包括数据组织形式都可以预训练阶段所采用的技术。指令微调中的优化器设置（AdamW 或 Adafactor）、稳定训练技巧（权重衰减
和梯度裁剪）和训练技术（3D 并行、ZeRO 和混合精度训练）都与预训练保持阶段一致，可以完全沿用。以下是指令微调与预训练的不同之处。&lt;/p&gt;
&lt;h2 id=&#34;优化设置&#34;&gt;优化设置
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目标函数。预训练阶段通常采用语言建模损失（详见第 6.1.1 节），优化模型在每一个词元上的损失。而指令微调可以被视为一个有监督的训练过程，通常采用的目标函数为序列到序列损失，仅在输出部分计算损失，而不计算输入部分的损失。&lt;/li&gt;
&lt;li&gt;批次大小和学习率。指令微调阶段通常只需要使用较小的批次大小和学习率对模型进行小幅度的调整。&lt;/li&gt;
&lt;li&gt;　　多轮对话数据的高效训练。对于一个多轮对话数据，通常的训练算法是将其拆分成多个不同的对话数据进行单独训练。为了提升训练效率，可以采用特殊的掩码机制来实现多轮对话数据的高效训练。在因果解码器架构中，由于输入输出没有明显的分界，可以将所有一个对话的多轮内容一次性输入模型，通过设计损失掩码来实现仅针对每轮对话的模型输出部分进行损失计算，从而显著减少重复前缀计算的开销。多轮对话涉及多次用户输入和模型的输出，但是训练中仅需要在模型的输出上计算损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据组织策略&#34;&gt;数据组织策略
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;平衡数据分布
　　混合使用现有的多个指令数据集，以此来实现模型能力的综合改进。最常用方法：样本比例混合策略，即把所有数据集进行合并，然后从混合数据集中等概率采样每个实例。研究者建议混合使用 NLP 任务数据（如 FLAN v2）、对话数据（如ShareGPT）和合成数据（如 GPT4-Alpaca），来进行大模型的指令微调。&lt;/li&gt;
&lt;li&gt;多阶段指令数据微调
YuLan-Chat-3 采用了“多阶段指令微调”策略：首先使用大规模 NLP 任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调。为了避免能力遗忘问题，可以在第二阶段中添加一些 NLP 指令数据。这种多阶段的微调策略也可以应用于其他训练设置。例如，对于不同的微调阶段，训练中可以逐渐增加指令的难度和复杂性，从而逐渐提高大模型遵循复杂指令的能力。&lt;/li&gt;
&lt;li&gt;结合预训练数据与指令微调数据
为了使得微调过程更加有效和稳定，可以在指令微调期间引入了预训练数据和任务，这可以看作是对于指令微调的正则化。OPT-IML在指令微调阶段引入了 5% 的预训练数据，在分类和生成任务上都能取得增益；然而，进一步增加预训练数据会对生成任务有利，但有可能损失分类任务的表现。
另一方面，将指令数据引入到预训练阶段也成为了一种常见的训练技术。通过提前使用指令微调数据，有可能会帮助模型在预训练阶段更好地感知下游任务，从而更为针对性地从预训练数据中学习知识与能力。如，GLM-130B的预训练过程由 95% 的传统自监督预训练和 5% 的指令微调任务混合组成。MiniCPM提出在预训练阶段和指令微调阶段之间添加一个“退火阶段”，该阶段混合使用高质量的预训练数据和指令微调数据，其实验结果表明该策略优于先预训练再指令微调的两阶段策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参数高效的模型微调&#34;&gt;参数高效的模型微调
&lt;/h2&gt;&lt;p&gt;　　参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调（Lightweight Fine-tuning）。&lt;/p&gt;
&lt;h3 id=&#34;lora&#34;&gt;LoRA
&lt;/h3&gt;&lt;p&gt;低秩适配（Low-Rank Adaptation, LoRA）
模型在针对特定任务进行适配时，参数矩阵往往是过参数化（Over-parametrized）的，其存在一个较低的内在秩。
假设 LoRA 需要训练的参数量为$P_{LoRA}$，模型原始参数为P。$P_{LoRA}$ ≪ P，LoRA 微调需要的显存大小从全量微调的16P大幅减少为$2P+16P_{LoRA}$。&lt;/p&gt;
&lt;h3 id=&#34;lora变种&#34;&gt;LoRA变种
&lt;/h3&gt;&lt;p&gt;在原始的 LoRA 实现中，每个低秩矩阵的低秩参数 𝑅 都被设置为固定且相同的数值，并且在训练过程中无法进行调整，这种设定忽略了不同的秩在微调任务中可能产生的差异化影响。因此，通过这种方式训练得到的低秩矩阵往往并非最优解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AdaLoRA。它讨论了如何更好地进行秩的设置。它引入了一种动态低秩适应技术，在训练过程中动态调整每个参数矩阵需要训练的秩同时控制训练的参数总量。具体来说，模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的参数矩阵被赋予比较高的秩，进而能够更好地学习到有助于任务的信息。相对而言，不太重要的参数矩阵被赋予比较低的秩，来防止过拟合并节省计算资源。尽管 LoRA 能够有效地节省显存，但对于参数规模达到上百亿级别的模型而言，其微调所需的成本仍然相当高昂。&lt;/li&gt;
&lt;li&gt;QLoRA。它将原始的参数矩阵量化为 4 比特，而低秩参数部分仍使用 16 比特进行训练，在保持微调效果的同时进一步节省了显存开销。根据上一小节的分析，对于给定参数量为 𝑃 的模型，QLoRA 微调所需要的显存由 LoRA 微调所需要的2𝑃 进一步下降为 0.5𝑃。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;其它高效微调方法&#34;&gt;其它高效微调方法
&lt;/h3&gt;&lt;p&gt;适配器微调、前缀微调、提示微调。这三种方法在预训练语言模型中被广泛使用，但是在大语言模型中的应用相对较少。&lt;/p&gt;
&lt;h4 id=&#34;适配器微调&#34;&gt;适配器微调
&lt;/h4&gt;&lt;p&gt;适配器微调（Adapter Tuning）在 Transformer 模型中引入了小型神经网络模块（称为适配器）。为了实现适配器模块，研究者提出使用瓶颈网络架构：它首先将原始的特征向量压缩到较低维度，然后使用激活函数进行非线性变换，最后再将其恢复到原始维度。通常来说，适配器模块将会被集成到Transformer 架构的每一层中，使用串行的方式分别插入在多头注意力层和前馈网络层之后、层归一化之前。在微调过程中，适配器模块将根据特定的任务目标进行优化，而原始的语言模型参数在这个过程中保持不变。通过这种方式，可以在微调过程中有效减少需要训练参数的数量。
&lt;img src=&#34;https://img-blog.csdnimg.cn/direct/7bc64be5e98c4908818be832041c1bd2.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;适配器微调&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;前缀微调&#34;&gt;前缀微调
&lt;/h3&gt;&lt;p&gt;前缀微调（Prefix Tuning）在语言模型的每个多头注意力层中都添加了一组前缀参数。这些前缀参数组成了一个可训练的连续矩阵，可以视为若干虚拟词元的嵌入向量，它们会根据特定任务进行学习。在前缀微调过程中，整个模型中只有前缀参数会被训练，因此可以实现参数高效的模型优化。
&lt;img src=&#34;https://img-blog.csdnimg.cn/direct/3fa7cafb4f2641ae8843580df3ba4ecf.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;前缀微调&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;提示微调&#34;&gt;提示微调
&lt;/h3&gt;&lt;p&gt;提示微调仅在输入嵌入层中加入可训练的提示向量。在离散提示方法的基础上（可查阅系列文章&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/hubojing/article/details/138040305&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;提示学习&lt;/a&gt;），提示微调首先在输入文本端插入一组连续嵌入数值的提示词元，这些提示词元可以以自由形式或前缀形式来增强输入文本，用于解决特定的下游任务。在具体实现中，只需要将可学习的特定任务提示向量与输入文本向量结合起来一起输入到语言模型中。
&lt;img src=&#34;https://img-blog.csdnimg.cn/direct/08584e7b84004a18a9e768059cdf238c.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;提示微调&#34;
	
	
&gt;
P-tuning提出了使用自由形式来组合输入文本和提示向量，通过双向 LSTM来学习软提示词元的表示，它可以同时适用于自然语言理解和生成任务。
Prompt Tuning以前缀形式添加提示，直接在输入前拼接连续型向量。
在提示微调的训练过程中，只有提示的嵌入向量会根据特定任务进行监督学习，然而由于只在输入层中包含了极少量的可训练参数，有研究工作表明该方法的性能高度依赖底层语言模型的能力。&lt;/p&gt;</description>
        </item>
        <item>
        <title>【大模型系列】提示学习</title>
        <link>https://hubojing.github.io/s3h6op3z/</link>
        <pubDate>Sun, 21 Apr 2024 12:39:43 +0000</pubDate>
        
        <guid>https://hubojing.github.io/s3h6op3z/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;\images\大模型.jpg&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;大模型系列笔记之提示工程（基础、上下文学习、思维链）。&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;基础提示&#34;&gt;基础提示
&lt;/h1&gt;&lt;h2 id=&#34;人工设计提示&#34;&gt;人工设计提示
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;任务描述&lt;/li&gt;
&lt;li&gt;输入数据&lt;/li&gt;
&lt;li&gt;上下文信息&lt;/li&gt;
&lt;li&gt;提示策略
“你是这个任务的专家”
“让我们一步一步地思考”
“通过依次执行以下任务形成一段连贯的叙述：1. &amp;hellip;；2. &amp;hellip;；3. &amp;hellip;”
对话式大模型，可以将提示拆分为多个子任务提示，以多轮对话的方法逐步输入给大模型。
提供少样本示例
使用特殊符号进行分割（###、三引号、XML标签等）
英文指令理解更好&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;自动提示优化&#34;&gt;自动提示优化
&lt;/h2&gt;&lt;h3 id=&#34;离散提示优化&#34;&gt;离散提示优化
&lt;/h3&gt;&lt;p&gt;离散提示通常是由一系列自然语言词元组成的完整句子表达（如“请根据提供的检索信息回答下列问题”）。在离散的词元空间中进行组合搜索，时间复杂度高且搜索结果可能不是最优。因此有以下的优化方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于梯度的方法
通过梯度更新技术以最大化模型的似然分数来优化离散提示的搜索过程。将提示词初始化为一系列“[MASK]”标记，然后迭代地将提示中的词元替换为词典中的其它词元，通过词元替换产生的对数似然变化来近似估计梯度，进而为提示的每个位置贪心搜索出最佳的词元。由于对提示的每个位置都进行所有候选词元的替换和梯度评估，因此模型需要进行多次前向和后向计算，搜索过程效率较低。为此，可将离散词元转化为连续嵌入表示（软词元），使用梯度直接对连续嵌入参数进行优化，最后将每个连续嵌入映射为词典中最邻近的离散词元。&lt;/li&gt;
&lt;li&gt;基于强化学习的方法
将预训练语言模型作为强化学习中的策略网络并依次生成提示中的词元。在提示生成结束之后，策略网络可以获得任务特定的奖励信号，该奖励信号可通过强化学习算法用于策略网络参数的训练。可设计不同类型的奖励信号，如真实便签与基于提示的预测标签是否一致、生成文本与给定条件的匹配程度。在测试阶段，基于训练好的策略网络，可采用贪心搜索策略来生成任务提示中的每个词元。&lt;/li&gt;
&lt;li&gt;基于编辑的方法
特别适用于无法直接访问模型内部状态（如梯度）的情况，比如只能通过API调用的模型。这种方法需事先定义好编辑操作，然后迭代地修改提示，直到最大迭代轮次或者模型最佳性能。常用的提示编辑操作：修改任务描述、添加或删除上下文任务示例、调整输入到输出的标签映射器（二分类用‘postive/negtive’、‘正/负’）。&lt;/li&gt;
&lt;li&gt;基于大语言模型的方法
该方法首先利用提示生成模型（用于生成指示指令的大语言模型）基于少量上下文示例生成一批候选的任务指令。随后，使用目标模型（用于下游测试的大语言模型）对这些候选指令在目标任务上的表现逐一评估。评估过程可采用模型困惑度或任务准确率作为衡量指令质量的指标。上述过程可通过基于蒙特卡洛搜索的多轮优化策略进行扩展。在每轮迭代中，根据模型表现对候选指令进行筛选得到高评分指令，并利用大语言模型生成与高评分指令相似的新指令，从而扩展候选指令集。迭代完成后，选择模型表现最佳的候选指令作为最终使用的提示。但是这种方法可能在提示搜索过程中陷入局部最优或者产生效果震荡，无法生成更好的提示。为解决该问题，可将所有改进的历史提示及其分数纳入提示优化过程，以指导大语言模型逐步生成更好的新提示。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;连续提示优化&#34;&gt;连续提示优化
&lt;/h3&gt;&lt;p&gt;由一组连续空间中的嵌入向量组成，可根据下游任务的损失直接通过梯度更新进行优化。已有连续提示优化的工作主要是基于预训练语言模型开展，由于参数量巨大，受到的关注有限。已有研究通常依赖于有监督学习方法，数据稀缺时还可采用迁移学习来缓解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习方法
将连续提示向量视为可训练的模型参数，基于下游任务数据，通过最小化交叉熵损失来优化连续提示。Prefix-tuning在语言模型每个Transformer层预置一串前缀（即一组可训练的连续向量），而Prompt-tuning只会在输入层加入可训练的提示向量。通过固定语言模型的大规模参数而只微调这些连续的提示向量，可以有效节省训练所需要的参数量。然而这些提示优化方法通常与输入无关，缺乏对于输入语义的充分考虑。&lt;/li&gt;
&lt;li&gt;迁移学习方法
为若干个具有代表性的源任务学习一个所有任务共享的连续提示，然后使用该提示初始化目标任务的提示，但它在解决目标任务的所有实例时都使用了相同提示，未必适合所有的任务实例。为此，可以为每个源任务独自学习任务特定的连续提示（而不是所有源任务共享），在解决目标任务的实例时，可以采用注意力机制等方式学习目标实例与每个源任务提示的相关性权重系数，对若干个源任务的提示向量进行加权组合，将组合后的新提示（为连续向量形式）用于帮助模型解决当前任务实例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;上下文学习&#34;&gt;上下文学习
&lt;/h1&gt;&lt;p&gt;GPT3提出In-context learning（ICL），由任务描述和（或）示例所组成的自然语言文本作为提示。
首先，通过自然语言描述任务，并从任务数据集中选择一些样本作为示例。其次，根据特定的模板，将这些示例按照特定顺序组合成提示内容。最后，将测试样本添加到提示后面，整体输入到大语言模型以生成输出。基于任务描述以及示例信息，大语言模型无需显式的梯度更新即可识别和执行新的任务。&lt;/p&gt;
&lt;h2 id=&#34;示例选择&#34;&gt;示例选择
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;基于相关度排序的方法
KNN相似度检测算法，选出与目标任务实例相关的示例。具体来说，可以使用文本嵌入模型（如 BERT）将所有候选样本映射到低维嵌入空间中，然后根据这些候选样本与测试样本的嵌入语义相似度进行排序，并选择出最相关的 𝑘 个示例，最后将筛选出的示例作为上下文学习的示例集合。&lt;/li&gt;
&lt;li&gt;基于集合多样性的方法
经典启发式MMR（Maximum Margin Ranking）算法、基于行列式点过程的DPP算法（Determinantal Point Process）&lt;/li&gt;
&lt;li&gt;基于大语言模型的方法
将大语言模型作为评分器对候选样本进行评估，进而选择出优质的示例。一种最直接的评估方法是通过计算在加入当前示例后大语言模型性能的增益来评估示例的有效性，以此筛选出有效的示例。但是，这种方法需要大语言模型进行重复多次计算，才能选择出最优的示例集合。为了减少大语言模型评估的开销，还可以根据大语言模型的评分结果选择出少量的正负示例用于训练一个分类器，该分类器通过正负示例能够学习到如何准确地区分和筛选出高质量示例，从而更准确地来指导后续的示例选择过程。
总体来说，在上下文学习中进行示例选择时应确保所选示例包含丰富的任务信息且与测试样本保持高度的相关性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;示例格式&#34;&gt;示例格式
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;人工标注的格式&lt;/li&gt;
&lt;li&gt;自动生成的格式
首先人工标注一部分的示例模板作为种子集合加入到大语言模型的输入中。然后，利用大语言模型强大的少样本学习能力，指导其为新任务生成相应的示例模版。最后，对这些生成的示例模版进行筛选与后处理，使之符合任务要求。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;示例顺序&#34;&gt;示例顺序
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;产生候选示例顺序
大语言模型在做出预测时，倾向于依赖于提示末端的信息。常用的方式是根据示例与测试样本之间的语义相似度进行排序，然后将与测试样例相似度更高的示例放在更靠近测试样本的位置。&lt;/li&gt;
&lt;li&gt;评估示例顺序质量
在测试集样本可获得的情况下，可以直接测试大语言模型基于该示例顺序的任务性能，以此作为当前示例顺序的评分。无法获得测试样本时，需要人工创建独立的验证集进行示例顺序的评估。另一种不依赖测试数据的评估方法是采用模型对于预测结果的不确定性作为评估指标。具体来说，可以计算基于该示例顺序大语言模型预测分布的熵值，选择熵值较低的示例顺序作为较为有效的顺序。熵值越低，意味着模型预测分布越不均匀，则模型预测的置信度更高。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;思维链提示&#34;&gt;思维链提示
&lt;/h1&gt;&lt;p&gt;思维链提示作为上下文学习的一种扩展形式，将原始的&amp;lt;输入，输出&amp;gt;映射关系转换为&amp;lt;输入，思维链，输出&amp;gt;这一三元组形式。
“Let’s think step by step.”
“Take a deep breath and work on this problem step-by-step.”&lt;/p&gt;
&lt;h2 id=&#34;思维链示例设计&#34;&gt;思维链示例设计
&lt;/h2&gt;&lt;p&gt;从输入侧对思维链示例进行增强。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复杂化的思维链&lt;/li&gt;
&lt;li&gt;推理步骤多、问题长&lt;/li&gt;
&lt;li&gt;多样化的思维链
首先利用聚类算法（例如 𝑘-means 聚类）将训练集中的问题划分为 𝑘 个簇（𝑘 为所需的示例数量），簇内部的问题比较相似，而不同簇的问题差别较大。然后，预定义一系列启发式规则，从每个簇中选择距离质心最近且满足规则的问题作为该簇的代表性问题，将该问题输入给大语言模型并生成对应的思维链和答案作为示例。由于每个问题来自于不同的簇，从而保证了示例的多样性。实验发现，虽然大模型生成的思维链示例可能存在错误，但是当选择更加多样化的示例时，思维链示例中的错误对模型性能的影响会显著降低。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;思维链生成方法&#34;&gt;思维链生成方法
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;基于采样的方法
单一思维链推理容易出错，可采样多条推理路径来缓解。代表性方法：self-consistency。首先使用大语言模型生成多个推理路径和对应的答案，然后对于这些候选答案进行集成并获得最终输出。具体的集成方法可选择各条推理路径所得到答案中出现频率最高的那个答案作为最终输出，也可对所有答案进行某种形式的加权。
但问题太简单用思维链反而效果不好。例如，对于句子的情感分类任务，由于问题过于简单，加入思维链提示之后反而会使模型过度思考，从而得出错误的答案。&lt;/li&gt;
&lt;li&gt;基于验证的方法
思维链提示所具有的顺序推理本质可能导致推理过程中出现错误传递或累积的现象。为了解决这一问题，可以使用专门训练的验证器或大语言模型自身来验证所生成的推理步骤的准确性。
例：DIVERSE 方法
针对整个推理路径的验证器通过如下方法训练得到：首先选择一个包含大量问题答案对的数据集，然后将问题输入给大语言模型，通过思维链提示的方法使其生成推理路径和最终答案。如果模型生成的答案和数据集标注的答案一致，则判为正例，否则判为负例。最后使用构造的&amp;lt;问题，推理链，答案&amp;gt;数据训练一个二分类器，从而可以对任意一个推理路径进行打分。训练针对中间步骤的验证器也可以采用类似的方案。然而，与整体推理路径的数据标注相比，构造面向中间步骤的正负例数据更加困难。这里可以采取一个简化处理：对于每一个训练集中的问题，我们采样多次得到多个推理路径，对于得出正确答案的推理路径，中间的每一个步骤我们都认为是正确的，作为正例；对于得出错误答案的推理路径，如果其中某个步骤和正例的推理路径相一致，也认为是正例，否则作为负例。通过这样构造出来的数据，用同样的方法训练一个二分类器，从而可以对模型输出的中间步骤进行打分。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;拓展的推理结构&#34;&gt;拓展的推理结构
&lt;/h2&gt;&lt;p&gt;链式推理结构在处理较为复杂的任务时（例如需要进行前瞻和回溯探索）仍然存在一定的局限性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;树形推理结构
代表性工作：思维树（Tree of Thought, ToT）
它和思维链的区别在于：思维链从一个节点出发，只能生成一个节点，而思维树则可以生成多个节点。当某一个思考步骤无法得到正确答案时，可以回溯到它的父节点，选择另一个子节点继续推理。&lt;/li&gt;
&lt;li&gt;图形推理结构
代表性工作：思维图（Graph of Thought, GoT）
思维图和思维树的区别在于，思维树的子节点只能进行前向搜索和回溯，而思维图的子节点可以和其他子节点进行汇聚，得到新的中间步骤，然后进行下一步的推理。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>大模型相关论文笔记</title>
        <link>https://hubojing.github.io/9ocgomtj/</link>
        <pubDate>Wed, 22 Nov 2023 22:20:55 +0000</pubDate>
        
        <guid>https://hubojing.github.io/9ocgomtj/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;\images\paper.png&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
 　　
　　**大模型相关论文阅读笔记。 **
　　**倒序排列论文，最新阅读的在最上面。**
　　**2024年1月26日更新**
 &lt;/div&gt;
&lt;h1 id=&#34;retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks&#34;&gt;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
&lt;/h1&gt;&lt;p&gt;用于知识密集型NLP任务的检索增强生成
Facebook 2020
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.11401v4.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;br&gt;
（论文代码链接已失效，以上是最新链接）&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;大模型有幻觉问题（hallucinations），检索增强生成(retrieval-augmented generation, RAG)可以解决它。&lt;/p&gt;
&lt;h2 id=&#34;方法&#34;&gt;方法
&lt;/h2&gt;&lt;p&gt;输入为x，外部检索资源为z，生成目标序列y。
模型有两块：一个检索器$p_\eta(z|x)$，$\eta$为参数，给定一个查询q，根据文本返回top-K个分布；一个生成器$p_\theta(y_i|x,z,y_{1:i-1})$，参数为$\theta$，它基于过去i-1个tokens $y_{1:i-1}$、原始输入x和检索器信息z，产生一个当前的token。
为了端到端的训练检索器和生成器，我们将检索文档作为一个隐变量。我们提出了两个模型，他们以不同的方式边缘化隐变量，从而在文本上产生分布。在我们的方法里，第一步，RAG-Sequence，这个模型使用相同的文本预测每一个目标token；第二步，RAG-Token，基于不同的文件预测每一个目标token。&lt;/p&gt;
&lt;h3 id=&#34;模型&#34;&gt;模型
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;RAG-Sequence模型
$$p_{RAG-Sequence}(y|x)≈\sum_{z∈top-k(p(·|x))}p_\eta(z|x)p_\theta(y|x,z)=\sum_{z∈top-k(p(·|x))}p_\eta(z|x)\prod^N_ip_\theta(y_i|x,z,y_{1:i-1})$$&lt;/li&gt;
&lt;li&gt;RAG-Token模型
$$p_{RAG-Token}(y|x)≈\prod^N_i\sum_{z∈top-k(p(·|x))}p_\eta(z|x)p_\theta(y_i|x,z,y_{1:i-1})$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;检索器dpr&#34;&gt;检索器：DPR
&lt;/h3&gt;$$p_\eta(z|x)∝exp(d(z)^Tq(x)) ~~~ d(z)=BERT_d(z), q(x)=BERT_q(x)$$&lt;p&gt;
其中，d(z)是使用BERT编码得到的密集表示，q(x)是问题通过BERT编码得到的表示。计算top-k($p_\eta(·|x)$)是一个MIPS(Maximum Inner Product Search)问题，可以在亚线性时间内解决。我们使用一个基于DPR的预训练双向编码器来初始化我们的检索器并建立索引，将其视为非参数记忆(non-parametric memory)。&lt;/p&gt;
&lt;h3 id=&#34;生成器bart&#34;&gt;生成器：BART
&lt;/h3&gt;&lt;p&gt;生成器可以使用任何编码器-解码器。我们使用的是BART-large。&lt;/p&gt;
&lt;h3 id=&#34;训练&#34;&gt;训练
&lt;/h3&gt;&lt;p&gt;求最小似然log-likelihood、Adam&lt;/p&gt;
&lt;h3 id=&#34;解码&#34;&gt;解码
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;RAG-Toke&lt;br&gt;
$$p^{&#39;}_\theta(y_i|x,y_{1:i-1})=\sum_{z∈top-k(p(·|x))}p_\eta(z_i|x)p_\theta(y_i|x,z_i,y_{1:i-1})$$
将$p^{&amp;rsquo;}&lt;em&gt;\theta(y_i|x,y&lt;/em&gt;{1:i-1})$送入标准beam解码器中。&lt;/li&gt;
&lt;li&gt;RAG-Sequence&lt;br&gt;
Thorough Decoding
Fast Decoding&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness&#34;&gt;FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness
&lt;/h1&gt;&lt;p&gt;FlashAttention: 具有IO感知的快速和有效存储精确注意力
2022年6月24日
&lt;a class=&#34;link&#34; href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/Dao-AILab/flash-attention&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要
&lt;/h2&gt;&lt;p&gt;　　自注意力的时间和空间复杂度在序列长度上是二次关系，近似注意力机制尝试在模型质量和复杂度计算中折中来解决该问题，但是经常不能实现wall-clock加速。本文认为，需要一种规范，可以根据GPU读写水平使注意力IO感知。为此，本文提出FLASHATTENTION，一种IO感知的精确注意力机制，它使用tiling技术来减少GPU HBM（high bandwidth memory）和GPU芯片内SRAM的存储读写次数。FLASHATTENTION相比标准注意力机制要减少这方面开销。&lt;/p&gt;
&lt;h2 id=&#34;引言-1&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cFlashAttention.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;FlashAttention&#34;
	
	
&gt;&lt;br&gt;
　　现代GPU的计算速度比存储速度快，在Transformer里的许多操作都受限于存储接入。现在的公共Python接口，比如PyTorch和Tensorflow对于内存接入没有精细化管理。本文提出FLASHATTENTION，一种计算注意力时减少内存接入操作的新注意力算法。它的目标是减少在HBM中读写的注意力矩阵。这需要
（1）在不访问整体输入的情况下计算softmax reduction；
（2）反向传播时不存大量中间过程的注意力矩阵。
　　本文提出两种方法解决上面的问题。
（1）我们重新构建了注意力计算模块，将输入分块，在输入块中形成多个通道，因此递增地执行softmax reduction（也就是tiling)；
（2）从前向传播到反向传播中快速重新计算片上注意力，我们存储了其中的softmax标准化因素，这比从HBM读取中间注意力矩阵的标准方法要快。
　　在CUDA使用FLASHATTENTION去实现精细化存储控制以及在GPU内核中融合所有的注意力操作。即使因为重计算会增加FLOP（Floating Point Operations），相对于标准注意力而言，我们的算法依然更快、需要更少的内存，在序列长度上是线性的，这是因为HBM的接入大量减少。
　　FLASHATTENTION在HBM上的复杂度是O(N^2d^2M^{-1})，其中d是头head的维度，M是SRAM的规模，标准注意力的复杂度是$Ω(Nd+N^2)$。
　　本文贡献点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更快的模型训练速度&lt;/li&gt;
&lt;li&gt;更高的模型质量&lt;/li&gt;
&lt;li&gt;比现有基线注意力都要快&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景
&lt;/h2&gt;&lt;h3 id=&#34;硬件性能&#34;&gt;硬件性能
&lt;/h3&gt;&lt;p&gt;　　重点描述GPU。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPU存储等级
　　HBM、SRAM&lt;/li&gt;
&lt;li&gt;执行模型
　　GPU有很多线程去执行一个操作（称为核）。每个核从HBM登记加载输入，SRAM计算，再将输出写入HBM。&lt;/li&gt;
&lt;li&gt;性能特点&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;计算密集型Compute-bound&lt;/li&gt;
&lt;li&gt;存储密集型Memory-bound&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;内核融合
　　最常见的加速存储密集操作的就是内核融合。如果多个操作同时应用在相同的输入时，可以从HBM一次性加载输入。编译器会自动融合许多elementwise操作。然而，根据模型训练的上下文，中间过程的值为了反向传播仍然需要写入HBM，这降低了原生内核融合的效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;标准注意力&#34;&gt;标准注意力
&lt;/h4&gt;$$S=QK^T∈\mathbb{R}^{N×N}, P=softmax(S)∈\mathbb{R}^{N×N}, O=PV∈\mathbb{R}^{N×d}$$&lt;p&gt;
softmax按行(row-wise)使用。
　　标准注意力将S和P扔给HBM，这花费了$O(N^2)$存储。一般来说，N&amp;raquo;d，比如GPT2里N=1024，d=64。大部分操作是存储密集型（比如softmax），大量的存储读写造成wall-clock时间变慢。
　　其它操作更加加剧了这个问题，比如加在注意力矩阵的elementwise操作、加在S上的遮罩或者加在P上的dropout。为此，有很多融合几种elementwise操作的方法尝试，比如一些文献用softmax融合遮罩。
&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cFlashAttention-StandardAttention%e4%bc%aa%e4%bb%a3%e7%a0%81.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;标准注意力伪代码&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;flashattention&#34;&gt;FLASHATTENTION
&lt;/h2&gt;&lt;p&gt;　　两种方法：tiling和recomputation
　　主要思路：将输入的Q、K、V分块，将它们从慢的HBM放到快的SRAM中，计算了注意力输出后再返回到各自块里。在每个快的输出相加之前，通过标准化进行缩放，最终得到结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tiling&lt;/li&gt;
&lt;li&gt;Recomputation
　　我们的目标之一是不要存储反向传播中间过程值$O(N^2)$。反向传播需要矩阵$S,P∈\mathbb{R}^{N×N}$来计算Q、K、V的梯度。
&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cFlashAttention%e4%bc%aa%e4%bb%a3%e7%a0%81.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;FlashAttention伪代码&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;lora-low-rank-adaptation-of-large-language-models&#34;&gt;LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
&lt;/h1&gt;&lt;p&gt;　　大模型的低秩适配器&lt;br&gt;
　　微软 2021年&lt;br&gt;
　　Low-Rank Adaptation, LORA&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/microsoft/LoRA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;
　　冻结预训练模型的参数，在Transformer架构每一层注入一个可训练的低秩分解矩阵（rank decomposition matrices），大幅减少了下游任务的训练参数。&lt;br&gt;
　　对比GPT-3 175B Adam微调，LoRA可以减少10000倍训练参数、3倍GPU存储。&lt;br&gt;
　　对比RoBERTa, DeBERTa, GPT-2, GPT-3，训练参数虽少，模型微调质量更好，更高的训练吞吐量，而且不像适配器，没有额外的推理延迟。&lt;/p&gt;
&lt;h2 id=&#34;引言-2&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;　　微调会更新预训练模型的全部参数，下游任务的新模型和原模型参数一样多。许多研究者通过只更新部分参数或学习新任务的额外模块进行迁移，这样可以只保存和加载一小部分任务相关的参数即可，部署时提高了效率。但是现有方法通过延伸模型深度或者减少模型可用的序列长度会导致推理延迟。而且这些策略达不到微调的基线效果，在效率和模型质量上做了折中。&lt;br&gt;
&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cLoRA.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;LoRA&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;问题陈述&#34;&gt;问题陈述
&lt;/h2&gt;$$\underset{\Phi}{max}\sum_{(x,y)∈Z}\sum^{|y|}_{t=1}log(P_{\Phi}(y_t|x,y_{&lt;t}))$$$$\underset{\Theta}{max}\sum_{(x,y)∈Z}\sum^{|y|}_{t=1}log(P_{\Phi}(y_t|x,y&lt;t))$$&lt;p&gt;
　　本文提出了一种低秩表示来编码$△\Phi$。对于GPT-3 175B，$|\Theta|$的训练参数量是$|\Phi_0|$的0.01%。&lt;/p&gt;
&lt;h2 id=&#34;之前方法的缺点&#34;&gt;之前方法的缺点
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;适配器层引入了推理延迟&lt;/li&gt;
&lt;li&gt;直接优化Prompt很难&lt;br&gt;
　　比如prefix tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法
&lt;/h2&gt;&lt;h3 id=&#34;低秩参数更新矩阵&#34;&gt;低秩参数更新矩阵
&lt;/h3&gt;$$h = W_0x+△Wx = W_0x + BAx$$&lt;p&gt;
当遇到不用的下游任务时，只需要替换BA就行，所以没有推理延迟。&lt;/p&gt;
&lt;h3 id=&#34;将lora应用到transformer&#34;&gt;将LoRA应用到Transformer
&lt;/h3&gt;&lt;p&gt;在Transformer架构中，在自注意力模块有四个权重矩阵（$W_q$、$W_k$、$W_v$、$W_o$），在MLP模块有两个。本文将Transformer架构中的$W_q$（或者$W_k$，$W_v$）设为一个$d_{model}×d_{model}$的单矩阵。对于下游任务，只改变注意力权重，冻结MLP模块的。&lt;/p&gt;
&lt;h1 id=&#34;llama-2-open-foundation-and-fine-tuned-chat-models&#34;&gt;Llama 2: Open Foundation and Fine-Tuned Chat Models
&lt;/h1&gt;&lt;p&gt;2023年7月  77页
GenAI, Meta
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2307.09288.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/facebookresearch/llama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;引言-3&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;　　本文发布了两个模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLAMA 2，它是LLAMA 1的升级版本，训练语料新增40%，模型上下文长度翻倍，采用了分组查询注意力。发布了7B, 13B, 70B，34B也训了但没发布&lt;/li&gt;
&lt;li&gt;LLAMA 2-CHAT，它是LLAMA 2用于用户对话的微调版本。发布了7B，13B，70B参数模型
　　提供了&lt;a class=&#34;link&#34; href=&#34;https://ai.meta.com/llama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;用户手册&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;%e2%80%96https://github.com/facebookresearch/llama&#34; &gt;代码样例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;预训练&#34;&gt;预训练
&lt;/h2&gt;&lt;p&gt;　　预训练数据：2 trillion&lt;br&gt;
　　训练，与LLAMA 1相同之处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标准transformer&lt;/li&gt;
&lt;li&gt;RMSNorm预归一化&lt;/li&gt;
&lt;li&gt;SwiGLU激活函数&lt;/li&gt;
&lt;li&gt;旋转位置embedding RoPE
　　不同之处：&lt;/li&gt;
&lt;li&gt;增加上下文长度&lt;/li&gt;
&lt;li&gt;分组查询注意力（GQA）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　优化器：AdamW&lt;br&gt;
　　分词器：与LLAMA 1相同，此表规模32k tokens&lt;/p&gt;
&lt;p&gt;　　它甚至还写了LLAMA 2的碳排放情况&amp;hellip;&lt;/p&gt;
&lt;p&gt;　　评价指标方面，LLAMA 2 70B在MMLU和GSM8K上与GPT-3.5相近，但是在代码基线上有差距，在几乎所有的基线上都比PaLM(540B)强。与GPT-4和PaLM-2-L相比，还有很大差距。&lt;/p&gt;
&lt;h2 id=&#34;微调&#34;&gt;微调
&lt;/h2&gt;&lt;h3 id=&#34;sft监督微调&#34;&gt;SFT监督微调
&lt;/h3&gt;&lt;p&gt;　　使用了公开的微调数据，自己标了一些，有27540这么多就不标了，因为数量少质量高效果也能好。 &lt;br&gt;
　　对于微调过程，每一个样本包含一个提示prompt和一个答案。为了确保模型序列长度都被填充，训练集里将全部的prompt和答案相连。用了特殊token划分prompt和答案片段。使用了一种自回归目标函数，并将来自用户prompt的token计算loss归零，所以只在答案token上反向传播。模型微调轮数为2。&lt;/p&gt;
&lt;h3 id=&#34;rlhf-人类反馈的增强学习&#34;&gt;RLHF 人类反馈的增强学习
&lt;/h3&gt;&lt;p&gt;　　RLHF是一种将微调模型行为与人类偏好和指令对齐的一种模型训练过程。首先让标注员写一个提示（prompt），然后对两个样本模型的回答根据制定的标准选择更好的一个，这种数据用于训练奖励模型（reward model）。为了多样性最大化，两个模型参数和超参数不同。为了让参与者强制选择，每个回答会打标程度（很好，好，一般，不确定）。合适的标注从两个方面考虑，有用性和安全性。在其它答案是安全的情况下，不会选择不安全的回答为最佳，因为本文认为安全回答也是更好的。&lt;br&gt;
　　收集了更多的偏好数据后，奖励模型进步了，LLAMA 2-CHAT就会训练地更好，而它更好又会改变模型数据分布。如果不用最新的样本分布，奖励模型的精确度就会很快降低。所以在微调新一轮LLAMA 2-CHAT前使用当前最新的LLAMA 2-CHAT收集数据很重要。这一步让奖励模型保持正确的分布，也能保持最新模型的准确奖励值。&lt;br&gt;
　　本文收集了超过一百万（1 million）人工标注数据，称做Meta奖励模型数据。不同领域提示和回答的数量不同，总结和在线公式数据一般有较长的提示，然而对话型提示通常较短。平均来看，本文的数据比开源数据集有更多轮的对话，并且更长。&lt;br&gt;
　　奖励模型将模型回复和对应的提示（包括之前的多轮上下文）作为输入，输出一个标量分数来衡量模型生成质量。将这些分数作为奖励，通过RLHF优化LLAMA 2-CHAT获得更好的效果，提升有用性和安全性。
　　有用性和安全性往往需要折中，所以本文训练了两个分开的奖励模型，一个用来优化可用性（Helpfulness RM），一个用来优化安全性（Safety RM）。&lt;br&gt;
　　本文使用预训练对话模型checkpoint来初始化奖励模型，这样可以确保所有的模型从预训练中得到知识。除了将下一个token预测分类头替换为一个标量奖励值的回归头输出以外，模型架构和超参数和预训练模型一致。&lt;/p&gt;
$$L_{ranking}=-log(\sigma(r_\theta(x, y_c)-r_\theta(x,y_r)))$$$$L_{ranking}=-log(\sigma(r_\theta(x, y_c)-r_\theta(x,y_r) - m(r)))$$&lt;p&gt;
　　其中，$m(r)$是评分的离散函数，两个回答越不同，这个边距越大，两个回答越相似，这个边距越近。这个边距可以提升有用性。&lt;/p&gt;
&lt;p&gt;　　训练细节：训了一轮，本文发现训久了过拟合。&lt;/p&gt;
&lt;p&gt;　　本文采用了两种主要的RLHF微调算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PPO(Proximal Policy Optimization)算法&lt;/li&gt;
&lt;li&gt;拒绝采用微调(Rejection Sampling fine-tuning)，只在70B上用了
在K个模型输出中使用奖励模型选择最好的，将选择出来的做梯度更新。对于每个提示，包含最好奖励分数的样本作为新的标准。然后在新的样本中微调，加强奖励。
两个算法主要在广度和深度上有区别。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;多轮对话一致性的系统信息&#34;&gt;多轮对话一致性的系统信息
&lt;/h3&gt;&lt;p&gt;有些指令应该贯穿对话始终，比如“扮演xx”指令，但是原始的RLHF模型在几轮后会忘记初始指令。为此，提出Ghost注意力（GAtt）。&lt;/p&gt;
&lt;h1 id=&#34;llama-open-and-efficient-foundation-language-models&#34;&gt;LLaMA: Open and Efficient Foundation Language Models
&lt;/h1&gt;&lt;p&gt;Meta AI
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2302.13971.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/facebookresearch/llama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述
&lt;/h2&gt;&lt;p&gt;　　不像Chinchilla、PaLM或者GPT-3，只使用公开可用的数据训练。
　　训练不是最快的，但是推理是最快的。
　　本文目标是打造一系列用更多token训练的最佳推理性能的大模型。LLaMA：6.7B、13.0B、32.5B、65.2B
　　LLaMA-13B超过GPT3。
　　LLaMA-65B与Chinchilla或PaLM-540B相当。&lt;/p&gt;
&lt;h2 id=&#34;方法-1&#34;&gt;方法
&lt;/h2&gt;&lt;h3 id=&#34;预训练数据&#34;&gt;预训练数据
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;English CommonCrawl(67%)：使用fastText去掉非英语文本、使用一个n-gram语言模型去掉低质量内容、用一个线性模型对维基百科中用作参考文献的页面与随机抽样的页面以及未归类为参考文献的废弃页面进行分类&lt;/li&gt;
&lt;li&gt;C4(15%)：与CCNet的主要区别是质量过滤，它主要依赖于启发式，如标点符号的存在或网页中的单词和句子的数量&lt;/li&gt;
&lt;li&gt;Github(4.5%):基于行长度或字母数字字符比例的启发式法过滤低质量文件&lt;/li&gt;
&lt;li&gt;Wikipedia(4.5%)&lt;/li&gt;
&lt;li&gt;Gutenberg and Book3(4.5%)：去掉了有90%重复的书籍&lt;/li&gt;
&lt;li&gt;ArXiv(2.5%):删除了第一部分之前的所有内容、参考书目、.tex文件中的评论、用户编写的内联扩展定义和宏&lt;/li&gt;
&lt;li&gt;Stack Exchange(2%)：问答数据，删除了文本中的HTML标签，并根据分数对答案进行了排序(由高到低)&lt;br&gt;
　　分词：BPE算法&lt;br&gt;
　　整个训练集经分词后有1.4T个token(33B和65B)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;架构&#34;&gt;架构
&lt;/h3&gt;&lt;p&gt;　　transformer架构基础上魔改。&lt;br&gt;
　　魔改点（中括号内为灵感来源）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预归一化[GPT3]：使用RMSNorm&lt;/li&gt;
&lt;li&gt;SwiGLU激活函数[PaLM]&lt;/li&gt;
&lt;li&gt;旋转embedding[GPTNeo]：将绝对位置编码换成了旋转位置嵌入RoPE&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　优化器：AdamW&lt;/p&gt;
&lt;p&gt;　　训练速度优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机多头注意力机制，不保存注意力权重，不计算key/query分数（masked）&lt;/li&gt;
&lt;li&gt;减少了反向传播中重复计算的激活单元的数量，只保存最耗费计算的单元，比如线性层的输出；没有使用PyTorch autograd；使用了模型和序列并行化减少模型内存占用；尽量将激活单元的计算和GPU之间的网络通信通用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　训练时间：&lt;br&gt;
　　65B参数模型：2048张A100 GPU，80GB内存，380 tokens/sec/GPU，1.4T tokens训练了21天&lt;/p&gt;
&lt;p&gt;　　它甚至还写了LLAMA的碳排放情况&amp;hellip;&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
