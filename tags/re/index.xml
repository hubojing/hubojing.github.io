<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>RE on 靖待</title>
        <link>https://hubojing.github.io/tags/re/</link>
        <description>Recent content in RE on 靖待</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>靖待</copyright>
        <lastBuildDate>Tue, 16 Aug 2022 19:37:35 +0000</lastBuildDate><atom:link href="https://hubojing.github.io/tags/re/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>关系抽取（RE）论文泛读</title>
        <link>https://hubojing.github.io/nx3rj2gp/</link>
        <pubDate>Tue, 16 Aug 2022 19:37:35 +0000</pubDate>
        
        <guid>https://hubojing.github.io/nx3rj2gp/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;https://hubojing.github.io/images/沉醉于知识的芬芳.png&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;论文泛读不定期更新。&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;document-level-relation-extraction-with-adaptive-focal-loss-and-knowledge-distillation&#34;&gt;Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation
&lt;/h1&gt;&lt;p&gt;具有自适应焦点损失和知识蒸馏的文档级关系抽取
阅读时间：2022-08-15&lt;/p&gt;
&lt;h2 id=&#34;论文概况&#34;&gt;论文概况
&lt;/h2&gt;&lt;p&gt;ACL 2022
阿里达摩院
Qingyu Tan, Ruidan He, Lidong Bing, Hwee Tou Ng
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2203.10900&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/tonytan48/KD-DocRE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;笔记&#34;&gt;笔记
&lt;/h2&gt;&lt;p&gt;文档级关系抽取要同时从多个句子中提取关系。本文提出DocRE算法，一个用于文档级别的关系抽取半监督算法，它有三个新组件。第一，用轴向注意力模块学习实体对之间的依赖关系。第二，提出了一个自适应的焦点损失来解决DocRE中类的不平衡问题。最后，利用知识蒸馏来克服人工标注数据与远程监督数据之间的差异。
现有问题：现存的方法关注实体对的句法特征，而忽略了实体对之间的交互作用；目前还没有工作可以直接的解决类的不平衡问题。现存的工作仅仅关注阈值学习来平衡正例和负例，但正例内部的类不平衡问题并没有得到解决；关于将远程监督数据应用于DocRE任务的研究很少。
贡献点：轴向注意力（提升two-hop关系的推理能力）、自适应焦点损失（解决标签分配不平衡的问题，长尾类在总的损失中占比较多）、知识蒸馏（克服标注数据和远程监督数据之间的差异）
&lt;img src=&#34;https://hubojing.github.io/images/KD-DocRE.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;KD-DocRE&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;packed-levitated-marker-for-entity-and-relation-extraction&#34;&gt;Packed Levitated Marker for Entity and Relation Extraction
&lt;/h1&gt;&lt;p&gt;打包悬浮标记用于实体和关系抽取
阅读时间：2022-08-15&lt;/p&gt;
&lt;h2 id=&#34;论文概述&#34;&gt;论文概述
&lt;/h2&gt;&lt;p&gt;ACL 2022
Deming Ye, Yankai Lin, Peng Li, Maosong Sun
清华大学与腾讯微信模式识别中心合作
&lt;a class=&#34;link&#34; href=&#34;https://aclanthology.org/2022.acl-long.337/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/thunlp/PL-Marker&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;笔记-1&#34;&gt;笔记
&lt;/h2&gt;&lt;p&gt;最近的命名实体识别和关系抽取工作专注于研究如何从预训练模型中获得更好的span表示。然而，许多工作忽略了span之间的相互关系。本文提出了一种基于悬浮标记的span表示方法，在编码过程中通过特定策略打包标记来考虑span之间的相互关系。对于命名实体识别任务，提出了一种面向邻居span的打包策略，以更好地建模实体边界信息。对于关系抽取任务，设计了一种面向头实体的打包策略，将每个头实体以及可能的尾实体打包，以共同建模同头实体的span对。
&lt;img src=&#34;https://hubojing.github.io/images/PL-Marker.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;PL-Marker&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;consistent-representation-learning-for-continual-relation-extraction&#34;&gt;Consistent Representation Learning for Continual Relation Extraction
&lt;/h1&gt;&lt;p&gt;一致表示学习用于连续关系抽取
阅读时间：2022-08-12&lt;/p&gt;
&lt;h2 id=&#34;论文概况-1&#34;&gt;论文概况
&lt;/h2&gt;&lt;p&gt;ACL 2022
Kang Zhao, Hua Xu, Jiangong Yang, Kai Gao
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2203.02721&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/thuiar/CRL&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;笔记-2&#34;&gt;笔记
&lt;/h2&gt;&lt;p&gt;通过对比学习和回放记忆时的知识蒸馏，提出一种新颖的一致性表示学习方法。使用基于记忆库的监督对比学习来训练每一个新的任务，以使模型高效学习特征表示。为了防止对老任务的遗忘，构造了记忆样本的连续回放，同时让模型保留在知识蒸馏中历史任务之间的关系。
&lt;img src=&#34;https://hubojing.github.io/images/CRL.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;CRL&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;pre-training-to-match-for-unified-low-shot-relation-extraction&#34;&gt;Pre-training to Match for Unified Low-shot Relation Extraction
&lt;/h1&gt;&lt;p&gt;预训练用于匹配统一少样本关系抽取
阅读时间：2022-08-12&lt;/p&gt;
&lt;h2 id=&#34;论文概况-2&#34;&gt;论文概况
&lt;/h2&gt;&lt;p&gt;ACL 2022
Fangchao Liu, Hongyu Lin, Xianpei Han, Boxi Cao, Le Sun
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2203.12274&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDF&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/fc-liu/MCMN&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CODE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;笔记-3&#34;&gt;笔记
&lt;/h2&gt;&lt;p&gt;低样本关系抽取旨在少样本甚至零样本场景下的关系抽取。由于低样本关系抽取所包含任务形式多样，传统方法难以统一处理。本文针对这一问题，提出了一种统一的低样本匹配网络：（1）基于语义提示（prompt）范式，构造了从关系描述到句子实例的匹配网络模型；（2）针对匹配网络模型学习，设计了三元组-复述的预训练方法，以增强模型对关系描述与实例之间语义匹配的泛化性。在零样本、小样本以及带负例的小样本关系抽取评测基准上的实验结果表明，该方法能有效提升低样本场景下关系抽取的性能，并且具备了较好的任务自适应能力。
&lt;img src=&#34;https://hubojing.github.io/images/MCMN.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;MCMN&#34;
	
	
&gt;&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
