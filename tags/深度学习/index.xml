<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>深度学习 on 靖待</title>
        <link>https://hubojing.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 深度学习 on 靖待</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>靖待</copyright>
        <lastBuildDate>Tue, 02 Mar 2021 09:20:00 +0000</lastBuildDate><atom:link href="https://hubojing.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>【Paper】Wide &amp; Deep Learning for Recommender Systems</title>
        <link>https://hubojing.github.io/ux6wmeer/</link>
        <pubDate>Tue, 02 Mar 2021 09:20:00 +0000</pubDate>
        
        <guid>https://hubojing.github.io/ux6wmeer/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;\images\Paper-wide&amp;deep-models.png&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;推荐系统 + 深度学习 2&lt;/strong&gt;
　　&lt;strong&gt;谷歌著名的Wide &amp;amp; Deep模型&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;论文背景&#34;&gt;论文背景
&lt;/h1&gt;&lt;p&gt;　　题目：Wide &amp;amp; Deep Learning for Recommender Systems
　　作者：Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
　　Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah, Google Inc.
　　会议信息：DLRS ’16 September 15-15, 2016, Boston, MA, USA&lt;/p&gt;
&lt;p&gt;　　谷歌引用数量：1324（截至2021年3月2日）&lt;/p&gt;
&lt;h1 id=&#34;引言-introduction&#34;&gt;引言 INTRODUCTION
&lt;/h1&gt;&lt;p&gt;　　推荐系统可视为搜索排序系统，输入是用户和上下文信息的查询，输出是物品列表。类似于一般的搜索排序问题，推荐系统中的一大挑战是同时实现记忆（memorization）和泛化（generalization）。
　　Memorization可以宽泛地定义为学习物品或特征的共现频率并探索历史数据的相关关系。
　　Generalization是基于相关性的传递性并探索过去从未出现过的新的特征组合。
　　基于memorization的推荐系统通常更局限于和直接与用户曾有过行为的物品相关。
　　基于generalization的推荐系统试图增强推荐物品的多样性。&lt;/p&gt;
&lt;p&gt;　　例子：如果用户安装了netflix，特征&amp;quot;user_installed_app=netflix&amp;quot;的值为1。
　　Memorization：通过使用稀疏特征的跨物品转换实现，例如AND(user_installed_app=netflix, impression_app=pandora&amp;quot;)，如果用户安装了netflix，然后显示pandora, AND的值为1。
　　Generalization：可以通过使用粒度更小的特性来添加，例如AND(user_installed_category=video，impression_category=music)，但是通常需要手动的特征工程。
　　叉积变换（cross-product transformations）的一个限制是它们不能推广到没有出现在训练数据中的查询项特征对。基于嵌入的模型，例如FM或者DNN跨域解决这个问题。但是当底层的查询项矩阵是稀疏且高阶的（例如用户具有特定的偏好或小范围的吸引力）时，很难学习查询和项的有效低维表示。在这种情况下，大多数查询项对之间应该没有交互，但密集嵌入将导致所有查询项对的预测非零，因此导致过拟合而产生不相干的推荐。具有叉积特征转换的线性模型可以用更少的参数记住这些“例外规则”。&lt;/p&gt;
&lt;p&gt;　　在本文中提出了Wide&amp;amp;Deep模型，通过联合训练一个线性模型组件和一个神经网络组件，实现在一个模型中记忆和泛化。
&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cPaper-wide&amp;amp;deep-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Wide&amp;Deep模型&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;贡献点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;带有嵌入和带有特征转换的线性模型的前馈神经网络联合训练的Wide&amp;amp;Deep学习框架，用于具有稀疏输入的通用推荐系统。&lt;/li&gt;
&lt;li&gt;在谷歌Play上实施和评估，谷歌Play是一个拥有超过10亿活跃用户和超过百万应用的移动应用商店。&lt;/li&gt;
&lt;li&gt;在TensorFlow中有开源代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;推荐系统概述-recommender-system-overview&#34;&gt;推荐系统概述 RECOMMENDER SYSTEM OVERVIEW
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://hubojing.github.io/images/Paper-wide&amp;amp;deep-models-overview.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;概述&#34;
	
	
&gt;
　　推荐系统一般分为召回（Retrieval）层，排序（Ranking）层。本文在排序层使用Wide &amp;amp; Deep学习框架。&lt;/p&gt;
&lt;h1 id=&#34;宽--深度学习框架-wide--deep-learning&#34;&gt;宽 &amp;amp; 深度学习框架 WIDE &amp;amp; DEEP LEARNING
&lt;/h1&gt;&lt;h2 id=&#34;宽度组件-the-wide-component&#34;&gt;宽度组件 The Wide Component
&lt;/h2&gt;$$\phi_k(x) = \prod_{i=1}^d{x_i}^{c_{ki}}, c_{ki}∈{0,1}$$&lt;p&gt;
　　其中，$c_{ki}$是一个布尔变量，如果第i个特征是第k个变换$\phi_k$的一部分，则为1，否则为0。
　　对于二进制特征，一个叉积变换“AND(gender=female, language=en)，只有当(“gender=female” and “language=en”)时才为1。
　　这捕获了二元特征之间的相互作用，并为广义线性模型增加了非线性。&lt;/p&gt;
&lt;h2 id=&#34;深度组件-the-deep-component&#34;&gt;深度组件 The Deep Component
&lt;/h2&gt;$$\alpha^{l+1} = f(W^{(l)}a^{(l)}) + b^{(l)})$$&lt;h2 id=&#34;模型联合训练-joint-training-of-wide--deep-model&#34;&gt;模型联合训练 Joint Training of Wide &amp;amp; Deep Model
&lt;/h2&gt;$$P(Y=1|x) = \sigma(w^T_{wide}[x,\phi(x)]+w^T_{deep}a^{(l_f)}+b)$$&lt;p&gt;
　　其中Y是二分类标签，$\sigma(·)$是sigmoid函数，$\phi(x)$是原始特征x的叉积变换，b是偏置项。$w_{wide}$是所有宽度模型权重的向量，$w_{deep}$是应用在最终激活$a^{(l_f)}$的权重。&lt;/p&gt;
&lt;h1 id=&#34;系统实现-system-implementation&#34;&gt;系统实现 SYSTEM IMPLEMENTATION
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cPaper-wide&amp;amp;deep-models-%e7%b3%bb%e7%bb%9f%e5%ae%9e%e7%8e%b0.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;系统实现&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;数据产生-data-generation&#34;&gt;数据产生 Data Generation
&lt;/h2&gt;&lt;p&gt;　　通过将一个特征值x映射到其累积分布函数P(X≤x)，将连续实值特征归一化为[0,1]，并分成$n_q$分位数。对于第i个分位数的值，规范化值为$\frac{i-1}{n_q-1}$。分位数边界i−1在数据生成时计算。&lt;/p&gt;
&lt;h2 id=&#34;模型训练-model-training&#34;&gt;模型训练 Model Training
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cPaper-wide&amp;amp;deep-models-%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;模型训练&#34;
	
	
&gt;
　　在训练过程中，输入层接收训练数据和词汇，并生成稀疏和密集特征以及标签。宽度组件包括用户安装应用和印象应用的叉积。对于模型的深层部分，每个分类特征学习一个32维的嵌入向量。将所有的嵌入与密集特征连接在一起，得到一个大约1200维的密集向量。然后将连接的矢量送入3个ReLU层，最后送入logistic输出单元。
　　Wide &amp;amp; Deep模型训练了超过5000亿个例子。每当一组新的训练数据到达时，模型就需要重新训练。然而，每次重新训练在计算上都是昂贵的，并且延迟了服务时间。为了解决这一挑战，本文实现了一个暖启动系统，该系统使用先前模型的嵌入和线性模型权值来初始化一个新的模型。
在将模型加载到模型服务器之前，需要对模型进行一次演练，以确保在服务实时流量时不会出现问题。本文根据经验来验证模型的质量，作为一个完整的检查。&lt;/p&gt;
&lt;h2 id=&#34;模型服务-model-serving&#34;&gt;模型服务 Model Serving
&lt;/h2&gt;&lt;p&gt;　　一旦模型经过训练和验证，就把它加载到模型服务器中。对于每个请求，服务器都会从应用程序检索系统和用户特性中接收一组应用程序候选项来为每个应用程序评分。然后，应用程序从最高分到最低分进行排名，并按照这个顺序向用户展示这些应用程序。分数是通过运行一个采用Wide &amp;amp; Deep模型的正向推理来计算的。
　　为了为每个请求提供10毫秒量级的服务，使用多线程并行来优化性能，通过并行运行较小的批处理，来代替在单个批处理推理步骤中对所有候选应用程序进行评分。&lt;/p&gt;
&lt;h1 id=&#34;实验-experiment-results&#34;&gt;实验 EXPERIMENT RESULTS
&lt;/h1&gt;&lt;h2 id=&#34;app-acquisitions&#34;&gt;App Acquisitions
&lt;/h2&gt;&lt;p&gt;　　本文在A/B测试框架下进行了为期3周的在线实时实验。对于对照组，随机选择1%的用户，并向他们展示由上一个版本的排名模型生成的推荐，该模型是一个高度优化的广泛性logistic回归模型，具有丰富的叉积特征转换。在实验组中，1%的用户使用了由相同的一组特征进行训练的Wide &amp;amp; Deep模型生成的推荐。
&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cPaper-wide&amp;amp;deep-models-t1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;性能对比&#34;
	
	
&gt;
　　Wide &amp;amp; Deep模式使app store主登陆页面的应用获取率比对照组提高了+3.9%。结果还与另1%组仅使用具有相同特征和神经网络结构的模型的深度部分进行了比较，Wide &amp;amp; deep模式比deep-only模型有+1%的增益。
　　除了在线实验，还展示了AUC。Wide &amp;amp; Deep的线下AUC略高，但对线上流量的影响更显著。一个可能的原因是离线数据集中的印象和标签是固定的，而在线系统可以通过混合归纳和记忆生成新的探索性推荐，并从新的用户反应中学习。&lt;/p&gt;
&lt;h2 id=&#34;服务性能-serving-performance&#34;&gt;服务性能 Serving Performance
&lt;/h2&gt;&lt;p&gt;　　面对我们的商业移动应用商店所面临的高流量，高吞吐量和低延迟的服务具有挑战性。在高峰流量时，我们的推荐服务器每秒可以获得超过1000万个应用。使用单个线程，在一次批处理中为所有候选人打分需要31毫秒。我们实现了多线程，并将每个批处理分成更小的部分，这显著地将客户端延迟减少到14毫秒（包括服务开销），如表所示。
&lt;img src=&#34;https://hubojing.github.io/%5cimages%5cPaper-wide&amp;amp;deep-models-t2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;服务性能&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;相关工作-related-work&#34;&gt;相关工作 RELATED WORK
&lt;/h1&gt;&lt;p&gt;　　结合带叉积转换的广义线性模型与深层神经网络嵌入的灵感来自以前的工作，比如FM，通过在两个低维嵌入向量之间使用点积分解两个变量间的相互作用，将线性模型了进行推广。在本文中，通过神经网络代替点积来学习嵌入之间高度非线性的相互作用，从而扩展了模型容量。
　　在语言模型中，通过学习输入和输出之间的直接权值，提出了使用n元特征的递归神经网络(RNNs)和最大熵模型联合训练，以显著降低RNN的复杂性(例如，隐藏层大小)。在计算机视觉中，深度残差学习已被用于降低训练更深层次模型的难度，并通过跳过一个或多个层次的捷径连接提高准确性。
　　神经网络与图形模型的联合训练还被应用于基于图像的人体姿态估计。在这项工作中，探讨了前馈神经网络和线性模型的联合训练，在稀疏特征和输出单元之间直接连接，用于输入数据稀疏的通用推荐和排序问题。
　　在推荐系统文献中，将内容信息的深度学习与评分矩阵的协同过滤(CF)相结合来探索协同深度学习。以前的工作也曾致力于手机应用推荐系统，如AppJoy在用户的应用使用记录上使用CF。不同于之前工作中基于cf或基于内容的方法，我们在app推荐系统中，基于用户和印象数据使用Wide &amp;amp; Deep模型联合训练。&lt;/p&gt;
&lt;h1 id=&#34;总结-conclusion&#34;&gt;总结 CONCLUSION
&lt;/h1&gt;&lt;p&gt;　　宽度线性模型可以通过叉积特征变换有效地记忆稀疏特征交互，而深度神经网络可以通过低维嵌入来泛化之前未见过的特征交互。在线实验结果表明，与Wide-only和Deep-only模型相比，Wide &amp;amp; Deep模型有显著提高。&lt;/p&gt;
&lt;h1 id=&#34;代码&#34;&gt;代码
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/estimator/canned/dnn_linear_combined.py&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/estimator/canned/dnn_linear_combined.py&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>深度学习常用数学知识</title>
        <link>https://hubojing.github.io/kw4ffint/</link>
        <pubDate>Sat, 15 Aug 2020 11:00:19 +0000</pubDate>
        
        <guid>https://hubojing.github.io/kw4ffint/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;\images\深度学习常用数学知识-cover.png&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;编辑这些公式给我累si了&lt;/strong&gt;
　　&lt;strong&gt;同样适合考研狗收藏&lt;/strong&gt;
　　&lt;strong&gt;非常欢迎纠错&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;高等数学&#34;&gt;高等数学
&lt;/h1&gt;&lt;h2 id=&#34;导数定义&#34;&gt;导数定义
&lt;/h2&gt;&lt;p&gt;导数和微分的概念&lt;/p&gt;
&lt;p&gt;$f&amp;rsquo;({x_0})=\underset{\Delta x\to 0}{\mathop{\lim }},\frac{f({x_0}+\Delta x)-f({x_0})}{\Delta x}$  （1）&lt;/p&gt;
&lt;p&gt;或者：&lt;/p&gt;
&lt;p&gt;$f&amp;rsquo;({x_0})=\underset{x\to {x_0}}{\mathop{\lim }},\frac{f(x)-f({x_0})}{x-{x_0}}$           （2）&lt;/p&gt;
&lt;h2 id=&#34;左右导数导数的几何意义和物理意义&#34;&gt;左右导数导数的几何意义和物理意义
&lt;/h2&gt;&lt;p&gt;函数$f(x)$在$x_0$处的左、右导数分别定义为：&lt;/p&gt;
&lt;p&gt;左导数：$f&amp;rsquo;&lt;em&gt;{-}(x_0)=\underset{\Delta x\to {0^{-}}}{\mathop{\lim }},\frac{f({x_0}+\Delta x)-f({x_0})}{\Delta x}=\underset{x\to x&lt;/em&gt;{0}^{-}}{\mathop{\lim }},\frac{f(x)-f({x_0})}{x-{x_0}},(x={x_0}+\Delta x)$&lt;/p&gt;
&lt;p&gt;右导数：$f&amp;rsquo;_{+}(x_0)=\underset{\Delta x\to {0^{+}}}{\mathop{\lim }},\frac{f({x_0}+\Delta x)-f({x_0})}{\Delta x}=\underset{x\to x_0^{+}}{\mathop{\lim }},\frac{f(x)-f({x_0})}{x-{x_0}}$&lt;/p&gt;
&lt;h2 id=&#34;函数的可导性与连续性之间的关系&#34;&gt;函数的可导性与连续性之间的关系
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Th1:&lt;/strong&gt; 函数$f(x)$在$x_0$处可微$\Leftrightarrow f(x)$在$x_0$处可导&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th2:&lt;/strong&gt; 若函数在点$x_0$处可导，则$y=f(x)$在点$x_0$处连续，反之则不成立。即函数连续不一定可导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th3:&lt;/strong&gt; $f&amp;rsquo;(x_0)$存在$\Leftrightarrow f&amp;rsquo;&lt;em&gt;{-}(x_0)=f&amp;rsquo;&lt;/em&gt;{+}(x_0)$&lt;/p&gt;
&lt;h2 id=&#34;平面曲线的切线和法线&#34;&gt;平面曲线的切线和法线
&lt;/h2&gt;&lt;p&gt;切线方程 : $y-{y_0}=f&amp;rsquo;({x_0})(x-x_0)$&lt;br&gt;
法线方程：$y-{y_0}=-\frac{1}{f&amp;rsquo;({x_0})}(x-{x_0}),f&amp;rsquo;({x_0})\ne 0$&lt;/p&gt;
&lt;h2 id=&#34;四则运算法则&#34;&gt;四则运算法则
&lt;/h2&gt;&lt;p&gt;设函数$u=u(x)，v=v(x)$在点$x$可导则&lt;br&gt;
(1) $(u\pm v)&amp;rsquo;={u}&amp;rsquo;\pm {v}&amp;rsquo;$&lt;br&gt;
$d(u\pm v)=du\pm dv$&lt;br&gt;
(2)$(uv)&amp;rsquo;=uv&amp;rsquo;+vu&amp;rsquo;$&lt;br&gt;
$d(uv)=udv+vdu$&lt;br&gt;
(3) $(\frac{u}{v})&amp;rsquo;=\frac{vu&amp;rsquo;-uv&amp;rsquo;}{v^{2}}(v\ne 0)$&lt;br&gt;
$d(\frac{u}{v})=\frac{vdu-udv}{v^{2}}$&lt;/p&gt;
&lt;h2 id=&#34;基本导数与微分表&#34;&gt;基本导数与微分表
&lt;/h2&gt;&lt;p&gt;(1) $y=c$（常数）    &lt;br&gt;
${y}&amp;rsquo;=0$       &lt;br&gt;
$dy=0$&lt;/p&gt;
&lt;p&gt;(2) $y={x^{\alpha }}$($\alpha$为实数) &lt;br&gt;
${y}&amp;rsquo;=\alpha {x^{\alpha -1}}$   &lt;br&gt;
$dy=\alpha {x^{\alpha -1}}dx$&lt;/p&gt;
&lt;p&gt;(3) $y={a^x}$   &lt;br&gt;
${y}&amp;rsquo;={a^x}\ln a$      &lt;br&gt;
$dy={a^x}\ln adx$&lt;br&gt;
特例:&lt;br&gt;
$({e^{x}}&amp;rsquo;={e^{x}}$          &lt;br&gt;
$d({e^{x}})={e^{x}}dx$&lt;/p&gt;
&lt;p&gt;(4) $y={\log_{a}}x$   ${y}&amp;rsquo;=\frac{1}{x\ln a}$        &lt;br&gt;
$dy=\frac{1}{x\ln a}dx$
特例:$y=\ln x$                   &lt;br&gt;
$(\ln x)&amp;rsquo;=\frac{1}{x}$    &lt;br&gt;
$d(\ln x)=\frac{1}{x}dx$&lt;/p&gt;
&lt;p&gt;(5) $y=\sin x$      &lt;br&gt;
${y}&amp;rsquo;=\cos x$        $d(\sin x)=\cos xdx$&lt;/p&gt;
&lt;p&gt;(6) $y=\cos x$   &lt;br&gt;
${y}&amp;rsquo;=-\sin x$       $d(\cos x)=-\sin xdx$&lt;/p&gt;
&lt;p&gt;(7) $y=\tan x$&lt;br&gt;
${y}&amp;rsquo;=\frac{1}{\cos^{2}x}=\sec^{2}x$&lt;br&gt;
$d(\tan x)=\sec^{2}xdx$&lt;/p&gt;
&lt;p&gt;(8) $y=\cot x$
${y}&amp;rsquo;=-\frac{1}{\sin^{2}x}=-\csc^{2}x$&lt;br&gt;
$d(\cot x)=-\csc^{2}xdx$&lt;/p&gt;
&lt;p&gt;(9) $y=\sec x$ ${y}&amp;rsquo;=\sec x\tan x$&lt;/p&gt;
&lt;p&gt;$d(\sec x)=\sec x\tan xdx$
(10) $y=\csc x$ ${y}&amp;rsquo;=-\csc x\cot x$&lt;/p&gt;
&lt;p&gt;$d(\csc x)=-\csc x\cot xdx$
(11) $y=\arcsin x$&lt;/p&gt;
&lt;p&gt;${y}&amp;rsquo;=\frac{1}{\sqrt{1-{x^{2}}}}$&lt;/p&gt;
&lt;p&gt;$d(\arcsin x)=\frac{1}{\sqrt{1-x^{2}}}dx$
(12) $y=\arccos x$&lt;/p&gt;
&lt;p&gt;${y}&amp;rsquo;=-\frac{1}{\sqrt{1-x^{2}}}$  &lt;br&gt;
$d(\arccos x)=-\frac{1}{\sqrt{1-x^{2}}}dx$&lt;/p&gt;
&lt;p&gt;(13) $y=\arctan x$
${y}&amp;rsquo;=\frac{1}{1+x^{2}}$  &lt;br&gt;
$d(\arctan x)=\frac{1}{1+x^{2}}dx$&lt;/p&gt;
&lt;p&gt;(14) $y=\operatorname{arc}\cot x$   &lt;br&gt;
${y}&amp;rsquo;=-\frac{1}{1+x^{2}}$&lt;br&gt;
$d(\operatorname{arc}\cot x)=-\frac{1}{1+x^{2}}dx$&lt;/p&gt;
&lt;p&gt;(15) $y=shx$ &lt;br&gt;
${y}&amp;rsquo;=chx$    &lt;br&gt;
$d(shx)=chxdx$&lt;/p&gt;
&lt;p&gt;(16) $y=chx$ &lt;br&gt;
${y}&amp;rsquo;=shx$    &lt;br&gt;
$d(chx)=shxdx$&lt;/p&gt;
&lt;h2 id=&#34;复合函数反函数隐函数以及参数方程所确定的函数的微分法&#34;&gt;复合函数，反函数，隐函数以及参数方程所确定的函数的微分法
&lt;/h2&gt;&lt;p&gt;(1) 反函数的运算法则: 设$y=f(x)$在点$x$的某邻域内单调连续，在点$x$处可导且$f&amp;rsquo;(x)\ne 0$，则其反函数在点$x$所对应的$y$处可导，并且有$\frac{dy}{dx}=\frac{1}{\frac{dx}{dy}}$&lt;/p&gt;
&lt;p&gt;(2) 复合函数的运算法则:若 $\mu =\varphi(x)$ 在点$x$可导,而$y=f(\mu)$在对应点$\mu$($\mu =\varphi (x)$)可导,则复合函数$y=f(\varphi (x))$在点$x$可导,且$y&amp;rsquo;=f&amp;rsquo;(\mu )\cdot {\varphi }&amp;rsquo;(x)$&lt;/p&gt;
&lt;p&gt;(3) 隐函数导数$\frac{dy}{dx}$的求法一般有三种方法：&lt;/p&gt;
&lt;p&gt;1)方程两边对$x$求导，要记住$y$是$x$的函数，则$y$的函数是$x$的复合函数.例如$\frac{1}{y}$，${y^{2}}$，$ln y$，${e^{y}}$等均是$x$的复合函数.
对$x$求导应按复合函数连锁法则做.&lt;/p&gt;
&lt;p&gt;2)公式法.由$F(x,y)=0$知 $\frac{dy}{dx}=-\frac{F&amp;rsquo;&lt;em&gt;{x}(x,y)}{F&amp;rsquo;&lt;/em&gt;{y}(x,y)}$,其中，${F&amp;rsquo;&lt;em&gt;{x}}(x,y)$，
${F&amp;rsquo;&lt;/em&gt;{y}}(x,y)$分别表示$F(x,y)$对$x$和$y$的偏导数&lt;/p&gt;
&lt;p&gt;3)利用微分形式不变性&lt;/p&gt;
&lt;h2 id=&#34;常用高阶导数公式&#34;&gt;常用高阶导数公式
&lt;/h2&gt;&lt;p&gt;（1）$(a^{x}){,}^{(n)}=a^{x}{\ln }^{n}a\quad (a&amp;gt;{0})\quad \quad (e^{x}){,}^{(n)}=e{,}^{x}$&lt;/p&gt;
&lt;p&gt;（2）$(\sin kx),^{(n)}=k^{n}\sin (kx+n\cdot \frac{\pi }{{2}})$&lt;/p&gt;
&lt;p&gt;（3）$(\cos kx),^{(n)}={k^{n}}\cos (kx+n\cdot \frac{\pi }{{2}})$&lt;/p&gt;
&lt;p&gt;（4）$(x^{m}),^{(n)}=m(m-1)\cdots (m-n+1){x^{m-n}}$&lt;/p&gt;
&lt;p&gt;（5）$(\ln x),^{(n)}={(-1)^{(n-1)}}\frac{(n-1)!}{x^{n}}$&lt;/p&gt;
&lt;p&gt;（6）莱布尼兹公式：若$u(x),,v(x)$均$n$阶可导，则
${(uv)^{(n)}}=\sum\limits_{i=0}^{n}{c_{n}^{i}{u^{(i)}}{v^{(n-i)}}}$，其中${u^{({0})}}=u$，${v^{({0})}}=v$&lt;/p&gt;
&lt;h2 id=&#34;微分中值定理泰勒公式&#34;&gt;微分中值定理，泰勒公式
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Th1:&lt;/strong&gt;(费马定理)
若函数$f(x)$满足条件：
(1)函数$f(x)$在${x_{0}}$的某邻域内有定义，并且在此邻域内恒有
$f(x)\le f(x_{0})$或$f(x)\ge f(x_{0})$,&lt;/p&gt;
&lt;p&gt;(2) $f(x)$在${x_{0}}$处可导,则有 ${f}&amp;rsquo;(x_{0})=0$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th2:&lt;/strong&gt;(罗尔定理)
设函数$f(x)$满足条件：&lt;br&gt;
(1)在闭区间$[a,b]$上连续；&lt;br&gt;
(2)在$(a,b)$内可导；&lt;br&gt;
(3)$f(a)=f(b)$；&lt;br&gt;
则在$(a,b)$内一存在个$\xi$，使  ${f}&amp;rsquo;(\xi )=0$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th3:&lt;/strong&gt; (拉格朗日中值定理)
设函数$f(x)$满足条件：
(1)在$[a,b]$上连续；
(2)在$(a,b)$内可导；
则在$(a,b)$内一存在个$\xi$，使  $\frac{f(b)-f(a)}{b-a}={f}&amp;rsquo;(\xi )$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th4:&lt;/strong&gt; (柯西中值定理)
设函数$f(x)$，$g(x)$满足条件：
(1) 在$[a,b]$上连续；
(2) 在$(a,b)$内可导且$f&amp;rsquo;(x)$，$g&amp;rsquo;(x)$均存在，且$g&amp;rsquo;(x)\ne 0$
则在$(a,b)$内存在一个$\xi$，使  $\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f&amp;rsquo;(\xi )}{g&amp;rsquo;(\xi )}$&lt;/p&gt;
&lt;h2 id=&#34;洛必达法则&#34;&gt;洛必达法则
&lt;/h2&gt;&lt;p&gt;法则Ⅰ ($\frac{0}{0}$型)
设函数$f\left( x \right),g\left( x \right)$满足条件：
$\underset{x\to {x_{0}}}{\mathop{\lim }},f\left( x \right)=0,\underset{x\to {x_{0}}}{\mathop{\lim }},g\left( x \right)=0$;&lt;/p&gt;
&lt;p&gt;$f\left( x \right),g\left( x \right)$在${x_{0}}$的邻域内可导，(在${x_{0}}$处可除外)且${g}&amp;rsquo;\left( x \right)\ne 0$;&lt;/p&gt;
&lt;p&gt;$\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f&amp;rsquo;\left( x \right)}{g&amp;rsquo;\left( x \right)}$存在(或$\infty$)。&lt;/p&gt;
&lt;p&gt;则:
$\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f\left( x \right)}{g\left( x \right)}=\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f&amp;rsquo;\left( x \right)}{g&amp;rsquo;\left( x \right)}$。
法则${I&amp;rsquo;}$ ($\frac{0}{0}$型)设函数$f\left( x \right),g\left( x \right)$满足条件：
$\underset{x\to \infty }{\mathop{\lim }},f\left( x \right)=0,\underset{x\to \infty }{\mathop{\lim }},g\left( x \right)=0$;&lt;/p&gt;
&lt;p&gt;存在一个$X&amp;gt;0$,当$\left| x \right|&amp;gt;X$时,$f\left( x \right),g\left( x \right)$可导,且$g&amp;rsquo;\left( x \right)\ne 0$;$\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f&amp;rsquo;\left( x \right)}{g&amp;rsquo;\left( x \right)}$存在(或$\infty$)。&lt;/p&gt;
&lt;p&gt;则$\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f\left( x \right)}{g\left( x \right)}=\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f&amp;rsquo;\left( x \right)}{g&amp;rsquo;\left( x \right)}$
法则Ⅱ( $\frac{\infty }{\infty }$ 型) 设函数 $f\left( x \right),g\left( x \right)$ 满足条件：
$\underset{x\to {x_{0}}}{\mathop{\lim }},f\left( x \right)=\infty,\underset{x\to {x_{0}}}{\mathop{\lim }},g\left( x \right)=\infty$;&lt;br&gt;
$f\left( x \right),g\left( x \right)$ 在 ${x_{0}}$ 的邻域内可导(在${x_{0}}$处可除外)且$g&amp;rsquo;\left( x \right)\ne 0$;$\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f&amp;rsquo;\left( x \right)}{g&amp;rsquo;\left( x \right)}$ 存在(或$\infty$)。
则$\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f\left( x \right)}{g\left( x \right)}=\underset{x\to {x_{0}}}{\mathop{\lim }},\frac{f&amp;rsquo;\left( x \right)}{g&amp;rsquo;\left( x \right)}$ 同理法则${II&amp;rsquo;}$ ( $\frac{\infty }{\infty }$ 型)仿法则 $I&amp;rsquo;}$ 可写出。&lt;/p&gt;
&lt;h2 id=&#34;泰勒公式&#34;&gt;泰勒公式
&lt;/h2&gt;&lt;p&gt;设函数$f(x)$在点${x_{0}}$处的某邻域内具有$n+1$阶导数，则对该邻域内异于${x_{0}}$的任意点$x$，在${x_{0}}$与$x$之间至少存在一个$\xi$，使得：
$f(x)=f(x_{0})+f&amp;rsquo;(x_{0})(x-x_{0})+\frac{1}{2!}f&amp;rsquo;&amp;rsquo;(x_{0})(x-x_{0})^{2}+\cdots +\frac{f^{(n)}(x_{0})}{n!}(x-x_{0})^{n}+R_{n}(x)$&lt;br&gt;
其中 $R_{n}(x)=\frac{f^{(n+1)}(\xi )}{(n+1)!}{(x-x_{0})^{n+1}}$称为$f(x)$在点$x_{0}$处的$n$阶泰勒余项。&lt;/p&gt;
&lt;p&gt;令$x_{0}=0$，则$n$阶泰勒公式
$f(x)=f(0)+f&amp;rsquo;(0)x+\frac{1}{2!}f&amp;rsquo;&amp;rsquo;(0){x^{2}}+\cdots +\frac{f^{(n)}(0)}{n!}x^{n}+R_{n}(x)$……(1)
其中 $R_{n}(x)=\frac{f^{(n+1)}(\xi )}{(n+1)!}x^{n+1}$，$\xi$在0与$x$之间.(1)式称为麦克劳林公式&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;常用五种函数在${x_{0}}=0$处的泰勒公式&lt;/strong&gt;
(1) $e^{x}=1+x+\frac{1}{2!}x^{2}+\cdots +\frac{1}{n!}x^{n}+\frac{x^{n+1}}{(n+1)!}e^{\xi }$&lt;/p&gt;
&lt;p&gt;或 $e^{x}=1+x+\frac{1}{2!}x^{2}+\cdots +\frac{1}{n!}x^{n}+o(x^{n})$&lt;/p&gt;
&lt;p&gt;(2) $\sin x=x-\frac{1}{3!}x^{3}+\cdots +\frac{x^{n}}{n!}\sin \frac{n\pi }{2}+\frac{x^{n+1}}{(n+1)!}\sin (\xi +\frac{n+1}{2}\pi )$&lt;/p&gt;
&lt;p&gt;或  $\sin x=x-\frac{1}{3!}{x^{3}}+\cdots +\frac{x^{n}}{n!}\sin \frac{n\pi }{2}+o(x^{n})$&lt;/p&gt;
&lt;p&gt;(3) $\cos x=1-\frac{1}{2!}x^{2}+\cdots +\frac{x^{n}}{n!}\cos \frac{n\pi }{2}+\frac{x^{n+1}}{(n+1)!}\cos (\xi +\frac{n+1}{2}\pi )$&lt;/p&gt;
&lt;p&gt;或   $\cos x=1-\frac{1}{2!}x^{2}+\cdots +\frac{x^{n}}{n!}\cos \frac{n\pi }{2}+o(x^{n})$&lt;/p&gt;
&lt;p&gt;(4) $\ln (1+x)=x-\frac{1}{2}x^{2}+\frac{1}{3}x^{3}-\cdots +(-1)^{n-1}\frac{x^{n}}{n}+\frac{(-1)^{n}x^{n+1}}{(n+1)(1+\xi )^{n+1}}$&lt;br&gt;
或      $\ln (1+x)=x-\frac{1}{2}x^{2}+\frac{1}{3}x^{3}-\cdots +(-1)^{n-1}\frac{x^{n}}{n}+o(x^{n})$&lt;/p&gt;
&lt;p&gt;(5) $(1+x)^{m}=1+mx+\frac{m(m-1)}{2!}x^{2}+\cdots +\frac{m(m-1)\cdots (m-n+1)}{n!}x^{n}$
$+\frac{m(m-1)\cdots (m-n+1)}{(n+1)!}x^{n+1}{(1+\xi )}^{m-n-1}$&lt;/p&gt;
&lt;p&gt;或$(1+x)^{m}=1+mx+\frac{m(m-1)}{2!}x^{2}+\cdots +\frac{m(m-1)\cdots (m-n+1)}{n!}x^{n}+o(x^{n})$&lt;/p&gt;
&lt;h2 id=&#34;函数单调性的判断&#34;&gt;函数单调性的判断
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Th1:&lt;/strong&gt;  设函数$f(x)$在$(a,b)$区间内可导，如果对$\forall x\in (a,b)$，都有$f,&amp;rsquo;(x)&amp;gt;0$（或$f,&amp;rsquo;(x)&amp;lt;0$），则函数$f(x)$在$(a,b)$内是单调增加的（或单调减少）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th2:&lt;/strong&gt; （取极值的必要条件）设函数$f(x)$在$x_{0}$处可导，且在$x_{0}$处取极值，则$f,&amp;rsquo;(x_{0})=0$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th3:&lt;/strong&gt; （取极值的第一充分条件）设函数$f(x)$在$x_{0}$的某一邻域内可微，且$f,&amp;rsquo;(x_{0})=0$（或$f(x)$在$x_{0}$处连续，但$f,&amp;rsquo;(x_{0})$不存在。）
(1)若当$x$经过$x_{0}$时，$f,&amp;rsquo;(x)$由“+”变“-”，则$f(x_{0})$为极大值；
(2)若当$x$经过$x_{0}$时，$f,&amp;rsquo;(x)$由“-”变“+”，则$f(x_{0})$为极小值；
(3)若$f,&amp;rsquo;(x)$经过$x={x_{0}}$的两侧不变号，则$f(x_{0})$不是极值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th4:&lt;/strong&gt; (取极值的第二充分条件)设$f(x)$在点${x_{0}}$处有$f&amp;rsquo;&amp;rsquo;(x)\ne 0$，且$f,&amp;rsquo;(x_{0})=0$，则 当$f&amp;rsquo;,&amp;rsquo;(x_{0})&amp;lt;0$时，$f({x_{0}})$为极大值；
当$f&amp;rsquo;,&amp;rsquo;(x_{0})&amp;gt;0$时，$f({x_{0}})$为极小值。
注：如果$f&amp;rsquo;,&amp;rsquo;(x_{0})=0$，此方法失效。&lt;/p&gt;
&lt;h2 id=&#34;渐近线的求法&#34;&gt;渐近线的求法
&lt;/h2&gt;&lt;p&gt;(1)水平渐近线&lt;br&gt;
若$\underset{x\to +\infty }{\mathop{\lim }},f(x)=b$，或$\underset{x\to -\infty }{\mathop{\lim }},f(x)=b$，则&lt;/p&gt;
&lt;p&gt;$y=b$称为函数$y=f(x)$的水平渐近线。&lt;/p&gt;
&lt;p&gt;(2)铅直渐近线&lt;br&gt;
若$\underset{x\to x_{0}^{-}}{\mathop{\lim }},f(x)=\infty$，或$\underset{x\to x_{0}^{+}}{\mathop{\lim }},f(x)=\infty$，则&lt;/p&gt;
&lt;p&gt;$x={x_{0}}$称为$y=f(x)$的铅直渐近线。&lt;/p&gt;
&lt;p&gt;(3)斜渐近线&lt;br&gt;
若$a=\underset{x\to \infty }{\mathop{\lim }},\frac{f(x)}{x},\quad b=\underset{x\to \infty }{\mathop{\lim }},[f(x)-ax]$，则
$y=ax+b$称为$y=f(x)$的斜渐近线。&lt;/p&gt;
&lt;h2 id=&#34;函数凹凸性的判断&#34;&gt;函数凹凸性的判断
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Th1:&lt;/strong&gt; (凹凸性的判别定理）若在I上$f&amp;rsquo;&amp;rsquo;(x)&amp;lt;0$（或$f&amp;rsquo;&amp;rsquo;(x)&amp;gt;0$），则$f(x)$在I上是凸的（或凹的）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th2:&lt;/strong&gt; (拐点的判别定理1)若在${x_{0}}$处$f&amp;rsquo;&amp;rsquo;(x)=0$，（或$f&amp;rsquo;&amp;rsquo;(x)$不存在），当$x$变动经过${x_{0}}$时，$f&amp;rsquo;&amp;rsquo;(x)$变号，则$({x_{0}},f({x_{0}}))$为拐点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Th3:&lt;/strong&gt; (拐点的判别定理2)设$f(x)$在${x_{0}}$点的某邻域内有三阶导数，且$f&amp;rsquo;&amp;rsquo;(x)=0$，$f&amp;rsquo;&amp;rsquo;&amp;rsquo;(x)\ne 0$，则$({x_{0}},f({x_{0}}))$为拐点。&lt;/p&gt;
&lt;h2 id=&#34;弧微分&#34;&gt;弧微分
&lt;/h2&gt;&lt;p&gt;$dS=\sqrt{1+y&amp;rsquo;^{2}}dx$&lt;/p&gt;
&lt;h2 id=&#34;曲率&#34;&gt;曲率
&lt;/h2&gt;&lt;p&gt;曲线$y=f(x)$在点$(x,y)$处的曲率$k=\frac{\left| y&amp;rsquo;&amp;rsquo; \right|}{(1+y&amp;rsquo;^{2})^{\tfrac{3}{2}}}$。
对于参数方程$\left{\begin{matrix}x=\varphi(t) \  y=\psi (t) \end{matrix}\right.,$
$k=\frac{\left| \varphi &amp;lsquo;(t)\psi &amp;lsquo;&amp;rsquo;(t)-\varphi &amp;lsquo;&amp;rsquo;(t)\psi &amp;lsquo;(t) \right|}{[\varphi &amp;lsquo;^{2}(t)+\psi &amp;lsquo;^{2}(t)]^{\tfrac{3}{2}}}$。&lt;/p&gt;
&lt;h2 id=&#34;曲率半径&#34;&gt;曲率半径
&lt;/h2&gt;&lt;p&gt;曲线在点$M$处的曲率$k(k\ne 0)$与曲线在点$M$处的曲率半径$\rho$有如下关系：$\rho =\frac{1}{k}$。&lt;/p&gt;
&lt;h1 id=&#34;线性代数&#34;&gt;线性代数
&lt;/h1&gt;&lt;h2 id=&#34;行列式&#34;&gt;行列式
&lt;/h2&gt;&lt;h3 id=&#34;行列式按行列展开定理&#34;&gt;行列式按行（列）展开定理
&lt;/h3&gt;&lt;p&gt;(1) 设$A = ( a_{{ij}} )&lt;em&gt;{n \times n}$，则：$a&lt;/em&gt;{i1}A_{j1} +a_{i2}A_{j2} + \cdots + a_{{in}}A_{{jn}} = \begin{cases}|A|,i=j\ 0,i \neq j\end{cases}$&lt;/p&gt;
&lt;p&gt;或$a_{1i}A_{1j} + a_{2i}A_{2j} + \cdots + a_{{ni}}A_{{nj}} = \begin{cases}|A|,i=j\ 0,i \neq j\end{cases}$即 $AA^{&lt;em&gt;} = A^{&lt;/em&gt;}A = \left| A \right|E,$其中：$A^{*} = \begin{pmatrix} A_{11} &amp;amp; A_{12} &amp;amp; \ldots &amp;amp; A_{1n} \ A_{21} &amp;amp; A_{22} &amp;amp; \ldots &amp;amp; A_{2n} \ \ldots &amp;amp; \ldots &amp;amp; \ldots &amp;amp; \ldots \ A_{n1} &amp;amp; A_{n2} &amp;amp; \ldots &amp;amp; A_{{nn}} \ \end{pmatrix} = (A_{{ji}}) = {(A_{{ij}})}^{T}$&lt;/p&gt;
&lt;p&gt;$D_{n} = \begin{vmatrix} 1 &amp;amp; 1 &amp;amp; \ldots &amp;amp; 1 \ x_{1} &amp;amp; x_{2} &amp;amp; \ldots &amp;amp; x_{n} \ \ldots &amp;amp; \ldots &amp;amp; \ldots &amp;amp; \ldots \ x_{1}^{n - 1} &amp;amp; x_{2}^{n - 1} &amp;amp; \ldots &amp;amp; x_{n}^{n - 1} \ \end{vmatrix} = \prod_{1 \leq j &amp;lt; i \leq n}^{},(x_{i} - x_{j})$&lt;/p&gt;
&lt;p&gt;(2) 设$A,B$为$n$阶方阵，则$\left| {AB} \right| = \left| A \right|\left| B \right| = \left| B \right|\left| A \right| = \left| {BA} \right|$，但$\left| A \pm B \right| = \left| A \right| \pm \left| B \right|$不一定成立。&lt;/p&gt;
&lt;p&gt;(3) $\left| {kA} \right| = k^{n}\left| A \right|$,$A$为$n$阶方阵。&lt;/p&gt;
&lt;p&gt;(4) 设$A$为$n$阶方阵，$|A^{T}| = |A|;|A^{- 1}| = |A|^{- 1}$（若$A$可逆），$|A^{*}| = |A|^{n - 1}$&lt;/p&gt;
&lt;p&gt;$n \geq 2$&lt;/p&gt;
&lt;p&gt;(5) $\left| \begin{matrix}  &amp;amp; {A\quad O} \  &amp;amp; {O\quad B} \ \end{matrix} \right| = \left| \begin{matrix}  &amp;amp; {A\quad C} \  &amp;amp; {O\quad B} \ \end{matrix} \right| = \left| \begin{matrix}  &amp;amp; {A\quad O} \  &amp;amp; {C\quad B} \ \end{matrix} \right| =| A||B|$
，$A,B$为方阵，但$\left| \begin{matrix} {O} &amp;amp; A_{m \times m} \  B_{n \times n} &amp;amp; { O} \ \end{matrix} \right| = ({- 1)}^{{mn}}|A||B|$ 。&lt;/p&gt;
&lt;p&gt;(6) 范德蒙行列式$D_{n} = \begin{vmatrix} 1 &amp;amp; 1 &amp;amp; \ldots &amp;amp; 1 \ x_{1} &amp;amp; x_{2} &amp;amp; \ldots &amp;amp; x_{n} \ \ldots &amp;amp; \ldots &amp;amp; \ldots &amp;amp; \ldots \ x_{1}^{n - 1} &amp;amp; x_{2}^{n 1} &amp;amp; \ldots &amp;amp; x_{n}^{n - 1} \ \end{vmatrix} =  \prod_{1 \leq j &amp;lt; i \leq n}^{},(x_{i} - x_{j})$&lt;/p&gt;
&lt;p&gt;设$A$是$n$阶方阵，$\lambda_{i}(i = 1,2\cdots,n)$是$A$的$n$个特征值，则
$|A| = \prod_{i = 1}^{n}\lambda_{i}$&lt;/p&gt;
&lt;h2 id=&#34;矩阵&#34;&gt;矩阵
&lt;/h2&gt;&lt;p&gt;矩阵：$m \times n$个数$a_{{ij}}$排成$m$行$n$列的表格$\begin{bmatrix}  a_{11}\quad a_{12}\quad\cdots\quad a_{1n} \ a_{21}\quad a_{22}\quad\cdots\quad a_{2n} \ \quad\cdots\cdots\cdots\cdots\cdots \  a_{m1}\quad a_{m2}\quad\cdots\quad a_{{mn}} \ \end{bmatrix}$ 称为矩阵，简记为$A$，或者$\left( a_{{ij}} \right)_{m \times n}$ 。若$m = n$，则称$A$是$n$阶矩阵或$n$阶方阵。&lt;/p&gt;
&lt;h3 id=&#34;矩阵的线性运算&#34;&gt;矩阵的线性运算
&lt;/h3&gt;&lt;h4 id=&#34;矩阵的加法&#34;&gt;矩阵的加法
&lt;/h4&gt;&lt;p&gt;设$A = (a_{{ij}}),B = (b_{{ij}})$是两个$m \times n$矩阵，则$m \times n$ 矩阵$C = c_{{ij}}) = a_{{ij}} + b_{{ij}}$称为矩阵$A$与$B$的和，记为$A + B = C$ 。&lt;/p&gt;
&lt;h4 id=&#34;矩阵的数乘&#34;&gt;矩阵的数乘
&lt;/h4&gt;&lt;p&gt;设$A = (a_{{ij}})$是$m \times n$矩阵，$k$是一个常数，则$m \times n$矩阵$(ka_{{ij}})$称为数$k$与矩阵$A$的数乘，记为${kA}$。&lt;/p&gt;
&lt;h4 id=&#34;矩阵的乘法&#34;&gt;矩阵的乘法
&lt;/h4&gt;&lt;p&gt;设$A = (a_{{ij}})$是$m \times n$矩阵，$B = (b_{{ij}})$是$n \times s$矩阵，那么$m \times s$矩阵$C = (c_{{ij}})$，其中$c_{{ij}} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{{in}}b_{{nj}} = \sum_{k =1}^{n}{a_{{ik}}b_{{kj}}}$称为${AB}$的乘积，记为$C = AB$ 。&lt;/p&gt;
&lt;h3 id=&#34;mathbfamathbftmathbfamathbf-1mathbfamathbf三者之间的关系&#34;&gt;$\mathbf{A}^{\mathbf{T}}$&lt;strong&gt;、&lt;/strong&gt;$\mathbf{A}^{\mathbf{-1}}$&lt;strong&gt;、&lt;/strong&gt;$\mathbf{A}^{\mathbf{*}}$&lt;strong&gt;三者之间的关系&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;(1) ${(A^{T})}^{T} = A,{(AB)}^{T} = B^{T}A^{T},{(kA)}^{T} = kA^{T},{(A \pm B)}^{T} = A^{T} \pm B^{T}$&lt;/p&gt;
&lt;p&gt;(2) $\left( A^{- 1} \right)^{- 1} = A,\left( {AB} \right)^{- 1} = B^{- 1}A^{- 1},\left( {kA} \right)^{- 1} = \frac{1}{k}A^{- 1},$&lt;/p&gt;
&lt;p&gt;但 ${(A \pm B)}^{- 1} = A^{- 1} \pm B^{- 1}$不一定成立。&lt;/p&gt;
&lt;p&gt;(3) $\left( A^{&lt;em&gt;} \right)^{&lt;/em&gt;} = |A|^{n - 2}\ A\ \ (n \geq 3)$，$\left({AB} \right)^{&lt;em&gt;} = B^{&lt;/em&gt;}A^{&lt;em&gt;},$ $\left( {kA} \right)^{&lt;/em&gt;} = k^{n -1}A^{*}{\ \ }\left( n \geq 2 \right)$&lt;/p&gt;
&lt;p&gt;但$\left( A \pm B \right)^{&lt;em&gt;} = A^{&lt;/em&gt;} \pm B^{*}$不一定成立。&lt;/p&gt;
&lt;p&gt;(4) ${(A^{- 1})}^{T} = {(A^{T})}^{- 1},\ \left( A^{- 1} \right)^{&lt;em&gt;} ={(AA^{&lt;/em&gt;})}^{- 1},{(A^{&lt;em&gt;})}^{T} = \left( A^{T} \right)^{&lt;/em&gt;}$&lt;/p&gt;
&lt;h3 id=&#34;有关mathbfamathbf的结论&#34;&gt;有关**$\mathbf{A}^{\mathbf{*}}$&lt;strong&gt;的结论&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;(1) $AA^{&lt;em&gt;} = A^{&lt;/em&gt;}A = |A|E$&lt;/p&gt;
&lt;p&gt;(2) $|A^{&lt;em&gt;}| = |A|^{n - 1}\ (n \geq 2),\ \ \ \ (kA)^{&lt;/em&gt;} = k^{n -1}A^{&lt;em&gt;},{\ \ }\left( A^{&lt;/em&gt;} \right)^{*} = |A|^{n - 2}A(n \geq 3)$&lt;/p&gt;
&lt;p&gt;(3) 若$A$可逆，则$A^{&lt;em&gt;} = |A|A^{- 1},(A^{&lt;/em&gt;})^{*} = \frac{1}{|A|}A$&lt;/p&gt;
&lt;p&gt;(4) 若$A$为$n$阶方阵，则：&lt;/p&gt;
&lt;p&gt;$r(A^*)=\begin{cases}n,\quad r(A)=n\ 1,\quad r(A)=n-1\ 0,\quad r(A)&amp;lt;n-1\end{cases}$&lt;/p&gt;
&lt;h3 id=&#34;有关mathbfamathbf--1的结论&#34;&gt;有关**$\mathbf{A}^{\mathbf{- 1}}$&lt;strong&gt;的结论&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;$A$可逆$\Leftrightarrow AB = E; \Leftrightarrow |A| \neq 0; \Leftrightarrow r(A) = n;$&lt;/p&gt;
&lt;p&gt;$\Leftrightarrow A$可以表示为初等矩阵的乘积；$\Leftrightarrow Ax = 0$只有零解。&lt;/p&gt;
&lt;h3 id=&#34;有关矩阵秩的结论&#34;&gt;有关矩阵秩的结论**
&lt;/h3&gt;&lt;p&gt;(1) 秩$r(A)$=行秩=列秩；&lt;/p&gt;
&lt;p&gt;(2) $r(A_{m \times n}) \leq \min(m,n);$&lt;/p&gt;
&lt;p&gt;(3) $A \neq 0 \Rightarrow r(A) \geq 1$；&lt;/p&gt;
&lt;p&gt;(4) $r(A \pm B) \leq r(A) + r(B);$&lt;/p&gt;
&lt;p&gt;(5) 初等变换不改变矩阵的秩&lt;/p&gt;
&lt;p&gt;(6) $r(A) + r(B) - n \leq r(AB) \leq \min(r(A),r(B)),$特别若$AB = O$
则：$r(A) + r(B) \leq n$&lt;/p&gt;
&lt;p&gt;(7) 若$A^{- 1}$存在$\Rightarrow r(AB) = r(B);$ 若$B^{- 1}$存在
$\Rightarrow r(AB) = r(A);$&lt;/p&gt;
&lt;p&gt;若$r(A_{m \times n}) = n \Rightarrow r(AB) = r(B);$ 若$r(A_{m \times s}) = n\Rightarrow r(AB) = r\left( A \right)$。&lt;/p&gt;
&lt;p&gt;(8) $r(A_{m \times s}) = n \Leftrightarrow Ax = 0$只有零解&lt;/p&gt;
&lt;h3 id=&#34;分块求逆公式&#34;&gt;分块求逆公式**
&lt;/h3&gt;&lt;p&gt;$\begin{pmatrix} A &amp;amp; O \ O &amp;amp; B \ \end{pmatrix}^{- 1} = \begin{pmatrix} A^{-1} &amp;amp; O \ O &amp;amp; B^{- 1} \ \end{pmatrix}$； $\begin{pmatrix} A &amp;amp; C \ O &amp;amp; B \\end{pmatrix}^{- 1} = \begin{pmatrix} A^{- 1}&amp;amp; - A^{- 1}CB^{- 1} \ O &amp;amp; B^{- 1} \ \end{pmatrix}$；&lt;/p&gt;
&lt;p&gt;$\begin{pmatrix} A &amp;amp; O \ C &amp;amp; B \ \end{pmatrix}^{- 1} = \begin{pmatrix}  A^{- 1}&amp;amp;{O} \   - B^{- 1}CA^{- 1} &amp;amp; B^{- 1} \\end{pmatrix}$； $\begin{pmatrix} O &amp;amp; A \ B &amp;amp; O \ \end{pmatrix}^{- 1} =\begin{pmatrix} O &amp;amp; B^{- 1} \ A^{- 1} &amp;amp; O \ \end{pmatrix}$&lt;/p&gt;
&lt;p&gt;这里$A$，$B$均为可逆方阵。&lt;/p&gt;
&lt;h2 id=&#34;向量&#34;&gt;向量
&lt;/h2&gt;&lt;h3 id=&#34;有关向量组的线性表示&#34;&gt;有关向量组的线性表示
&lt;/h3&gt;&lt;p&gt;(1)$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性相关$\Leftrightarrow$至少有一个向量可以用其余向量线性表示。&lt;/p&gt;
&lt;p&gt;(2)$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，$\beta$线性相关$\Leftrightarrow \beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$唯一线性表示。&lt;/p&gt;
&lt;p&gt;(3) $\beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性表示
$\Leftrightarrow r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s}) =r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s},\beta)$ 。&lt;/p&gt;
&lt;h3 id=&#34;有关向量组的线性相关性&#34;&gt;有关向量组的线性相关性
&lt;/h3&gt;&lt;p&gt;(1)部分相关，整体相关；整体无关，部分无关.&lt;/p&gt;
&lt;p&gt;(2) ① $n$个$n$维向量
$\alpha_{1},\alpha_{2}\cdots\alpha_{n}$线性无关$\Leftrightarrow \left|\left\lbrack \alpha_{1}\alpha_{2}\cdots\alpha_{n} \right\rbrack \right| \neq0$， $n$个$n$维向量$\alpha_{1},\alpha_{2}\cdots\alpha_{n}$线性相关
$\Leftrightarrow |\lbrack\alpha_{1},\alpha_{2},\cdots,\alpha_{n}\rbrack| = 0$
。&lt;/p&gt;
&lt;p&gt;② $n + 1$个$n$维向量线性相关。&lt;/p&gt;
&lt;p&gt;③ 若$\alpha_{1},\alpha_{2}\cdots\alpha_{S}$线性无关，则添加分量后仍线性无关；或一组向量线性相关，去掉某些分量后仍线性相关。&lt;/p&gt;
&lt;h3 id=&#34;有关向量组的线性表示-1&#34;&gt;有关向量组的线性表示
&lt;/h3&gt;&lt;p&gt;(1) $\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性相关$\Leftrightarrow$至少有一个向量可以用其余向量线性表示。&lt;/p&gt;
&lt;p&gt;(2) $\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，$\beta$线性相关$\Leftrightarrow\beta$ 可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$唯一线性表示。&lt;/p&gt;
&lt;p&gt;(3) $\beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性表示
$\Leftrightarrow r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s}) =r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s},\beta)$&lt;/p&gt;
&lt;h3 id=&#34;向量组的秩与矩阵的秩之间的关系&#34;&gt;向量组的秩与矩阵的秩之间的关系
&lt;/h3&gt;&lt;p&gt;设$r(A_{m \times n}) =r$，则$A$的秩$r(A)$与$A$的行列向量组的线性相关性关系为：&lt;/p&gt;
&lt;p&gt;(1) 若$r(A_{m \times n}) = r = m$，则$A$的行向量组线性无关。&lt;/p&gt;
&lt;p&gt;(2) 若$r(A_{m \times n}) = r &amp;lt; m$，则$A$的行向量组线性相关。&lt;/p&gt;
&lt;p&gt;(3) 若$r(A_{m \times n}) = r = n$，则$A$的列向量组线性无关。&lt;/p&gt;
&lt;p&gt;(4) 若$r(A_{m \times n}) = r &amp;lt; n$，则$A$的列向量组线性相关。&lt;/p&gt;
&lt;h3 id=&#34;mathbfn维向量空间的基变换公式及过渡矩阵&#34;&gt;$\mathbf{n}$**维向量空间的基变换公式及过渡矩阵
&lt;/h3&gt;&lt;p&gt;若$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$与$\beta_{1},\beta_{2},\cdots,\beta_{n}$是向量空间$V$的两组基，则基变换公式为：&lt;/p&gt;
&lt;p&gt;$(\beta_{1},\beta_{2},\cdots,\beta_{n}) = (\alpha_{1},\alpha_{2},\cdots,\alpha_{n})\begin{bmatrix}  c_{11}&amp;amp; c_{12}&amp;amp; \cdots &amp;amp; c_{1n} \  c_{21}&amp;amp; c_{22}&amp;amp;\cdots &amp;amp; c_{2n} \ \cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots \  c_{n1}&amp;amp; c_{n2} &amp;amp; \cdots &amp;amp; c_{{nn}} \\end{bmatrix} = (\alpha_{1},\alpha_{2},\cdots,\alpha_{n})C$&lt;/p&gt;
&lt;p&gt;其中$C$是可逆矩阵，称为由基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$到基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的过渡矩阵。&lt;/p&gt;
&lt;h3 id=&#34;坐标变换公式&#34;&gt;坐标变换公式
&lt;/h3&gt;&lt;p&gt;若向量$\gamma$在基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$与基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的坐标分别是
$X = {(x_{1},x_{2},\cdots,x_{n})}^{T}$，&lt;/p&gt;
&lt;p&gt;$Y = \left( y_{1},y_{2},\cdots,y_{n} \right)^{T}$ 即： $\gamma =x_{1}\alpha_{1} + x_{2}\alpha_{2} + \cdots + x_{n}\alpha_{n} = y_{1}\beta_{1} +y_{2}\beta_{2} + \cdots + y_{n}\beta_{n}$，则向量坐标变换公式为$X = CY$ 或$Y = C^{- 1}X$，其中$C$是从基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$到基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的过渡矩阵。&lt;/p&gt;
&lt;h3 id=&#34;向量的内积&#34;&gt;向量的内积
&lt;/h3&gt;&lt;p&gt;$(\alpha,\beta) = a_{1}b_{1} + a_{2}b_{2} + \cdots + a_{n}b_{n} = \alpha^{T}\beta = \beta^{T}\alpha$&lt;/p&gt;
&lt;h3 id=&#34;8schmidt正交化&#34;&gt;8.Schmidt正交化
&lt;/h3&gt;&lt;p&gt;若$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，则可构造$\beta_{1},\beta_{2},\cdots,\beta_{s}$使其两两正交，且$\beta_{i}$仅是$\alpha_{1},\alpha_{2},\cdots,\alpha_{i}$的线性组合$(i= 1,2,\cdots,n)$，再把$\beta_{i}$单位化，记$\gamma_{i} =\frac{\beta_{i}}{\left| \beta_{i}\right|}$，则$\gamma_{1},\gamma_{2},\cdots,\gamma_{i}$是规范正交向量组。其中
$\beta_{1} = \alpha_{1}$， $\beta_{2} = \alpha_{2} -\frac{(\alpha_{2},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}$ ， $\beta_{3} =\alpha_{3} - \frac{(\alpha_{3},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1} -\frac{(\alpha_{3},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2}$ ，&lt;/p&gt;
&lt;p&gt;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;$\beta_{s} = \alpha_{s} - \frac{(\alpha_{s},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1} - \frac{(\alpha_{s},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2} - \cdots - \frac{(\alpha_{s},\beta_{s - 1})}{(\beta_{s - 1},\beta_{s - 1})}\beta_{s - 1}$&lt;/p&gt;
&lt;h3 id=&#34;正交基及规范正交基&#34;&gt;正交基及规范正交基
&lt;/h3&gt;&lt;p&gt;向量空间一组基中的向量如果两两正交，就称为正交基；若正交基中每个向量都是单位向量，就称其为规范正交基。&lt;/p&gt;
&lt;h2 id=&#34;线性方程组&#34;&gt;线性方程组
&lt;/h2&gt;&lt;h3 id=&#34;克莱姆法则&#34;&gt;克莱姆法则
&lt;/h3&gt;&lt;p&gt;线性方程组$\begin{cases}  a_{11}x_{1} + a_{12}x_{2} + \cdots +a_{1n}x_{n} = b_{1} \   a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} =b_{2} \   \quad\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots \ a_{n1}x_{1} + a_{n2}x_{2} + \cdots + a_{{nn}}x_{n} = b_{n} \ \end{cases}$，如果系数行列式$D = \left| A \right| \neq 0$，则方程组有唯一解，$x_{1} = \frac{D_{1}}{D},x_{2} = \frac{D_{2}}{D},\cdots,x_{n} =\frac{D_{n}}{D}$，其中$D_{j}$是把$D$中第$j$列元素换成方程组右端的常数列所得的行列式。&lt;/p&gt;
&lt;h3 id=&#34;规律&#34;&gt;规律
&lt;/h3&gt;&lt;p&gt;$n$阶矩阵$A$可逆$\Leftrightarrow Ax = 0$只有零解。$\Leftrightarrow\forall b,Ax = b$总有唯一解，一般地，$r(A_{m \times n}) = n \Leftrightarrow Ax= 0$只有零解。&lt;/p&gt;
&lt;h3 id=&#34;非奇次线性方程组有解的充分必要条件线性方程组解的性质和解的结构&#34;&gt;非奇次线性方程组有解的充分必要条件，线性方程组解的性质和解的结构
&lt;/h3&gt;&lt;p&gt;(1) 设$A$为$m \times n$矩阵，若$r(A_{m \times n}) = m$，则对$Ax =b$而言必有$r(A) = r(A \vdots b) = m$，从而$Ax = b$有解。&lt;/p&gt;
&lt;p&gt;(2) 设$x_{1},x_{2},\cdots x_{s}$为$Ax = b$的解，则$k_{1}x_{1} + k_{2}x_{2}\cdots + k_{s}x_{s}$当$k_{1} + k_{2} + \cdots + k_{s} = 1$时仍为$Ax =b$的解；但当$k_{1} + k_{2} + \cdots + k_{s} = 0$时，则为$Ax =0$的解。特别$\frac{x_{1} + x_{2}}{2}$为$Ax = b$的解；$2x_{3} - (x_{1} +x_{2})$为$Ax = 0$的解。&lt;/p&gt;
&lt;p&gt;(3) 非齐次线性方程组${Ax} = b$无解$\Leftrightarrow r(A) + 1 =r(\overline{A}) \Leftrightarrow b$不能由$A$的列向量$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$线性表示。&lt;/p&gt;
&lt;h3 id=&#34;奇次线性方程组的基础解系和通解解空间非奇次线性方程组的通解&#34;&gt;奇次线性方程组的基础解系和通解，解空间，非奇次线性方程组的通解
&lt;/h3&gt;&lt;p&gt;(1) 齐次方程组${Ax} = 0$恒有解(必有零解)。当有非零解时，由于解向量的任意线性组合仍是该齐次方程组的解向量，因此${Ax}= 0$的全体解向量构成一个向量空间，称为该方程组的解空间，解空间的维数是$n - r(A)$，解空间的一组基称为齐次方程组的基础解系。&lt;/p&gt;
&lt;p&gt;(2) $\eta_{1},\eta_{2},\cdots,\eta_{t}$是${Ax} = 0$的基础解系，即：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\eta_{1},\eta_{2},\cdots,\eta_{t}$是${Ax} = 0$的解；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\eta_{1},\eta_{2},\cdots,\eta_{t}$线性无关；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;${Ax} = 0$的任一解都可以由$\eta_{1},\eta_{2},\cdots,\eta_{t}$线性表出.
$k_{1}\eta_{1} + k_{2}\eta_{2} + \cdots + k_{t}\eta_{t}$是${Ax} = 0$的通解，其中$k_{1},k_{2},\cdots,k_{t}$是任意常数。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;矩阵的特征值和特征向量&#34;&gt;矩阵的特征值和特征向量
&lt;/h2&gt;&lt;h3 id=&#34;矩阵的特征值和特征向量的概念及性质&#34;&gt;矩阵的特征值和特征向量的概念及性质
&lt;/h3&gt;&lt;p&gt;(1) 设$\lambda$是$A$的一个特征值，则 ${kA},{aA} + {bE},A^{2},A^{m},f(A),A^{T},A^{- 1},A^{*}$有一个特征值分别为
${kλ},{aλ} + b,\lambda^{2},\lambda^{m},f(\lambda),\lambda,\lambda^{- 1},\frac{|A|}{\lambda},$且对应特征向量相同（$A^{T}$ 例外）。&lt;/p&gt;
&lt;p&gt;(2)若$\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$为$A$的$n$个特征值，则$\sum_{i= 1}^{n}\lambda_{i} = \sum_{i = 1}^{n}a_{{ii}},\prod_{i = 1}^{n}\lambda_{i}= |A|$ ,从而$|A| \neq 0 \Leftrightarrow A$没有特征值。&lt;/p&gt;
&lt;p&gt;(3)设$\lambda_{1},\lambda_{2},\cdots,\lambda_{s}$为$A$的$s$个特征值，对应特征向量为$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，&lt;/p&gt;
&lt;p&gt;若: $\alpha = k_{1}\alpha_{1} + k_{2}\alpha_{2} + \cdots + k_{s}\alpha_{s}$ ,&lt;/p&gt;
&lt;p&gt;则: $A^{n}\alpha = k_{1}A^{n}\alpha_{1} + k_{2}A^{n}\alpha_{2} + \cdots +k_{s}A^{n}\alpha_{s} = k_{1}\lambda_{1}^{n}\alpha_{1} +k_{2}\lambda_{2}^{n}\alpha_{2} + \cdots k_{s}\lambda_{s}^{n}\alpha_{s}$ 。&lt;/p&gt;
&lt;h3 id=&#34;相似变换相似矩阵的概念及性质&#34;&gt;相似变换、相似矩阵的概念及性质
&lt;/h3&gt;&lt;p&gt;(1) 若$A \sim B$，则&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$A^{T} \sim B^{T},A^{- 1} \sim B^{- 1},,A^{&lt;em&gt;} \sim B^{&lt;/em&gt;}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$|A| = |B|,\sum_{i = 1}^{n}A_{{ii}} = \sum_{i =1}^{n}b_{{ii}},r(A) = r(B)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$|\lambda E - A| = |\lambda E - B|$，对$\forall\lambda$成立&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;矩阵可相似对角化的充分必要条件&#34;&gt;矩阵可相似对角化的充分必要条件
&lt;/h3&gt;&lt;p&gt;(1)设$A$为$n$阶方阵，则$A$可对角化$\Leftrightarrow$对每个$k_{i}$重根特征值$\lambda_{i}$，有$n-r(\lambda_{i}E - A) = k_{i}$&lt;/p&gt;
&lt;p&gt;(2) 设$A$可对角化，则由$P^{- 1}{AP} = \Lambda,$有$A = {PΛ}P^{-1}$，从而$A^{n} = P\Lambda^{n}P^{- 1}$&lt;/p&gt;
&lt;p&gt;(3) 重要结论&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;若$A \sim B,C \sim D$，则$\begin{bmatrix}  A &amp;amp; O \ O &amp;amp; C \\end{bmatrix} \sim \begin{bmatrix} B &amp;amp; O \  O &amp;amp; D \\end{bmatrix}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若$A \sim B$，则$f(A) \sim f(B),\left| f(A) \right| \sim \left| f(B)\right|$，其中$f(A)$为关于$n$阶方阵$A$的多项式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若$A$为可对角化矩阵，则其非零特征值的个数(重根重复计算)＝秩($A$)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;实对称矩阵的特征值特征向量及相似对角阵&#34;&gt;实对称矩阵的特征值、特征向量及相似对角阵
&lt;/h3&gt;&lt;p&gt;(1)相似矩阵：设$A,B$为两个$n$阶方阵，如果存在一个可逆矩阵$P$，使得$B =P^{- 1}{AP}$成立，则称矩阵$A$与$B$相似，记为$A \sim B$。&lt;/p&gt;
&lt;p&gt;(2)相似矩阵的性质：如果$A \sim B$则有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$A^{T} \sim B^{T}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A^{- 1} \sim B^{- 1}$ （若$A$，$B$均可逆）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A^{k} \sim B^{k}$ （$k$为正整数）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\left| {λE} - A \right| = \left| {λE} - B \right|$，从而$A,B$
有相同的特征值&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\left| A \right| = \left| B \right|$，从而$A,B$同时可逆或者不可逆&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;秩$\left( A \right) =$秩$\left( B \right),\left| {λE} - A \right| =\left| {λE} - B \right|$，$A,B$不一定相似&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;二次型&#34;&gt;二次型
&lt;/h2&gt;&lt;h3 id=&#34;mathbfn个变量mathbfx_mathbf1mathbfmathbfx_mathbf2mathbfcdotsmathbfx_mathbfn的二次齐次函数&#34;&gt;$\mathbf{n}$&lt;strong&gt;个变量&lt;/strong&gt;$\mathbf{x}&lt;em&gt;{\mathbf{1}}\mathbf{,}\mathbf{x}&lt;/em&gt;{\mathbf{2}}\mathbf{,\cdots,}\mathbf{x}_{\mathbf{n}}$**的二次齐次函数
&lt;/h3&gt;&lt;p&gt;$f(x_{1},x_{2},\cdots,x_{n}) = \sum_{i = 1}^{n}{\sum_{j =1}^{n}{a_{{ij}}x_{i}y_{j}}}$，其中$a_{{ij}} = a_{{ji}}(i,j =1,2,\cdots,n)$，称为$n$元二次型，简称二次型. 若令$x = \ \begin{bmatrix}x_{1} \ x_{1} \  \vdots \ x_{n} \ \end{bmatrix},A = \begin{bmatrix}  a_{11}&amp;amp; a_{12}&amp;amp; \cdots &amp;amp; a_{1n} \  a_{21}&amp;amp; a_{22}&amp;amp; \cdots &amp;amp; a_{2n} \ \cdots &amp;amp;\cdots &amp;amp;\cdots &amp;amp;\cdots \  a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{{nn}} \\end{bmatrix}$,这二次型$f$可改写成矩阵向量形式$f =x^{T}{Ax}$。其中$A$称为二次型矩阵，因为$a_{{ij}} =a_{{ji}}(i,j =1,2,\cdots,n)$，所以二次型矩阵均为对称矩阵，且二次型与对称矩阵一一对应，并把矩阵$A$的秩称为二次型的秩。&lt;/p&gt;
&lt;h3 id=&#34;惯性定理二次型的标准形和规范形&#34;&gt;惯性定理，二次型的标准形和规范形
&lt;/h3&gt;&lt;p&gt;(1) 惯性定理&lt;/p&gt;
&lt;p&gt;对于任一二次型，不论选取怎样的合同变换使它化为仅含平方项的标准型，其正负惯性指数与所选变换无关，这就是所谓的惯性定理。&lt;/p&gt;
&lt;p&gt;(2) 标准形&lt;/p&gt;
&lt;p&gt;二次型$f = \left( x_{1},x_{2},\cdots,x_{n} \right) =x^{T}{Ax}$经过合同变换$x = {Cy}$化为$f = x^{T}{Ax} =y^{T}C^{T}{AC}$&lt;/p&gt;
&lt;p&gt;$y = \sum_{i = 1}^{r}{d_{i}y_{i}^{2}}$称为 $f(r \leq n)$的标准形。在一般的数域内，二次型的标准形不是唯一的，与所作的合同变换有关，但系数不为零的平方项的个数由$r(A)$唯一确定。&lt;/p&gt;
&lt;p&gt;(3) 规范形&lt;/p&gt;
&lt;p&gt;任一实二次型$f$都可经过合同变换化为规范形$f = z_{1}^{2} + z_{2}^{2} + \cdots z_{p}^{2} - z_{p + 1}^{2} - \cdots -z_{r}^{2}$，其中$r$为$A$的秩，$p$为正惯性指数，$r -p$为负惯性指数，且规范型唯一。&lt;/p&gt;
&lt;h3 id=&#34;用正交变换和配方法化二次型为标准形二次型及其矩阵的正定性&#34;&gt;用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性
&lt;/h3&gt;&lt;p&gt;设$A$正定$\Rightarrow {kA}(k &amp;gt; 0),A^{T},A^{- 1},A^{*}$正定；$|A| &amp;gt;0$,$A$可逆；$a_{{ii}} &amp;gt; 0$，且$|A_{{ii}}| &amp;gt; 0$&lt;/p&gt;
&lt;p&gt;$A$，$B$正定$\Rightarrow A +B$正定，但${AB}$，${BA}$不一定正定&lt;/p&gt;
&lt;p&gt;$A$正定$\Leftrightarrow f(x) = x^{T}{Ax} &amp;gt; 0,\forall x \neq 0$&lt;/p&gt;
&lt;p&gt;$\Leftrightarrow A$的各阶顺序主子式全大于零&lt;/p&gt;
&lt;p&gt;$\Leftrightarrow A$的所有特征值大于零&lt;/p&gt;
&lt;p&gt;$\Leftrightarrow A$的正惯性指数为$n$&lt;/p&gt;
&lt;p&gt;$\Leftrightarrow$存在可逆阵$P$使$A = P^{T}P$&lt;/p&gt;
&lt;p&gt;$\Leftrightarrow$存在正交矩阵$Q$，使$Q^{T}{AQ} = Q^{- 1}{AQ} =\begin{pmatrix} \lambda_{1} &amp;amp; &amp;amp; \ \begin{matrix}  &amp;amp; \  &amp;amp; \ \end{matrix} &amp;amp;\ddots &amp;amp; \  &amp;amp; &amp;amp; \lambda_{n} \ \end{pmatrix},$&lt;/p&gt;
&lt;p&gt;其中$\lambda_{i} &amp;gt; 0,i = 1,2,\cdots,n.$正定$\Rightarrow {kA}(k &amp;gt;0),A^{T},A^{- 1},A^{*}$正定； $|A| &amp;gt; 0,A$可逆；$a_{{ii}} &amp;gt;0$，且$|A_{{ii}}| &amp;gt; 0$ 。&lt;/p&gt;
&lt;h1 id=&#34;概率论和数理统计&#34;&gt;概率论和数理统计
&lt;/h1&gt;&lt;h2 id=&#34;随机事件和概率&#34;&gt;随机事件和概率
&lt;/h2&gt;&lt;h3 id=&#34;事件的关系与运算&#34;&gt;事件的关系与运算
&lt;/h3&gt;&lt;p&gt;(1) 子事件：$A \subset B$，若$A$发生，则$B$发生。&lt;/p&gt;
&lt;p&gt;(2) 相等事件：$A = B$，即$A \subset B$，且$B \subset A$ 。&lt;/p&gt;
&lt;p&gt;(3) 和事件：$A\bigcup B$（或$A + B$），$A$与$B$中至少有一个发生。&lt;/p&gt;
&lt;p&gt;(4) 差事件：$A - B$，$A$发生但$B$不发生。&lt;/p&gt;
&lt;p&gt;(5) 积事件：$A\bigcap B$（或${AB}$），$A$与$B$同时发生。&lt;/p&gt;
&lt;p&gt;(6) 互斥事件（互不相容）：$A\bigcap B$=$\varnothing$。&lt;/p&gt;
&lt;p&gt;(7) 互逆事件（对立事件）：
$A\bigcap B=\varnothing ,A\bigcup B=\Omega ,A=\bar{B},B=\bar{A}$&lt;/p&gt;
&lt;h3 id=&#34;运算律&#34;&gt;运算律
&lt;/h3&gt;&lt;p&gt;(1) 交换律：$A\bigcup B=B\bigcup A,A\bigcap B=B\bigcap A$
(2) 结合律：$(A\bigcup B)\bigcup C=A\bigcup (B\bigcup C)$
(3) 分配律：$(A\bigcap B)\bigcap C=A\bigcap (B\bigcap C)$&lt;/p&gt;
&lt;h3 id=&#34;德centerdot摩根律&#34;&gt;德$\centerdot$摩根律
&lt;/h3&gt;&lt;p&gt;$\overline{A\bigcup B}=\bar{A}\bigcap \bar{B}$                 $\overline{A\bigcap B}=\bar{A}\bigcup \bar{B}$&lt;/p&gt;
&lt;h3 id=&#34;完全事件组&#34;&gt;完全事件组
&lt;/h3&gt;&lt;p&gt;$A_{1}A_{2}\cdots A_{n}$两两互斥，且和事件为必然事件，即${A_{i}}\bigcap {A_{j}}=\varnothing, i\ne j ,\underset{i=1}{\overset{n}{\mathop {\bigcup }}},=\Omega$&lt;/p&gt;
&lt;h3 id=&#34;概率的基本公式&#34;&gt;概率的基本公式
&lt;/h3&gt;&lt;p&gt;(1)条件概率:
$P(B|A)=\frac{P(AB)}{P(A)}$,表示$A$发生的条件下，$B$发生的概率。&lt;/p&gt;
&lt;p&gt;(2)全概率公式：
$P(A)=\sum\limits_{i=1}^{n}{P(A|{B_{i}})P({B_{i}}),{B_{i}}{B_{j}}}=\varnothing ,i\ne j,\underset{i=1}{\overset{n}{\mathop{\bigcup }}},{B_{i}}=\Omega$&lt;/p&gt;
&lt;p&gt;(3) Bayes公式：&lt;/p&gt;
&lt;p&gt;$P({B_{j}}|A)=\frac{P(A|{B_{j}})P({B_{j}})}{\sum\limits_{i=1}^{n}{P(A|{B_{i}})P({B_{i}})}},j=1,2,\cdots ,n$
注：上述公式中事件${B_{i}}$的个数可为可列个。&lt;/p&gt;
&lt;p&gt;(4)乘法公式：
$P({A_{1}}{A_{2}})=P({A_{1}})P({A_{2}}|{A_{1}})=P({A_{2}})P({A_{1}}|{A_{2}})$
$P({A_{1}}{A_{2}}\cdots {A_{n}})=P({A_{1}})P({A_{2}}|{A_{1}})P({A_{3}}|{A_{1}}{A_{2}})\cdots P({A_{n}}|{A_{1}}{A_{2}}\cdots {A_{n-1}})$&lt;/p&gt;
&lt;h3 id=&#34;事件的独立性&#34;&gt;事件的独立性
&lt;/h3&gt;&lt;p&gt;(1)$A$与$B$相互独立$\Leftrightarrow P(AB)=P(A)P(B)$&lt;/p&gt;
&lt;p&gt;(2)$A$，$B$，$C$两两独立
$\Leftrightarrow P(AB)=P(A)P(B)$;$P(BC)=P(B)P(C)$ ;$P(AC)=P(A)P(C)$;&lt;/p&gt;
&lt;p&gt;(3)$A$，$B$，$C$相互独立
$\Leftrightarrow P(AB)=P(A)P(B)$;     $P(BC)=P(B)P(C)$ ;
$P(AC)=P(A)P(C)$  ;   $P(ABC)=P(A)P(B)P(C)$&lt;/p&gt;
&lt;h3 id=&#34;独立重复试验&#34;&gt;独立重复试验
&lt;/h3&gt;&lt;p&gt;将某试验独立重复$n$次，若每次实验中事件A发生的概率为$p$，则$n$次试验中$A$发生$k$次的概率为：
$P(X=k)=C_{n}^{k}{p^{k}}{(1-p)^{n-k}}$&lt;/p&gt;
&lt;h3 id=&#34;重要公式与结论&#34;&gt;重要公式与结论
&lt;/h3&gt;&lt;p&gt;$(1)P(\bar{A})=1-P(A)$&lt;/p&gt;
&lt;p&gt;$(2)P(A\bigcup B)=P(A)+P(B)-P(AB)$
$P(A\bigcup B\bigcup C)=P(A)+P(B)+P(C)-P(AB)-P(BC)-P(AC)+P(ABC)$&lt;/p&gt;
&lt;p&gt;$(3)P(A-B)=P(A)-P(AB)$&lt;/p&gt;
&lt;p&gt;$(4)P(A\bar{B})=P(A)-P(AB),P(A)=P(AB)+P(A\bar{B}),$
$P(A\bigcup B)=P(A)+P(\bar{A}B)=P(AB)+P(A\bar{B})+P(\bar{A}B)$&lt;/p&gt;
&lt;p&gt;(5)条件概率$P(\centerdot |B)$满足概率的所有性质，
例如：. $P({\bar{A}&lt;em&gt;{1}}|B)=1-P({A&lt;/em&gt;{1}}|B)$&lt;br&gt;
$P({A_{1}}\bigcup {A_{2}}|B)=P({A_{1}}|B)+P({A_{2}}|B)-P({A_{1}}{A_{2}}|B)$
$P({A_{1}}{A_{2}}|B)=P({A_{1}}|B)P({A_{2}}|{A_{1}}B)$&lt;/p&gt;
&lt;p&gt;(6)若${A_{1}},{A_{2}},\cdots ,{A_{n}}$相互独立，则$P(\bigcap\limits_{i=1}^{n}A_{i})=\prod\limits_{i=1}^{n}{P({A_{i}})},$
$P(\bigcup\limits_{i=1}^{n}A_{i})=\prod\limits_{i=1}^{n}{(1-P(A_{i}))}$&lt;/p&gt;
&lt;p&gt;(7)互斥、互逆与独立性之间的关系：
$A$与$B$互逆$\Rightarrow$ $A$与$B$互斥，但反之不成立，$A$与$B$互斥（或互逆）且均非零概率事件$\Rightarrow$$A$与$B$不独立.&lt;/p&gt;
&lt;p&gt;(8)若${A_{1}},{A_{2}},\cdots ,{A_{m}},{B_{1}},{B_{2}},\cdots ,{B_{n}}$相互独立，则$f({A_{1}},{A_{2}},\cdots ,{A_{m}})$与$g({B_{1}},{B_{2}},\cdots ,{B_{n}})$也相互独立，其中$f(\centerdot ),g(\centerdot )$分别表示对相应事件做任意事件运算后所得的事件，另外，概率为1（或0）的事件与任何事件相互独立.&lt;/p&gt;
&lt;h2 id=&#34;随机变量及其概率分布&#34;&gt;随机变量及其概率分布
&lt;/h2&gt;&lt;h3 id=&#34;随机变量及概率分布&#34;&gt;随机变量及概率分布
&lt;/h3&gt;&lt;p&gt;取值带有随机性的变量，严格地说是定义在样本空间上，取值于实数的函数称为随机变量，概率分布通常指分布函数或分布律&lt;/p&gt;
&lt;h3 id=&#34;分布函数的概念与性质&#34;&gt;分布函数的概念与性质
&lt;/h3&gt;&lt;p&gt;定义： $F(x) = P(X \leq x), - \infty &amp;lt; x &amp;lt; + \infty$&lt;/p&gt;
&lt;p&gt;性质：(1)$0 \leq F(x) \leq 1$&lt;/p&gt;
&lt;p&gt;(2) $F(x)$单调不减&lt;/p&gt;
&lt;p&gt;(3) 右连续$F(x + 0) = F(x)$&lt;/p&gt;
&lt;p&gt;(4) $F( - \infty) = 0,F( + \infty) = 1$&lt;/p&gt;
&lt;h3 id=&#34;离散型随机变量的概率分布&#34;&gt;离散型随机变量的概率分布
&lt;/h3&gt;&lt;p&gt;$P(X = x_{i}) = p_{i},i = 1,2,\cdots,n,\cdots\quad\quad p_{i} \geq 0,\sum_{i =1}^{\infty}p_{i} = 1$&lt;/p&gt;
&lt;h3 id=&#34;连续型随机变量的概率密度&#34;&gt;连续型随机变量的概率密度
&lt;/h3&gt;&lt;p&gt;概率密度$f(x)$;非负可积，且:&lt;/p&gt;
&lt;p&gt;(1)$f(x) \geq 0,$&lt;/p&gt;
&lt;p&gt;(2)$\int_{- \infty}^{+\infty}{f(x){dx} = 1}$&lt;/p&gt;
&lt;p&gt;(3)$x$为$f(x)$的连续点，则:&lt;/p&gt;
&lt;p&gt;$f(x) = F&amp;rsquo;(x)$分布函数$F(x) = \int_{- \infty}^{x}{f(t){dt}}$&lt;/p&gt;
&lt;h3 id=&#34;常见分布&#34;&gt;常见分布
&lt;/h3&gt;&lt;p&gt;(1) 0-1分布:$P(X = k) = p^{k}(1 - p)^{1 - k},k = 0,1$&lt;/p&gt;
&lt;p&gt;(2) 二项分布:$B(n,p)$： $P(X = k) = C_{n}^{k}p^{k}(1 - p)^{n - k},k =0,1,\cdots,n$&lt;/p&gt;
&lt;p&gt;(3) &lt;strong&gt;Poisson&lt;/strong&gt;分布:$p(\lambda)$： $P(X = k) = \frac{\lambda^{k}}{k!}e^{-\lambda},\lambda &amp;gt; 0,k = 0,1,2\cdots$&lt;/p&gt;
&lt;p&gt;(4) 均匀分布$U(a,b)$：$f(x) = { \begin{matrix}  &amp;amp; \frac{1}{b - a},a &amp;lt; x&amp;lt; b \  &amp;amp; 0, \ \end{matrix}$&lt;/p&gt;
&lt;p&gt;(5) 正态分布:$N(\mu,\sigma^{2}):$ $\varphi(x) =\frac{1}{\sqrt{2\pi}\sigma}e^{- \frac{(x - \mu)^{2}}{2\sigma^{2}}},\sigma &amp;gt; 0,\infty &amp;lt; x &amp;lt; + \infty$&lt;/p&gt;
&lt;p&gt;(6)指数分布:$E(\lambda):f(x) ={ \begin{matrix}  &amp;amp; \lambda e^{-{λx}},x &amp;gt; 0,\lambda &amp;gt; 0 \  &amp;amp; 0, \ \end{matrix}$&lt;/p&gt;
&lt;p&gt;(7)几何分布:$G(p):P(X = k) = {(1 - p)}^{k - 1}p,0 &amp;lt; p &amp;lt; 1,k = 1,2,\cdots.$&lt;/p&gt;
&lt;p&gt;(8)超几何分布: $H(N,M,n):P(X = k) = \frac{C_{M}^{k}C_{N - M}^{n -k}}{C_{N}^{n}},k =0,1,\cdots,min(n,M)$&lt;/p&gt;
&lt;h3 id=&#34;随机变量函数的概率分布&#34;&gt;随机变量函数的概率分布
&lt;/h3&gt;&lt;p&gt;(1)离散型：$P(X = x_{1}) = p_{i},Y = g(X)$&lt;/p&gt;
&lt;p&gt;则: $P(Y = y_{j}) = \sum_{g(x_{i}) = y_{i}}^{}{P(X = x_{i})}$&lt;/p&gt;
&lt;p&gt;(2)连续型：$X\tilde{\ }f_{X}(x),Y = g(x)$&lt;/p&gt;
&lt;p&gt;则:$F_{y}(y) = P(Y \leq y) = P(g(X) \leq y) = \int_{g(x) \leq y}^{}{f_{x}(x)dx}$， $f_{Y}(y) = F&amp;rsquo;_{Y}(y)$&lt;/p&gt;
&lt;h3 id=&#34;重要公式与结论-1&#34;&gt;重要公式与结论
&lt;/h3&gt;&lt;p&gt;(1) $X\sim N(0,1) \Rightarrow \varphi(0) = \frac{1}{\sqrt{2\pi}},\Phi(0) =\frac{1}{2},$ $\Phi( - a) = P(X \leq - a) = 1 - \Phi(a)$&lt;/p&gt;
&lt;p&gt;(2) $X\sim N\left( \mu,\sigma^{2} \right) \Rightarrow \frac{X -\mu}{\sigma}\sim N\left( 0,1 \right),P(X \leq a) = \Phi(\frac{a -\mu}{\sigma})$&lt;/p&gt;
&lt;p&gt;(3) $X\sim E(\lambda) \Rightarrow P(X &amp;gt; s + t|X &amp;gt; s) = P(X &amp;gt; t)$&lt;/p&gt;
&lt;p&gt;(4) $X\sim G(p) \Rightarrow P(X = m + k|X &amp;gt; m) = P(X = k)$&lt;/p&gt;
&lt;p&gt;(5) 离散型随机变量的分布函数为阶梯间断函数；连续型随机变量的分布函数为连续函数，但不一定为处处可导函数。&lt;/p&gt;
&lt;p&gt;(6) 存在既非离散也非连续型随机变量。&lt;/p&gt;
&lt;h2 id=&#34;多维随机变量及其分布&#34;&gt;多维随机变量及其分布
&lt;/h2&gt;&lt;h3 id=&#34;二维随机变量及其联合分布&#34;&gt;二维随机变量及其联合分布
&lt;/h3&gt;&lt;p&gt;由两个随机变量构成的随机向量$(X,Y)$， 联合分布为$F(x,y) = P(X \leq x,Y \leq y)$&lt;/p&gt;
&lt;h3 id=&#34;二维离散型随机变量的分布&#34;&gt;二维离散型随机变量的分布
&lt;/h3&gt;&lt;p&gt;(1) 联合概率分布律 $P{ X = x_{i},Y = y_{j}} = p_{{ij}};i,j =1,2,\cdots$&lt;/p&gt;
&lt;p&gt;(2) 边缘分布律 $p_{i \cdot} = \sum_{j = 1}^{\infty}p_{{ij}},i =1,2,\cdots$ $p_{\cdot j} = \sum_{i}^{\infty}p_{{ij}},j = 1,2,\cdots$&lt;/p&gt;
&lt;p&gt;(3) 条件分布律 $P{ X = x_{i}|Y = y_{j}} = \frac{p_{{ij}}}{p_{\cdot j}}$
$P{ Y = y_{j}|X = x_{i}} = \frac{p_{{ij}}}{p_{i \cdot}}$&lt;/p&gt;
&lt;h3 id=&#34;二维连续性随机变量的密度&#34;&gt;二维连续性随机变量的密度
&lt;/h3&gt;&lt;p&gt;(1) 联合概率密度$f(x,y):$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$f(x,y) \geq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\int_{- \infty}^{+ \infty}{\int_{- \infty}^{+ \infty}{f(x,y)dxdy}} = 1$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(2) 分布函数：$F(x,y) = \int_{- \infty}^{x}{\int_{- \infty}^{y}{f(u,v)dudv}}$&lt;/p&gt;
&lt;p&gt;(3) 边缘概率密度： $f_{X}\left( x \right) = \int_{- \infty}^{+ \infty}{f\left( x,y \right){dy}}$ $f_{Y}(y) = \int_{- \infty}^{+ \infty}{f(x,y)dx}$&lt;/p&gt;
&lt;p&gt;(4) 条件概率密度：$f_{X|Y}\left( x \middle| y \right) = \frac{f\left( x,y \right)}{f_{Y}\left( y \right)}$ $f_{Y|X}(y|x) = \frac{f(x,y)}{f_{X}(x)}$&lt;/p&gt;
&lt;h3 id=&#34;常见二维随机变量的联合分布&#34;&gt;常见二维随机变量的联合分布
&lt;/h3&gt;&lt;p&gt;(1) 二维均匀分布：$(x,y) \sim U(D)$ ,$f(x,y) = \begin{cases} \frac{1}{S(D)},(x,y) \in D \   0,其他  \end{cases}$&lt;/p&gt;
&lt;p&gt;(2) 二维正态分布：$(X,Y)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$,$(X,Y)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$&lt;/p&gt;
&lt;p&gt;$f(x,y) = \frac{1}{2\pi\sigma_{1}\sigma_{2}\sqrt{1 - \rho^{2}}}.\exp\left{ \frac{- 1}{2(1 - \rho^{2})}\lbrack\frac{(x - \mu_{1})^{2}}{\sigma_{1}^{2}} - 2\rho\frac{(x - \mu_{1})(y - \mu_{2})}{\sigma_{1}\sigma_{2}} + \frac{(y - \mu_{2})^{2}}{\sigma_{2}^{2}}\rbrack \right}$&lt;/p&gt;
&lt;h3 id=&#34;随机变量的独立性和相关性&#34;&gt;随机变量的独立性和相关性
&lt;/h3&gt;&lt;p&gt;$X$和$Y$的相互独立:$\Leftrightarrow F\left( x,y \right) = F_{X}\left( x \right)F_{Y}\left( y \right)$:&lt;/p&gt;
&lt;p&gt;$\Leftrightarrow p_{{ij}} = p_{i \cdot} \cdot p_{\cdot j}$（离散型）
$\Leftrightarrow f\left( x,y \right) = f_{X}\left( x \right)f_{Y}\left( y \right)$（连续型）&lt;/p&gt;
&lt;p&gt;$X$和$Y$的相关性：&lt;/p&gt;
&lt;p&gt;相关系数$\rho_{{XY}} = 0$时，称$X$和$Y$不相关，
否则称$X$和$Y$相关&lt;/p&gt;
&lt;h3 id=&#34;两个随机变量简单函数的概率分布&#34;&gt;两个随机变量简单函数的概率分布
&lt;/h3&gt;&lt;p&gt;离散型： $P\left( X = x_{i},Y = y_{i} \right) = p_{{ij}},Z = g\left( X,Y \right)$ 则：&lt;/p&gt;
&lt;p&gt;$P(Z = z_{k}) = P\left{ g\left( X,Y \right) = z_{k} \right} = \sum_{g\left( x_{i},y_{i} \right) = z_{k}}^{}{P\left( X = x_{i},Y = y_{j} \right)}$&lt;/p&gt;
&lt;p&gt;连续型： $\left( X,Y \right) \sim f\left( x,y \right),Z = g\left( X,Y \right)$
则：&lt;/p&gt;
&lt;p&gt;$F_{z}\left( z \right) = P\left{ g\left( X,Y \right) \leq z \right} = \iint_{g(x,y) \leq z}^{}{f(x,y)dxdy}$，$f_{z}(z) = F&amp;rsquo;_{z}(z)$&lt;/p&gt;
&lt;h3 id=&#34;重要公式与结论-2&#34;&gt;重要公式与结论
&lt;/h3&gt;&lt;p&gt;(1) 边缘密度公式： $f_{X}(x) = \int_{- \infty}^{+ \infty}{f(x,y)dy,}$
$f_{Y}(y) = \int_{- \infty}^{+ \infty}{f(x,y)dx}$&lt;/p&gt;
&lt;p&gt;(2) $P\left{ \left( X,Y \right) \in D \right} = \iint_{D}^{}{f\left( x,y \right){dxdy}}$&lt;/p&gt;
&lt;p&gt;(3) 若$(X,Y)$服从二维正态分布$N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$
则有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$X\sim N\left( \mu_{1},\sigma_{1}^{2} \right),Y\sim N(\mu_{2},\sigma_{2}^{2}).$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$与$Y$相互独立$\Leftrightarrow \rho = 0$，即$X$与$Y$不相关。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$C_{1}X + C_{2}Y\sim N(C_{1}\mu_{1} + C_{2}\mu_{2},C_{1}^{2}\sigma_{1}^{2} + C_{2}^{2}\sigma_{2}^{2} + 2C_{1}C_{2}\sigma_{1}\sigma_{2}\rho)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;${\ X}$关于$Y=y$的条件分布为： $N(\mu_{1} + \rho\frac{\sigma_{1}}{\sigma_{2}}(y - \mu_{2}),\sigma_{1}^{2}(1 - \rho^{2}))$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Y$关于$X = x$的条件分布为： $N(\mu_{2} + \rho\frac{\sigma_{2}}{\sigma_{1}}(x - \mu_{1}),\sigma_{2}^{2}(1 - \rho^{2}))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(4) 若$X$与$Y$独立，且分别服从$N(\mu_{1},\sigma_{1}^{2}),N(\mu_{1},\sigma_{2}^{2}),$
则：$\left( X,Y \right)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},0),$&lt;/p&gt;
&lt;p&gt;$C_{1}X + C_{2}Y\tilde{\ }N(C_{1}\mu_{1} + C_{2}\mu_{2},C_{1}^{2}\sigma_{1}^{2} C_{2}^{2}\sigma_{2}^{2}).$&lt;/p&gt;
&lt;p&gt;(5) 若$X$与$Y$相互独立，$f\left( x \right)$和$g\left( x \right)$为连续函数， 则$f\left( X \right)$和$g(Y)$也相互独立。&lt;/p&gt;
&lt;h2 id=&#34;随机变量的数字特征&#34;&gt;随机变量的数字特征
&lt;/h2&gt;&lt;h3 id=&#34;数学期望&#34;&gt;数学期望
&lt;/h3&gt;&lt;p&gt;离散型：$P\left{ X = x_{i} \right} = p_{i}$
$E(X) = \sum_{i}^{}{x_{i}p_{i}}$&lt;/p&gt;
&lt;p&gt;连续型： $X\sim f(x),E(X) = \int_{- \infty}^{+ \infty}{xf(x)dx}$&lt;/p&gt;
&lt;p&gt;性质：&lt;/p&gt;
&lt;p&gt;(1) $E(C) = C,E\lbrack E(X)\rbrack = E(X)$&lt;/p&gt;
&lt;p&gt;(2) $E(C_{1}X + C_{2}Y) = C_{1}E(X) + C_{2}E(Y)$&lt;/p&gt;
&lt;p&gt;(3) 若$X$和$Y$独立，则$E(XY) = E(X)E(Y)$&lt;/p&gt;
&lt;p&gt;(4)$\left\lbrack E(XY) \right\rbrack^{2} \leq E(X^{2})E(Y^{2})$&lt;/p&gt;
&lt;h3 id=&#34;方差&#34;&gt;方差
&lt;/h3&gt;&lt;p&gt;$D(X) = E\left\lbrack X - E(X) \right\rbrack^{2} = E(X^{2}) - \left\lbrack E(X) \right\rbrack^{2}$&lt;/p&gt;
&lt;h3 id=&#34;标准差&#34;&gt;标准差
&lt;/h3&gt;&lt;p&gt;$\sqrt{D(X)}$，&lt;/p&gt;
&lt;h3 id=&#34;离散型&#34;&gt;离散型
&lt;/h3&gt;&lt;p&gt;$D(X) = \sum_{i}^{}{\left\lbrack x_{i} - E(X) \right\rbrack^{2}p_{i}}$&lt;/p&gt;
&lt;h3 id=&#34;连续型&#34;&gt;连续型
&lt;/h3&gt;&lt;p&gt;$D(X) = {\int_{- \infty}^{+ \infty}\left\lbrack x - E(X) \right\rbrack}^{2}f(x)dx$&lt;/p&gt;
&lt;p&gt;性质：&lt;/p&gt;
&lt;p&gt;(1)$\ D(C) = 0,D\lbrack E(X)\rbrack = 0,D\lbrack D(X)\rbrack = 0$&lt;/p&gt;
&lt;p&gt;(2) $X$与$Y$相互独立，则$D(X \pm Y) = D(X) + D(Y)$&lt;/p&gt;
&lt;p&gt;(3)$\ D\left( C_{1}X + C_{2} \right) = C_{1}^{2}D\left( X \right)$&lt;/p&gt;
&lt;p&gt;(4) 一般有 $D(X \pm Y) = D(X) + D(Y) \pm 2Cov(X,Y) = D(X) + D(Y) \pm 2\rho\sqrt{D(X)}\sqrt{D(Y)}$&lt;/p&gt;
&lt;p&gt;(5)$\ D\left( X \right) &amp;lt; E\left( X - C \right)^{2},C \neq E\left( X \right)$&lt;/p&gt;
&lt;p&gt;(6)$\ D(X) = 0 \Leftrightarrow P\left{ X = C \right} = 1$&lt;/p&gt;
&lt;h3 id=&#34;随机变量函数的数学期望&#34;&gt;随机变量函数的数学期望
&lt;/h3&gt;&lt;p&gt;(1) 对于函数$Y = g(x)$&lt;/p&gt;
&lt;p&gt;$X$为离散型：$P{ X = x_{i}} = p_{i},E(Y) = \sum_{i}^{}{g(x_{i})p_{i}}$；&lt;/p&gt;
&lt;p&gt;$X$为连续型：$X\sim f(x),E(Y) = \int_{- \infty}^{+ \infty}{g(x)f(x)dx}$&lt;/p&gt;
&lt;p&gt;(2) $Z = g(X,Y)$;$\left( X,Y \right)\sim P{ X = x_{i},Y = y_{j}} = p_{{ij}}$; $E(Z) = \sum_{i}^{}{\sum_{j}^{}{g(x_{i},y_{j})p_{{ij}}}}$ $\left( X,Y \right)\sim f(x,y)$;$E(Z) = \int_{- \infty}^{+ \infty}{\int_{- \infty}^{+ \infty}{g(x,y)f(x,y)dxdy}}$&lt;/p&gt;
&lt;h3 id=&#34;协方差&#34;&gt;协方差
&lt;/h3&gt;&lt;p&gt;$Cov(X,Y) = E\left\lbrack (X - E(X)(Y - E(Y)) \right\rbrack$&lt;/p&gt;
&lt;h3 id=&#34;相关系数&#34;&gt;相关系数
&lt;/h3&gt;&lt;p&gt;$\rho_{{XY}} = \frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$,$k$阶原点矩 $E(X^{k})$;
$k$阶中心矩 $E\left{ {\lbrack X - E(X)\rbrack}^{k} \right}$&lt;/p&gt;
&lt;p&gt;性质：&lt;/p&gt;
&lt;p&gt;(1)$\ Cov(X,Y) = Cov(Y,X)$&lt;/p&gt;
&lt;p&gt;(2)$\ Cov(aX,bY) = abCov(Y,X)$&lt;/p&gt;
&lt;p&gt;(3)$\ Cov(X_{1} + X_{2},Y) = Cov(X_{1},Y) + Cov(X_{2},Y)$&lt;/p&gt;
&lt;p&gt;(4)$\ \left| \rho\left( X,Y \right) \right| \leq 1$&lt;/p&gt;
&lt;p&gt;(5) $\ \rho\left( X,Y \right) = 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$ ，其中$a &amp;gt; 0$&lt;/p&gt;
&lt;p&gt;$\rho\left( X,Y \right) = - 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$
，其中$a &amp;lt; 0$&lt;/p&gt;
&lt;h3 id=&#34;重要公式与结论-3&#34;&gt;重要公式与结论
&lt;/h3&gt;&lt;p&gt;(1)$\ D(X) = E(X^{2}) - E^{2}(X)$&lt;/p&gt;
&lt;p&gt;(2)$\ Cov(X,Y) = E(XY) - E(X)E(Y)$&lt;/p&gt;
&lt;p&gt;(3) $\left| \rho\left( X,Y \right) \right| \leq 1,$且 $\rho\left( X,Y \right) = 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$，其中$a &amp;gt; 0$&lt;/p&gt;
&lt;p&gt;$\rho\left( X,Y \right) = - 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$，其中$a &amp;lt; 0$&lt;/p&gt;
&lt;p&gt;(4) 下面5个条件互为充要条件：&lt;/p&gt;
&lt;p&gt;$\rho(X,Y) = 0$ $\Leftrightarrow Cov(X,Y) = 0$ $\Leftrightarrow E(X,Y) = E(X)E(Y)$ $\Leftrightarrow D(X + Y) = D(X) + D(Y)$ $\Leftrightarrow  D(X - Y) = D(X) + D(Y)$&lt;/p&gt;
&lt;p&gt;注：$X$与$Y$独立为上述5个条件中任何一个成立的充分条件，但非必要条件。&lt;/p&gt;
&lt;h2 id=&#34;数理统计的基本概念&#34;&gt;数理统计的基本概念
&lt;/h2&gt;&lt;h3 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h3&gt;&lt;p&gt;总体：研究对象的全体，它是一个随机变量，用$X$表示。&lt;/p&gt;
&lt;p&gt;个体：组成总体的每个基本元素。&lt;/p&gt;
&lt;p&gt;简单随机样本：来自总体$X$的$n$个相互独立且与总体同分布的随机变量$X_{1},X_{2}\cdots,X_{n}$，称为容量为$n$的简单随机样本，简称样本。&lt;/p&gt;
&lt;p&gt;统计量：设$X_{1},X_{2}\cdots,X_{n},$是来自总体$X$的一个样本，$g(X_{1},X_{2}\cdots,X_{n})$）是样本的连续函数，且$g()$中不含任何未知参数，则称$g(X_{1},X_{2}\cdots,X_{n})$为统计量。&lt;/p&gt;
&lt;p&gt;样本均值：$\overline{X} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}$&lt;/p&gt;
&lt;p&gt;样本方差：$S^{2} = \frac{1}{n - 1}\sum_{i = 1}^{n}{(X_{i} - \overline{X})}^{2}$&lt;/p&gt;
&lt;p&gt;样本矩：样本$k$阶原点矩：$A_{k} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}^{k},k = 1,2,\cdots$&lt;/p&gt;
&lt;p&gt;样本$k$阶中心矩：$B_{k} = \frac{1}{n}\sum_{i = 1}^{n}{(X_{i} - \overline{X})}^{k},k = 1,2,\cdots$&lt;/p&gt;
&lt;h3 id=&#34;分布&#34;&gt;分布
&lt;/h3&gt;&lt;p&gt;$\chi^{2}$分布：$\chi^{2} = X_{1}^{2} + X_{2}^{2} + \cdots + X_{n}^{2}\sim\chi^{2}(n)$，其中$X_{1},X_{2}\cdots,X_{n},$相互独立，且同服从$N(0,1)$&lt;/p&gt;
&lt;p&gt;$t$分布：$T = \frac{X}{\sqrt{Y/n}}\sim t(n)$ ，其中$X\sim N\left( 0,1 \right),Y\sim\chi^{2}(n),$且$X$，$Y$ 相互独立。&lt;/p&gt;
&lt;p&gt;$F$分布：$F = \frac{X/n_{1}}{Y/n_{2}}\sim F(n_{1},n_{2})$，其中$X\sim\chi^{2}\left( n_{1} \right),Y\sim\chi^{2}(n_{2}),$且$X$，$Y$相互独立。&lt;/p&gt;
&lt;p&gt;分位数：若$P(X \leq x_{\alpha}) = \alpha,$则称$x_{\alpha}$为$X$的$\alpha$分位数&lt;/p&gt;
&lt;h3 id=&#34;正态总体的常用样本分布&#34;&gt;正态总体的常用样本分布
&lt;/h3&gt;&lt;p&gt;(1) 设$X_{1},X_{2}\cdots,X_{n}$为来自正态总体$N(\mu,\sigma^{2})$的样本，&lt;/p&gt;
&lt;p&gt;$\overline{X} = \frac{1}{n}\sum_{i = 1}^{n}X_{i},S^{2} = \frac{1}{n - 1}\sum_{i = 1}^{n}{(X_{i} - \overline{X})^{2},}$则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\overline{X}\sim N\left( \mu,\frac{\sigma^{2}}{n} \right){\ \ }$或者$\frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0,1)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\frac{(n - 1)S^{2}}{\sigma^{2}} = \frac{1}{\sigma^{2}}\sum_{i = 1}^{n}{(X_{i} - \overline{X})^{2}\sim\chi^{2}(n - 1)}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\frac{1}{\sigma^{2}}\sum_{i = 1}^{n}{(X_{i} - \mu)^{2}\sim\chi^{2}(n)}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;4)${\ \ }\frac{\overline{X} - \mu}{S/\sqrt{n}}\sim t(n - 1)$&lt;/p&gt;
&lt;h3 id=&#34;重要公式与结论-4&#34;&gt;重要公式与结论
&lt;/h3&gt;&lt;p&gt;(1) 对于$\chi^{2}\sim\chi^{2}(n)$，有$E(\chi^{2}(n)) = n,D(\chi^{2}(n)) = 2n;$&lt;/p&gt;
&lt;p&gt;(2) 对于$T\sim t(n)$，有$E(T) = 0,D(T) = \frac{n}{n - 2}(n &amp;gt; 2)$；&lt;/p&gt;
&lt;p&gt;(3) 对于$F\tilde{\ }F(m,n)$，有 $\frac{1}{F}\sim F(n,m),F_{a/2}(m,n) = \frac{1}{F_{1 - a/2}(n,m)};$&lt;/p&gt;
&lt;p&gt;(4) 对于任意总体$X$，有 $E(\overline{X}) = E(X),E(S^{2}) = D(X),D(\overline{X}) = \frac{D(X)}{n}$&lt;/p&gt;</description>
        </item>
        <item>
        <title>一元线性回归</title>
        <link>https://hubojing.github.io/f9qaptva/</link>
        <pubDate>Mon, 25 Nov 2019 15:31:46 +0000</pubDate>
        
        <guid>https://hubojing.github.io/f9qaptva/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;http://img0.imgtn.bdimg.com/it/u=2317694558,3959665778&amp;fm=26&amp;gp=0.jpg&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;深度学习实践系列笔记&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h1&gt;&lt;p&gt;L1损失：基于模型预测的值与标签的实际值之差的绝对值
平方误差（L2误差）：均方误差（MSE）指每个样本的平均平方损失
&lt;img src=&#34;https://img-blog.csdnimg.cn/20191125164618335.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;MSE&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;梯度下降法&#34;&gt;梯度下降法
&lt;/h1&gt;&lt;p&gt;梯度：矢量
沿着负梯度方向探索&lt;/p&gt;
&lt;h1 id=&#34;超参数&#34;&gt;超参数
&lt;/h1&gt;&lt;p&gt;超参数：开始学习过程之前设置的参数，而不是训练得到的参数
典型超参数：学习率、神经网络的隐含层数量&lt;/p&gt;
&lt;h1 id=&#34;步骤&#34;&gt;步骤
&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;准备数据&lt;/li&gt;
&lt;li&gt;构建模型&lt;/li&gt;
&lt;li&gt;训练模型&lt;/li&gt;
&lt;li&gt;进行预测&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;生成人工数据集&#34;&gt;生成人工数据集
&lt;/h1&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#在jupyter中使用matplotlib显示图像需设为inline模式，否则不会显示图像&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matplotlib&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inline&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tensorflow&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tf&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#设置随机数种子&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#直接采用np生成等差数列的方法，生成100个点，每个点的取值在-1~1之间&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linspace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# y=2x+1+噪声，噪声的维度与x_data一致&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;numpy.random.randn(d0, d1, &amp;hellip;, dn)是从标准正态分布中返回一个或多个样本值
实参前加上&lt;code&gt;*&lt;/code&gt;和&lt;code&gt;**&lt;/code&gt;时代表拆包，单个&lt;code&gt;*&lt;/code&gt;表示将元祖拆成一个个单独的实参&lt;/p&gt;
&lt;h1 id=&#34;画图&#34;&gt;画图
&lt;/h1&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#画出随机生成数据的散点图&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#画出线性函数y=2x+1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;linewidth&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://img-blog.csdnimg.cn/20191125173109443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Ym9qaW5n,size_16,color_FFFFFF,t_70&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;散点图&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;构建模型&#34;&gt;构建模型
&lt;/h1&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;placeholder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;float&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;x&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;placeholder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;float&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;y&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;multiply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#构建线性函数的斜率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;w0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#构建线性函数的截距&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Variable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;b0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#pred是预测值，前向计算&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;训练模型&#34;&gt;训练模型
&lt;/h1&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#迭代次数（训练轮数）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train_epochs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#学习率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.05&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#采用均方差作为损失函数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_function&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reduce_mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;square&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#梯度下降优化器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;minimize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Session&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;init&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#开始训练，采用SGD随机梯度下降优化方法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_epochs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feed_dict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;b0temp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;session&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;w0temp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;session&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;#plt.plot(x_data, w0temp * x_data + b0temp)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;w:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;b:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Original data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Fitted line&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;linewidth&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;legend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#通过参数loc指定图例位置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;常见损失函数：均方差（Mean Square Error, MSE）和交叉熵（cross-entropy）
定义优化器Optimizer，初始化一个GradientDescentOptimizer
设置学习率和优化目标：最小化损失
&lt;img src=&#34;https://img-blog.csdnimg.cn/20191125190224581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Ym9qaW5n,size_16,color_FFFFFF,t_70&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;训练结果&#34;
	
	
&gt;
w: 1.9822965
b: 1.0420128
&lt;img src=&#34;https://img-blog.csdnimg.cn/20191125191721732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Ym9qaW5n,size_16,color_FFFFFF,t_70&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;对比&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;预测&#34;&gt;预测
&lt;/h1&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x_test&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;3.21&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feed_dict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;预测值：&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;target&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_test&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;目标值：&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;或者&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x_test&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;3.21&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_test&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;预测值：&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;预测值：7.405184
目标值：7.420000&lt;/p&gt;
&lt;h1 id=&#34;显示损失值&#34;&gt;显示损失值
&lt;/h1&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#训练步数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;display_step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_epochs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feed_dict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;#显示损失值loss,display_step控制报告的粒度&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;#若display_step为2，则将每训练2个样本输出一次损失值&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;loss_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;display_step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Train Epoch:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%02d&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Step: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%03d&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;loss=&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{:.9f}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#plt.plot(loss_list)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;r+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 408 loss= 0.125508696
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 410 loss= 0.036273275
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 412 loss= 0.000716237
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 414 loss= 0.097748078
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 416 loss= 0.026035903
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 418 loss= 0.633028984
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 420 loss= 0.084138028
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 422 loss= 0.088319123
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 424 loss= 0.002654018
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 426 loss= 0.116265893
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 428 loss= 0.018808722
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Train Epoch: 05 Step: 430 loss= 0.000472802
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;显示损失值-1&#34;&gt;显示损失值
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://img-blog.csdnimg.cn/20191125194553838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Ym9qaW5n,size_16,color_FFFFFF,t_70&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;loss1&#34;
	
	
&gt;
&lt;img src=&#34;https://img-blog.csdnimg.cn/20191125194606462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Ym9qaW5n,size_16,color_FFFFFF,t_70&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;loss2&#34;
	
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss_list&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;打印突出的点&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[1.0133754,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.2284044,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.0088208,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.2116321,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 2.3539772,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 2.3148305,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.3175836,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.0387748,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.5018207,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.547514,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.5514,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.5517284,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.5517554,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.551758,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.551758,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.551758,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 1.551758]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;随机梯度下降&#34;&gt;随机梯度下降
&lt;/h1&gt;&lt;p&gt;梯度下降法中，&lt;code&gt;批量&lt;/code&gt;指用于在单次迭代中计算梯度的样本总数。
批量可能相当巨大。
随机梯度下降法（SGD）每次迭代只是用一个样本（批量大小为1）。&lt;code&gt;随机&lt;/code&gt;表示构成各批量的一个样本是随机选择的。
小批量随机梯度下降法（小批量SGD）是介于全批量迭代与SGD之间的折中方案。通常包含10-1000个随机选择的样本。&lt;/p&gt;
&lt;h1 id=&#34;完整代码&#34;&gt;完整代码
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/hubojing/DeepLearningCode-TensorFlow/blob/master/Simple%20linear%20regression.py&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/hubojing/DeepLearningCode-TensorFlow/blob/master/Simple%20linear%20regression.py&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>
