<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>大模型 on 靖待</title>
        <link>https://hubojing.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
        <description>Recent content in 大模型 on 靖待</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>靖待</copyright>
        <lastBuildDate>Wed, 24 Apr 2024 00:15:21 +0000</lastBuildDate><atom:link href="https://hubojing.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>【大模型系列】指令微调</title>
        <link>https://hubojing.github.io/na527pxe/</link>
        <pubDate>Wed, 24 Apr 2024 00:15:21 +0000</pubDate>
        
        <guid>https://hubojing.github.io/na527pxe/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;\images\大模型.jpg&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;大模型系列之指令微调笔记。&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;概述&#34;&gt;概述
&lt;/h1&gt;&lt;p&gt;　　指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，22年谷歌ICLR论文中提出这个概念。在其它文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）或多任务提示训练（Multitask Prompted Training）。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。经过 NLP 指令数据微调后，大语言模型可以学习到指令遵循（Instruction Following）的能力，进而能够解决其他未见过的 NLP 任务。&lt;/p&gt;
&lt;h1 id=&#34;指令数据构建&#34;&gt;指令数据构建
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;基于现有NLP任务数据集构建&lt;/li&gt;
&lt;li&gt;基于日常对话数据构建&lt;/li&gt;
&lt;li&gt;基于合成数据构建
　　以下是几种合成数据构建的方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-instruct&#34;&gt;Self-Instruct
&lt;/h2&gt;&lt;p&gt;　　代表性工作Self-Instruct：借助大语言模型（例如 ChatGPT）所具备的数据合成能力，通过迭代的方法高效地生成大量的指令微调数据。作为初始任务池，该方法首先构建了 175 条高质量且多样的指令数据，之后经由两个主要步骤生成指令微调数据。
　　（1）指令数据生成：从任务池中随机选取少量指令数据作为示例，并针对 ChatGPT 设计精细指令来提示模型生成新的微调数据。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;你被要求提供 10 个多样化的任务指令。这些任务指令将被提供给 GPT 模型。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;以下是你提供指令需要满足的要求：
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1. 尽量不要在每个指令中重复动词，要最大化指令的多样性。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2. 使用指令的语气也应该多样化。例如，将问题与祈使句结合起来。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;……（省略后续要求）
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;下面是 10 个任务指令的列表：
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 指令：将 85 华氏度转换为摄氏度。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 输出：85 华氏度等于 29.44 摄氏度。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 指令：是否有科学无法解释的事情？
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;### 输出：有很多科学无法解释的事情，比如生命的起源、意识的存在……
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;……（省略上下文示例）
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;　　（2）过滤与后处理：剔除低质量或者重复的生成实例，从而保证指令数据的多样性与有效性。常见的过滤方法包括：去除与任务池中指令相似度过高的指令、语言模型难以生成回复的指令、过长或过短的指令以及输入或输出存在重复的实例。&lt;/p&gt;
&lt;h2 id=&#34;evol-instruct&#34;&gt;Evol-Instruct
&lt;/h2&gt;&lt;p&gt;　　Self-Instruct 生成的实例可能过于简单或缺乏多样性，提出一种改进的指令数据合成方法 Evol-Instruct。该方法通过基于深度和广度的演化来提高实例的复杂性和多样性。
　　（1）指令演化：分为深度演化和广度演化。深度演化通过五种特定类型的提示（添加约束、深化、具体化、增加推理步骤以及使输入复杂化）使得指令变得更加复杂与困难；而广度演化旨在扩充指令主题范围、指令涉及的能力范围以及整体数据集的多样性。
　　深度演化（添加约束）指令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;我希望您充当指令重写器。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您的目标是将给定的提示重写为更复杂的版本，使得著名的 AI 系统（例如 ChatGPT 和 GPT-4）更难处理。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;但重写的提示必须是合理的，且必须是人类能够理解和响应的。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您的重写不能省略 # 给定提示 # 中表格和代码等非文本部分。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您应该使用以下方法使给定的提示复杂化：
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;请在 # 给定提示 # 中添加一项约束或要求。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;你应该尽量不要让 # 重写提示 # 变得冗长，# 重写提示 # 只能在 # 给定提示 # 中
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;添加 10 到 20 个单词。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 重写提示 # 中不允许出现“# 给定提示 #”、“# 重写提示 #”字段。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 给定提示 #: {需要重写的指令}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 重写提示 #
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;　　广度演化指令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;我希望你充当指令创造器。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;您的目标是从 # 给定提示 # 中汲取灵感来创建全新的提示。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;此新提示应与 # 给定提示 # 属于同一领域，但更为少见。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 # 的长度和复杂性应与 # 给定提示 # 类似。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 # 必须合理，并且必须能够被人类理解和响应。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 # 中不允许出现“# 给定提示 #”、“# 创造提示 #”字段。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 给定提示 #: {需要重写的指令}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 创造提示 #:
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;　　（2）数据后处理：主要使用了如下的规则进行处理：使用 ChatGPT 比较演化前后的指令，移除 ChatGPT 认为差异很小的指令；移除大模型难以响应的指令，如响应中包含“sorry”或响应长度过短；移除仅包含标点符号和连词的指令或回复。&lt;/p&gt;
&lt;p&gt;　　其它方法：
　　Self-Align设计了多种基于人类对齐原则的合成数据过滤技术，该方法通过上下文提示让 ChatGPT 能够筛选出高质量的实例数据来训练新的模型，并进一步让新训练的模型产生更多与人类对齐的指令微调数据。
　　指令回译技术：直接使用现有的文本（例如维基网页数据）作为输出，然后利用上下文学习来逆向合成相应的输入指令。由于输出内容都是人工撰写的，该方法能够让模型学习生成准确且流畅的文本。&lt;/p&gt;
&lt;h2 id=&#34;指令数据构建的提升方法&#34;&gt;指令数据构建的提升方法
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;指令格式设计
　　训练过程中同时使用了带示例的指令数据（即少样本）和不带示例的指令数据（即零样本）、在指令微调数据集中引入思维链数据、在指令微调时同时引入了包含 CoT 和不包含 CoT 的实例，使用混合指令数据
指令数据并不是包含信息越多越好，添加某些看似有用的信息（例如需要避免的事项、原因或建议）到指令中，可能不会带来明显的效果提升，甚至可能产生不利影响&lt;/li&gt;
&lt;li&gt;扩展指令数量
　　对于NLP任务，FLAN-T5通过逐步将指令数量扩展至 0.18M、5.55M、7.2M 以及17.26M，模型性能呈持续上升的趋势。然而，当指令数量达到 7.2M后，模型性能的提升变得非常缓慢。FLAN-T5 中采用的指令可能仅对传统 NLP 任务适用，而对于日常对话这一至关重要的能力并未带来明显的提升。
　　对于一个较好的预训练基座模型，越来越多的研究工作表明一定数
量的高质量指令就可以激活大语言模型的对话能力。Alpaca [42] 使用了 52K 条合成数据来指令微调 LLaMA (7B)，在 179 条日常对话数据的评测中到达了接近text-davinci-003 的效果。进一步，LIMA [203] 仅使用了一千条人工标注的高质量指令数据来微调 LLaMA (65B)，就在 300 条日常对话测试中取得了较好的模型表现。
但是，仅依靠少量指令数据难以兼顾 NLP 任务和对话场景任务。&lt;/li&gt;
&lt;li&gt;指令重写与筛选
　　YuLan-Chat-3提出了“主题多样性”增强方法，预先从知乎收集了 293 种常见主题标签（例如，“教育”，“体育”），然后随机选择一种并使用 ChatGPT 对指令进行重写来适配到相应的主题（例如使用提示：“请帮我把以下指令重写为教育主题”），最后进行质量筛选来获取高质量的多样性指令数据。它提出了“平衡指令难度”策略，其用大模型的困
惑度分数来估算指令数据的难度水平，删除过于简单或过于困难的指令数据，从而缓解大模型训练不稳定或者过拟合的现象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;指令微调的训练策略&#34;&gt;指令微调的训练策略
&lt;/h1&gt;&lt;p&gt;　　在训练方式上，指令微调与预训练较为相似，很多设置包括数据组织形式都可以预训练阶段所采用的技术。指令微调中的优化器设置（AdamW 或 Adafactor）、稳定训练技巧（权重衰减
和梯度裁剪）和训练技术（3D 并行、ZeRO 和混合精度训练）都与预训练保持阶段一致，可以完全沿用。以下是指令微调与预训练的不同之处。&lt;/p&gt;
&lt;h2 id=&#34;优化设置&#34;&gt;优化设置
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目标函数。预训练阶段通常采用语言建模损失（详见第 6.1.1 节），优化模型在每一个词元上的损失。而指令微调可以被视为一个有监督的训练过程，通常采用的目标函数为序列到序列损失，仅在输出部分计算损失，而不计算输入部分的损失。&lt;/li&gt;
&lt;li&gt;批次大小和学习率。指令微调阶段通常只需要使用较小的批次大小和学习率对模型进行小幅度的调整。&lt;/li&gt;
&lt;li&gt;　　多轮对话数据的高效训练。对于一个多轮对话数据，通常的训练算法是将其拆分成多个不同的对话数据进行单独训练。为了提升训练效率，可以采用特殊的掩码机制来实现多轮对话数据的高效训练。在因果解码器架构中，由于输入输出没有明显的分界，可以将所有一个对话的多轮内容一次性输入模型，通过设计损失掩码来实现仅针对每轮对话的模型输出部分进行损失计算，从而显著减少重复前缀计算的开销。多轮对话涉及多次用户输入和模型的输出，但是训练中仅需要在模型的输出上计算损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据组织策略&#34;&gt;数据组织策略
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;平衡数据分布
　　混合使用现有的多个指令数据集，以此来实现模型能力的综合改进。最常用方法：样本比例混合策略，即把所有数据集进行合并，然后从混合数据集中等概率采样每个实例。研究者建议混合使用 NLP 任务数据（如 FLAN v2）、对话数据（如ShareGPT）和合成数据（如 GPT4-Alpaca），来进行大模型的指令微调。&lt;/li&gt;
&lt;li&gt;多阶段指令数据微调
YuLan-Chat-3 采用了“多阶段指令微调”策略：首先使用大规模 NLP 任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调。为了避免能力遗忘问题，可以在第二阶段中添加一些 NLP 指令数据。这种多阶段的微调策略也可以应用于其他训练设置。例如，对于不同的微调阶段，训练中可以逐渐增加指令的难度和复杂性，从而逐渐提高大模型遵循复杂指令的能力。&lt;/li&gt;
&lt;li&gt;结合预训练数据与指令微调数据
为了使得微调过程更加有效和稳定，可以在指令微调期间引入了预训练数据和任务，这可以看作是对于指令微调的正则化。OPT-IML在指令微调阶段引入了 5% 的预训练数据，在分类和生成任务上都能取得增益；然而，进一步增加预训练数据会对生成任务有利，但有可能损失分类任务的表现。
另一方面，将指令数据引入到预训练阶段也成为了一种常见的训练技术。通过提前使用指令微调数据，有可能会帮助模型在预训练阶段更好地感知下游任务，从而更为针对性地从预训练数据中学习知识与能力。如，GLM-130B的预训练过程由 95% 的传统自监督预训练和 5% 的指令微调任务混合组成。MiniCPM提出在预训练阶段和指令微调阶段之间添加一个“退火阶段”，该阶段混合使用高质量的预训练数据和指令微调数据，其实验结果表明该策略优于先预训练再指令微调的两阶段策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参数高效的模型微调&#34;&gt;参数高效的模型微调
&lt;/h2&gt;&lt;p&gt;　　参数高效微调（Parameter-efficient Fine-tuning），也称为轻量化微调（Lightweight Fine-tuning）。&lt;/p&gt;
&lt;h3 id=&#34;lora&#34;&gt;LoRA
&lt;/h3&gt;&lt;p&gt;低秩适配（Low-Rank Adaptation, LoRA）
模型在针对特定任务进行适配时，参数矩阵往往是过参数化（Over-parametrized）的，其存在一个较低的内在秩。
假设 LoRA 需要训练的参数量为$P_{LoRA}$，模型原始参数为P。$P_{LoRA}$ ≪ P，LoRA 微调需要的显存大小从全量微调的16P大幅减少为$2P+16P_{LoRA}$。&lt;/p&gt;
&lt;h3 id=&#34;lora变种&#34;&gt;LoRA变种
&lt;/h3&gt;&lt;p&gt;在原始的 LoRA 实现中，每个低秩矩阵的低秩参数 𝑅 都被设置为固定且相同的数值，并且在训练过程中无法进行调整，这种设定忽略了不同的秩在微调任务中可能产生的差异化影响。因此，通过这种方式训练得到的低秩矩阵往往并非最优解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AdaLoRA。它讨论了如何更好地进行秩的设置。它引入了一种动态低秩适应技术，在训练过程中动态调整每个参数矩阵需要训练的秩同时控制训练的参数总量。具体来说，模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的参数矩阵被赋予比较高的秩，进而能够更好地学习到有助于任务的信息。相对而言，不太重要的参数矩阵被赋予比较低的秩，来防止过拟合并节省计算资源。尽管 LoRA 能够有效地节省显存，但对于参数规模达到上百亿级别的模型而言，其微调所需的成本仍然相当高昂。&lt;/li&gt;
&lt;li&gt;QLoRA。它将原始的参数矩阵量化为 4 比特，而低秩参数部分仍使用 16 比特进行训练，在保持微调效果的同时进一步节省了显存开销。根据上一小节的分析，对于给定参数量为 𝑃 的模型，QLoRA 微调所需要的显存由 LoRA 微调所需要的2𝑃 进一步下降为 0.5𝑃。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;其它高效微调方法&#34;&gt;其它高效微调方法
&lt;/h3&gt;&lt;p&gt;适配器微调、前缀微调、提示微调。这三种方法在预训练语言模型中被广泛使用，但是在大语言模型中的应用相对较少。&lt;/p&gt;
&lt;h4 id=&#34;适配器微调&#34;&gt;适配器微调
&lt;/h4&gt;&lt;p&gt;适配器微调（Adapter Tuning）在 Transformer 模型中引入了小型神经网络模块（称为适配器）。为了实现适配器模块，研究者提出使用瓶颈网络架构：它首先将原始的特征向量压缩到较低维度，然后使用激活函数进行非线性变换，最后再将其恢复到原始维度。通常来说，适配器模块将会被集成到Transformer 架构的每一层中，使用串行的方式分别插入在多头注意力层和前馈网络层之后、层归一化之前。在微调过程中，适配器模块将根据特定的任务目标进行优化，而原始的语言模型参数在这个过程中保持不变。通过这种方式，可以在微调过程中有效减少需要训练参数的数量。
&lt;img src=&#34;https://img-blog.csdnimg.cn/direct/7bc64be5e98c4908818be832041c1bd2.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;适配器微调&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;前缀微调&#34;&gt;前缀微调
&lt;/h3&gt;&lt;p&gt;前缀微调（Prefix Tuning）在语言模型的每个多头注意力层中都添加了一组前缀参数。这些前缀参数组成了一个可训练的连续矩阵，可以视为若干虚拟词元的嵌入向量，它们会根据特定任务进行学习。在前缀微调过程中，整个模型中只有前缀参数会被训练，因此可以实现参数高效的模型优化。
&lt;img src=&#34;https://img-blog.csdnimg.cn/direct/3fa7cafb4f2641ae8843580df3ba4ecf.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;前缀微调&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;提示微调&#34;&gt;提示微调
&lt;/h3&gt;&lt;p&gt;提示微调仅在输入嵌入层中加入可训练的提示向量。在离散提示方法的基础上（可查阅系列文章&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/hubojing/article/details/138040305&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;提示学习&lt;/a&gt;），提示微调首先在输入文本端插入一组连续嵌入数值的提示词元，这些提示词元可以以自由形式或前缀形式来增强输入文本，用于解决特定的下游任务。在具体实现中，只需要将可学习的特定任务提示向量与输入文本向量结合起来一起输入到语言模型中。
&lt;img src=&#34;https://img-blog.csdnimg.cn/direct/08584e7b84004a18a9e768059cdf238c.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;提示微调&#34;
	
	
&gt;
P-tuning提出了使用自由形式来组合输入文本和提示向量，通过双向 LSTM来学习软提示词元的表示，它可以同时适用于自然语言理解和生成任务。
Prompt Tuning以前缀形式添加提示，直接在输入前拼接连续型向量。
在提示微调的训练过程中，只有提示的嵌入向量会根据特定任务进行监督学习，然而由于只在输入层中包含了极少量的可训练参数，有研究工作表明该方法的性能高度依赖底层语言模型的能力。&lt;/p&gt;</description>
        </item>
        <item>
        <title>【大模型系列】提示学习</title>
        <link>https://hubojing.github.io/s3h6op3z/</link>
        <pubDate>Sun, 21 Apr 2024 12:39:43 +0000</pubDate>
        
        <guid>https://hubojing.github.io/s3h6op3z/</guid>
        <description>&lt;div align=&#34;left&#34;&gt;
&lt;img src=&#34;\images\大模型.jpg&#34; width=&#34;300&#34; height=&#34;180&#34; style=&#34;float:right;&#34;/&gt;
&lt;p&gt;　　&lt;strong&gt;大模型系列笔记之提示工程（基础、上下文学习、思维链）。&lt;/strong&gt;&lt;/p&gt;
 &lt;/div&gt;
&lt;h1 id=&#34;基础提示&#34;&gt;基础提示
&lt;/h1&gt;&lt;h2 id=&#34;人工设计提示&#34;&gt;人工设计提示
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;任务描述&lt;/li&gt;
&lt;li&gt;输入数据&lt;/li&gt;
&lt;li&gt;上下文信息&lt;/li&gt;
&lt;li&gt;提示策略
“你是这个任务的专家”
“让我们一步一步地思考”
“通过依次执行以下任务形成一段连贯的叙述：1. &amp;hellip;；2. &amp;hellip;；3. &amp;hellip;”
对话式大模型，可以将提示拆分为多个子任务提示，以多轮对话的方法逐步输入给大模型。
提供少样本示例
使用特殊符号进行分割（###、三引号、XML标签等）
英文指令理解更好&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;自动提示优化&#34;&gt;自动提示优化
&lt;/h2&gt;&lt;h3 id=&#34;离散提示优化&#34;&gt;离散提示优化
&lt;/h3&gt;&lt;p&gt;离散提示通常是由一系列自然语言词元组成的完整句子表达（如“请根据提供的检索信息回答下列问题”）。在离散的词元空间中进行组合搜索，时间复杂度高且搜索结果可能不是最优。因此有以下的优化方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于梯度的方法
通过梯度更新技术以最大化模型的似然分数来优化离散提示的搜索过程。将提示词初始化为一系列“[MASK]”标记，然后迭代地将提示中的词元替换为词典中的其它词元，通过词元替换产生的对数似然变化来近似估计梯度，进而为提示的每个位置贪心搜索出最佳的词元。由于对提示的每个位置都进行所有候选词元的替换和梯度评估，因此模型需要进行多次前向和后向计算，搜索过程效率较低。为此，可将离散词元转化为连续嵌入表示（软词元），使用梯度直接对连续嵌入参数进行优化，最后将每个连续嵌入映射为词典中最邻近的离散词元。&lt;/li&gt;
&lt;li&gt;基于强化学习的方法
将预训练语言模型作为强化学习中的策略网络并依次生成提示中的词元。在提示生成结束之后，策略网络可以获得任务特定的奖励信号，该奖励信号可通过强化学习算法用于策略网络参数的训练。可设计不同类型的奖励信号，如真实便签与基于提示的预测标签是否一致、生成文本与给定条件的匹配程度。在测试阶段，基于训练好的策略网络，可采用贪心搜索策略来生成任务提示中的每个词元。&lt;/li&gt;
&lt;li&gt;基于编辑的方法
特别适用于无法直接访问模型内部状态（如梯度）的情况，比如只能通过API调用的模型。这种方法需事先定义好编辑操作，然后迭代地修改提示，直到最大迭代轮次或者模型最佳性能。常用的提示编辑操作：修改任务描述、添加或删除上下文任务示例、调整输入到输出的标签映射器（二分类用‘postive/negtive’、‘正/负’）。&lt;/li&gt;
&lt;li&gt;基于大语言模型的方法
该方法首先利用提示生成模型（用于生成指示指令的大语言模型）基于少量上下文示例生成一批候选的任务指令。随后，使用目标模型（用于下游测试的大语言模型）对这些候选指令在目标任务上的表现逐一评估。评估过程可采用模型困惑度或任务准确率作为衡量指令质量的指标。上述过程可通过基于蒙特卡洛搜索的多轮优化策略进行扩展。在每轮迭代中，根据模型表现对候选指令进行筛选得到高评分指令，并利用大语言模型生成与高评分指令相似的新指令，从而扩展候选指令集。迭代完成后，选择模型表现最佳的候选指令作为最终使用的提示。但是这种方法可能在提示搜索过程中陷入局部最优或者产生效果震荡，无法生成更好的提示。为解决该问题，可将所有改进的历史提示及其分数纳入提示优化过程，以指导大语言模型逐步生成更好的新提示。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;连续提示优化&#34;&gt;连续提示优化
&lt;/h3&gt;&lt;p&gt;由一组连续空间中的嵌入向量组成，可根据下游任务的损失直接通过梯度更新进行优化。已有连续提示优化的工作主要是基于预训练语言模型开展，由于参数量巨大，受到的关注有限。已有研究通常依赖于有监督学习方法，数据稀缺时还可采用迁移学习来缓解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习方法
将连续提示向量视为可训练的模型参数，基于下游任务数据，通过最小化交叉熵损失来优化连续提示。Prefix-tuning在语言模型每个Transformer层预置一串前缀（即一组可训练的连续向量），而Prompt-tuning只会在输入层加入可训练的提示向量。通过固定语言模型的大规模参数而只微调这些连续的提示向量，可以有效节省训练所需要的参数量。然而这些提示优化方法通常与输入无关，缺乏对于输入语义的充分考虑。&lt;/li&gt;
&lt;li&gt;迁移学习方法
为若干个具有代表性的源任务学习一个所有任务共享的连续提示，然后使用该提示初始化目标任务的提示，但它在解决目标任务的所有实例时都使用了相同提示，未必适合所有的任务实例。为此，可以为每个源任务独自学习任务特定的连续提示（而不是所有源任务共享），在解决目标任务的实例时，可以采用注意力机制等方式学习目标实例与每个源任务提示的相关性权重系数，对若干个源任务的提示向量进行加权组合，将组合后的新提示（为连续向量形式）用于帮助模型解决当前任务实例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;上下文学习&#34;&gt;上下文学习
&lt;/h1&gt;&lt;p&gt;GPT3提出In-context learning（ICL），由任务描述和（或）示例所组成的自然语言文本作为提示。
首先，通过自然语言描述任务，并从任务数据集中选择一些样本作为示例。其次，根据特定的模板，将这些示例按照特定顺序组合成提示内容。最后，将测试样本添加到提示后面，整体输入到大语言模型以生成输出。基于任务描述以及示例信息，大语言模型无需显式的梯度更新即可识别和执行新的任务。&lt;/p&gt;
&lt;h2 id=&#34;示例选择&#34;&gt;示例选择
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;基于相关度排序的方法
KNN相似度检测算法，选出与目标任务实例相关的示例。具体来说，可以使用文本嵌入模型（如 BERT）将所有候选样本映射到低维嵌入空间中，然后根据这些候选样本与测试样本的嵌入语义相似度进行排序，并选择出最相关的 𝑘 个示例，最后将筛选出的示例作为上下文学习的示例集合。&lt;/li&gt;
&lt;li&gt;基于集合多样性的方法
经典启发式MMR（Maximum Margin Ranking）算法、基于行列式点过程的DPP算法（Determinantal Point Process）&lt;/li&gt;
&lt;li&gt;基于大语言模型的方法
将大语言模型作为评分器对候选样本进行评估，进而选择出优质的示例。一种最直接的评估方法是通过计算在加入当前示例后大语言模型性能的增益来评估示例的有效性，以此筛选出有效的示例。但是，这种方法需要大语言模型进行重复多次计算，才能选择出最优的示例集合。为了减少大语言模型评估的开销，还可以根据大语言模型的评分结果选择出少量的正负示例用于训练一个分类器，该分类器通过正负示例能够学习到如何准确地区分和筛选出高质量示例，从而更准确地来指导后续的示例选择过程。
总体来说，在上下文学习中进行示例选择时应确保所选示例包含丰富的任务信息且与测试样本保持高度的相关性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;示例格式&#34;&gt;示例格式
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;人工标注的格式&lt;/li&gt;
&lt;li&gt;自动生成的格式
首先人工标注一部分的示例模板作为种子集合加入到大语言模型的输入中。然后，利用大语言模型强大的少样本学习能力，指导其为新任务生成相应的示例模版。最后，对这些生成的示例模版进行筛选与后处理，使之符合任务要求。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;示例顺序&#34;&gt;示例顺序
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;产生候选示例顺序
大语言模型在做出预测时，倾向于依赖于提示末端的信息。常用的方式是根据示例与测试样本之间的语义相似度进行排序，然后将与测试样例相似度更高的示例放在更靠近测试样本的位置。&lt;/li&gt;
&lt;li&gt;评估示例顺序质量
在测试集样本可获得的情况下，可以直接测试大语言模型基于该示例顺序的任务性能，以此作为当前示例顺序的评分。无法获得测试样本时，需要人工创建独立的验证集进行示例顺序的评估。另一种不依赖测试数据的评估方法是采用模型对于预测结果的不确定性作为评估指标。具体来说，可以计算基于该示例顺序大语言模型预测分布的熵值，选择熵值较低的示例顺序作为较为有效的顺序。熵值越低，意味着模型预测分布越不均匀，则模型预测的置信度更高。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;思维链提示&#34;&gt;思维链提示
&lt;/h1&gt;&lt;p&gt;思维链提示作为上下文学习的一种扩展形式，将原始的&amp;lt;输入，输出&amp;gt;映射关系转换为&amp;lt;输入，思维链，输出&amp;gt;这一三元组形式。
“Let’s think step by step.”
“Take a deep breath and work on this problem step-by-step.”&lt;/p&gt;
&lt;h2 id=&#34;思维链示例设计&#34;&gt;思维链示例设计
&lt;/h2&gt;&lt;p&gt;从输入侧对思维链示例进行增强。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复杂化的思维链&lt;/li&gt;
&lt;li&gt;推理步骤多、问题长&lt;/li&gt;
&lt;li&gt;多样化的思维链
首先利用聚类算法（例如 𝑘-means 聚类）将训练集中的问题划分为 𝑘 个簇（𝑘 为所需的示例数量），簇内部的问题比较相似，而不同簇的问题差别较大。然后，预定义一系列启发式规则，从每个簇中选择距离质心最近且满足规则的问题作为该簇的代表性问题，将该问题输入给大语言模型并生成对应的思维链和答案作为示例。由于每个问题来自于不同的簇，从而保证了示例的多样性。实验发现，虽然大模型生成的思维链示例可能存在错误，但是当选择更加多样化的示例时，思维链示例中的错误对模型性能的影响会显著降低。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;思维链生成方法&#34;&gt;思维链生成方法
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;基于采样的方法
单一思维链推理容易出错，可采样多条推理路径来缓解。代表性方法：self-consistency。首先使用大语言模型生成多个推理路径和对应的答案，然后对于这些候选答案进行集成并获得最终输出。具体的集成方法可选择各条推理路径所得到答案中出现频率最高的那个答案作为最终输出，也可对所有答案进行某种形式的加权。
但问题太简单用思维链反而效果不好。例如，对于句子的情感分类任务，由于问题过于简单，加入思维链提示之后反而会使模型过度思考，从而得出错误的答案。&lt;/li&gt;
&lt;li&gt;基于验证的方法
思维链提示所具有的顺序推理本质可能导致推理过程中出现错误传递或累积的现象。为了解决这一问题，可以使用专门训练的验证器或大语言模型自身来验证所生成的推理步骤的准确性。
例：DIVERSE 方法
针对整个推理路径的验证器通过如下方法训练得到：首先选择一个包含大量问题答案对的数据集，然后将问题输入给大语言模型，通过思维链提示的方法使其生成推理路径和最终答案。如果模型生成的答案和数据集标注的答案一致，则判为正例，否则判为负例。最后使用构造的&amp;lt;问题，推理链，答案&amp;gt;数据训练一个二分类器，从而可以对任意一个推理路径进行打分。训练针对中间步骤的验证器也可以采用类似的方案。然而，与整体推理路径的数据标注相比，构造面向中间步骤的正负例数据更加困难。这里可以采取一个简化处理：对于每一个训练集中的问题，我们采样多次得到多个推理路径，对于得出正确答案的推理路径，中间的每一个步骤我们都认为是正确的，作为正例；对于得出错误答案的推理路径，如果其中某个步骤和正例的推理路径相一致，也认为是正例，否则作为负例。通过这样构造出来的数据，用同样的方法训练一个二分类器，从而可以对模型输出的中间步骤进行打分。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;拓展的推理结构&#34;&gt;拓展的推理结构
&lt;/h2&gt;&lt;p&gt;链式推理结构在处理较为复杂的任务时（例如需要进行前瞻和回溯探索）仍然存在一定的局限性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;树形推理结构
代表性工作：思维树（Tree of Thought, ToT）
它和思维链的区别在于：思维链从一个节点出发，只能生成一个节点，而思维树则可以生成多个节点。当某一个思考步骤无法得到正确答案时，可以回溯到它的父节点，选择另一个子节点继续推理。&lt;/li&gt;
&lt;li&gt;图形推理结构
代表性工作：思维图（Graph of Thought, GoT）
思维图和思维树的区别在于，思维树的子节点只能进行前向搜索和回溯，而思维图的子节点可以和其他子节点进行汇聚，得到新的中间步骤，然后进行下一步的推理。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
