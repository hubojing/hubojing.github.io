<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>靖待的技术博客</title>
  
  <subtitle>小清新IT旅程 | 为中华之崛起而读书</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://hubojing.github.io/"/>
  <updated>2023-12-31T17:39:12.000Z</updated>
  <id>https://hubojing.github.io/</id>
  
  <author>
    <name>靖待</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2023年终总结</title>
    <link href="https://hubojing.github.io/2023/12/31/2023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    <id>https://hubojing.github.io/2023/12/31/2023年终总结/</id>
    <published>2023-12-31T14:37:55.000Z</published>
    <updated>2023-12-31T17:39:12.000Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="\images\年终总结.png" width="300" height="180" style="float:right;"><br> 　　<br>　　<strong>希望2024年能深耕技术。</strong><br><br><br><br> </div><a id="more"></a><p>　　2023年，技术方面，对自己不太满意。  </p><h1 id="今年总结"><a href="#今年总结" class="headerlink" title="今年总结"></a>今年总结</h1><p>　　先说说博客吧，毕竟博客是我技术向感受的输出窗口。今年发布的博文很少，尤其是4月以后到10月几乎停更。这有两大原因，一方面是生活上的，一方面是技术上的。<br>　　在生活中，今年总是会被外界看起来不大的事情所牵绊，陷入各种内耗中。2022年底阳康后，身体似乎比以前要虚很多。2023年发烧的次数似乎比前十年加起来都多，甚至出现了这辈子没体验过的40.4℃（幸好脑子影响不大…吧）。很多次都有想学的心，但是整体状态让我只想躺着休息。这就是心有余而力不足吗？可是我三十还没到呢，这不对。明年一定要改。   </p><p>　　技术博客只谈技术。<br>　　在技术上，去年底开始，大模型技术受到关注，今年三月火热程度仿佛新技术革命一般，我也兴致勃勃地去阅读chatGPT的论文，去理解什么是RLHF，什么是PPO，如何去模型分布式训练等。但是当我粗浅地了解这些东西后，我陷入了一种莫名的失落感。NLP算法似乎进入到了一种谁的数据多、谁的算力大，谁就能赢得更高的准确率和召回率，而那些细微的模型区别，在这些东西的掩盖下，似乎已经不那么重要。未来，可能只有极个别接触到模型的核心算法去做模型的事情，其他人沦为数据处理师或者模型应用师，比如最新的Prompt工程师。<br>　　这个词刚出来的时候，我是很不屑的。一个东西称为技术，应该是需要门槛的。但是Prompt这种玩意儿不需要会数据结构，也不需要会组成原理，甚至只要会打字（哦，语音输入也不是不行）就可以干的事情，也能算技术？这也能叫工程师？然而现实啪啪打脸，讲prompt优化的论文在各大顶会上如泉涌，一时间我真的分不清是因为这一块好灌水，还是因为prompt真的是技术。到现在，我还是有些不解，很多论文提出了各种玄学调prompt的方法，甚至还总结了很多经验，可是模型只要稍作修改，那些奇技淫巧很可能都失效了啊。这种感觉，就仿佛盲人摸象般，只知其然而不知其所以然。<br>　　总之，困惑不外有二。一是个人很难复现大模型能力，我也想改模型，也想训练调参，但是没有那么多卡，也没有几十个数据团队为我服务，现在几乎失去了个人单打独斗就能创造的机会，只能看看论文，找找开源代码玩参数很小的玩具；二是火热的prompt技术似乎也没什么技术含量。所以，四月后很长时间，我对大模型一直抱有抵触情绪，有时候恨不能刻意避开它。仿佛它并不是一种厉害的发明，更像是粗鲁的大力飞砖打败了细致的算法。那段时间，经常在知乎上看到这种论调：NLP已死。那我去年从搜广推转向NLP，岂不是49年入国军？<br>　　有时候也觉得自己很可笑，明明只是个刚入行的算法新手，竟然还挑剔起现在最火热的技术了，颇有点眼高手低。基础打扎实了吗就在这嚷嚷，我经常暗自嘲笑自己。然而我依然我行我素，我觉得扩散模型好玩、有用，多模态模型好玩、有用，大模型？啊nono。<br>　　这种困扰直到被人骂了才清醒许多。“既然你觉得prompt算不上技术，那以前微调那些，也并没有重训模型，只是在原模型上修修改改，要说技术含量，也算不了什么技术。那你干嘛还要学微调那些呢？技术都是慢慢发展起来的。”不知道为什么，就这几句话反而说动我了。是我应该去适应技术，而不是让世界迁就我。如果足够强大，提出更好的技术时，也许世界就会跟随我。（纯属白日做梦~哈哈）<br>　　所以，在很晚的11月，我又重新捡起大模型的论文看了起来。Better late than never嘛。</p><p>　　工作上，逐渐从新人成长为熟悉业务的老员工。可惜员工流动频繁，逐渐地，很少被指导技术了，自己一个人摸索总是不断走弯路。虽然一直明白，学技术要靠自己，可是还是偶尔做梦要是能跟着大佬学习多好。今年有了两个实习生，也是人生中第一次带人，自己非常理解如果有个经验稍多一点的人帮助白纸们规划或者点评一下，其实对他们来说也许可以省时省力一些。所以我尽量在他们困惑时帮一把，至少在这段实习中有些收获，虽然也很忐忑自己能力有限无法给予太多他们想学的东西，但是我已尽力啦，希望他们对我还算满意。</p><p>　　读书上，技术书籍今年读完了几本，还凑合。</p><h1 id="明年计划"><a href="#明年计划" class="headerlink" title="明年计划"></a>明年计划</h1><ul><li>跟着大模型的步伐，学！学就是了！</li><li>基础，打基础，NLP基础永不过时</li><li>搜广推算法持续关注</li><li>写几个自己感兴趣的小软件</li><li>了解机器人相关算法</li><li>强身健体，为祖国健康工作！</li></ul><p>　　希望明年能深耕技术。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;\images\年终总结.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt; 　　&lt;br&gt;　　&lt;strong&gt;希望2024年能深耕技术。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="杂谈" scheme="https://hubojing.github.io/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="年终总结" scheme="https://hubojing.github.io/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>大模型相关论文笔记</title>
    <link href="https://hubojing.github.io/2023/11/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>https://hubojing.github.io/2023/11/22/大模型相关论文笔记/</id>
    <published>2023-11-22T22:20:55.000Z</published>
    <updated>2024-02-12T13:30:22.754Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="\images\paper.png" width="300" height="180" style="float:right;"><br> 　　<br>　　<strong>大模型相关论文阅读笔记。 </strong><br>　　<strong>倒序排列论文，最新阅读的在最上面。</strong><br>　　<strong>2024年1月26日更新</strong><br><br><br> </div><a id="more"></a><h1 id="Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks"><a href="#Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks" class="headerlink" title="Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"></a>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</h1><p>用于知识密集型NLP任务的检索增强生成<br>Facebook 2020<br><a href="https://arxiv.org/pdf/2005.11401v4.pdf" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag" target="_blank" rel="noopener">CODE</a><br>（论文代码链接已失效，以上是最新链接）</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>大模型有幻觉问题（hallucinations），检索增强生成(retrieval-augmented generation, RAG)可以解决它。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>输入为x，外部检索资源为z，生成目标序列y。<br>模型有两块：一个检索器$p_\eta(z|x)$，$\eta$为参数，给定一个查询q，根据文本返回top-K个分布；一个生成器$p_\theta(y_i|x,z,y_{1:i-1})$，参数为$\theta$，它基于过去i-1个tokens $y_{1:i-1}$、原始输入x和检索器信息z，产生一个当前的token。<br>为了端到端的训练检索器和生成器，我们将检索文档作为一个隐变量。我们提出了两个模型，他们以不同的方式边缘化隐变量，从而在文本上产生分布。在我们的方法里，第一步，RAG-Sequence，这个模型使用相同的文本预测每一个目标token；第二步，RAG-Token，基于不同的文件预测每一个目标token。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul><li>RAG-Sequence模型<br>$$p_{RAG-Sequence}(y|x)≈\sum_{z∈top-k(p(·|x))}p_\eta(z|x)p_\theta(y|x,z)=\sum_{z∈top-k(p(·|x))}p_\eta(z|x)\prod^N_ip_\theta(y_i|x,z,y_{1:i-1})$$</li><li>RAG-Token模型<br>$$p_{RAG-Token}(y|x)≈\prod^N_i\sum_{z∈top-k(p(·|x))}p_\eta(z|x)p_\theta(y_i|x,z,y_{1:i-1})$$<h3 id="检索器：DPR"><a href="#检索器：DPR" class="headerlink" title="检索器：DPR"></a>检索器：DPR</h3>DPR(Dense Passage Retriever)，密集信息检索器<br>检索器$p_\eta(z|x)$基于DPR。DPR是一个双向编码器架构：<br>$$p_\eta(z|x)∝exp(d(z)^Tq(x)) ~~~ d(z)=BERT_d(z), q(x)=BERT_q(x)$$<br>其中，d(z)是使用BERT编码得到的密集表示，q(x)是问题通过BERT编码得到的表示。计算top-k($p_\eta(·|x)$)是一个MIPS(Maximum Inner Product Search)问题，可以在亚线性时间内解决。我们使用一个基于DPR的预训练双向编码器来初始化我们的检索器并建立索引，将其视为非参数记忆(non-parametric memory)。<h3 id="生成器：BART"><a href="#生成器：BART" class="headerlink" title="生成器：BART"></a>生成器：BART</h3>生成器可以使用任何编码器-解码器。我们使用的是BART-large。<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3>求最小似然log-likelihood、Adam<h3 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h3></li><li>RAG-Toke<br>$$p^{‘}_\theta(y_i|x,y_{1:i-1})=\sum_{z∈top-k(p(·|x))}p_\eta(z_i|x)p_\theta(y_i|x,z_i,y_{1:i-1})$$<br>将$p^{‘}_\theta(y_i|x,y_{1:i-1})$送入标准beam解码器中。</li><li>RAG-Sequence<br>Thorough Decoding<br>Fast Decoding</li></ul><h1 id="FLASHATTENTION-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness"><a href="#FLASHATTENTION-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness" class="headerlink" title="FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness"></a>FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness</h1><p>FlashAttention: 具有IO感知的快速和有效存储精确注意力<br>2022年6月24日<br><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/Dao-AILab/flash-attention" target="_blank" rel="noopener">CODE</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>　　自注意力的时间和空间复杂度在序列长度上是二次关系，近似注意力机制尝试在模型质量和复杂度计算中折中来解决该问题，但是经常不能实现wall-clock加速。本文认为，需要一种规范，可以根据GPU读写水平使注意力IO感知。为此，本文提出FLASHATTENTION，一种IO感知的精确注意力机制，它使用tiling技术来减少GPU HBM（high bandwidth memory）和GPU芯片内SRAM的存储读写次数。FLASHATTENTION相比标准注意力机制要减少这方面开销。</p><h2 id="引言-1"><a href="#引言-1" class="headerlink" title="引言"></a>引言</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="\images\FlashAttention.png" alt="FlashAttention" title>                </div>                <div class="image-caption">FlashAttention</div>            </figure>  <p>　　现代GPU的计算速度比存储速度快，在Transformer里的许多操作都受限于存储接入。现在的公共Python接口，比如PyTorch和Tensorflow对于内存接入没有精细化管理。本文提出FLASHATTENTION，一种计算注意力时减少内存接入操作的新注意力算法。它的目标是减少在HBM中读写的注意力矩阵。这需要<br>（1）在不访问整体输入的情况下计算softmax reduction；<br>（2）反向传播时不存大量中间过程的注意力矩阵。<br>　　本文提出两种方法解决上面的问题。<br>（1）我们重新构建了注意力计算模块，将输入分块，在输入块中形成多个通道，因此递增地执行softmax reduction（也就是tiling)；<br>（2）从前向传播到反向传播中快速重新计算片上注意力，我们存储了其中的softmax标准化因素，这比从HBM读取中间注意力矩阵的标准方法要快。<br>　　在CUDA使用FLASHATTENTION去实现精细化存储控制以及在GPU内核中融合所有的注意力操作。即使因为重计算会增加FLOP（Floating Point Operations），相对于标准注意力而言，我们的算法依然更快、需要更少的内存，在序列长度上是线性的，这是因为HBM的接入大量减少。<br>　　FLASHATTENTION在HBM上的复杂度是O(N^2d^2M^{-1})，其中d是头head的维度，M是SRAM的规模，标准注意力的复杂度是$Ω(Nd+N^2)$。<br>　　本文贡献点：</p><ul><li>更快的模型训练速度</li><li>更高的模型质量</li><li>比现有基线注意力都要快<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="硬件性能"><a href="#硬件性能" class="headerlink" title="硬件性能"></a>硬件性能</h3>　　重点描述GPU。</li><li>GPU存储等级<br>　　HBM、SRAM</li><li>执行模型<br>　　GPU有很多线程去执行一个操作（称为核）。每个核从HBM登记加载输入，SRAM计算，再将输出写入HBM。</li><li>性能特点</li></ul><ol><li>计算密集型Compute-bound</li><li>存储密集型Memory-bound</li></ol><ul><li>内核融合<br>　　最常见的加速存储密集操作的就是内核融合。如果多个操作同时应用在相同的输入时，可以从HBM一次性加载输入。编译器会自动融合许多elementwise操作。然而，根据模型训练的上下文，中间过程的值为了反向传播仍然需要写入HBM，这降低了原生内核融合的效率。<h4 id="标准注意力"><a href="#标准注意力" class="headerlink" title="标准注意力"></a>标准注意力</h4>　　输入序列$Q,K,V∈\mathbb{R}^{N×d}$，其中N是序列长度，d是头head维度。我们想要计算注意力输出$O∈\mathbb{R}^{N×d}$。<br>$$S=QK^T∈\mathbb{R}^{N×N}, P=softmax(S)∈\mathbb{R}^{N×N}, O=PV∈\mathbb{R}^{N×d}$$<br>softmax按行(row-wise)使用。<br>　　标准注意力将S和P扔给HBM，这花费了$O(N^2)$存储。一般来说，N&gt;&gt;d，比如GPT2里N=1024，d=64。大部分操作是存储密集型（比如softmax），大量的存储读写造成wall-clock时间变慢。<br>　　其它操作更加加剧了这个问题，比如加在注意力矩阵的elementwise操作、加在S上的遮罩或者加在P上的dropout。为此，有很多融合几种elementwise操作的方法尝试，比如一些文献用softmax融合遮罩。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="\images\FlashAttention-StandardAttention伪代码.png" alt="标准注意力伪代码" title>                </div>                <div class="image-caption">标准注意力伪代码</div>            </figure><h2 id="FLASHATTENTION"><a href="#FLASHATTENTION" class="headerlink" title="FLASHATTENTION"></a>FLASHATTENTION</h2>　　两种方法：tiling和recomputation<br>　　主要思路：将输入的Q、K、V分块，将它们从慢的HBM放到快的SRAM中，计算了注意力输出后再返回到各自块里。在每个快的输出相加之前，通过标准化进行缩放，最终得到结果。</li><li>Tiling</li><li>Recomputation<br>　　我们的目标之一是不要存储反向传播中间过程值$O(N^2)$。反向传播需要矩阵$S,P∈\mathbb{R}^{N×N}$来计算Q、K、V的梯度。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="\images\FlashAttention伪代码.png" alt="FlashAttention伪代码" title>                </div>                <div class="image-caption">FlashAttention伪代码</div>            </figure></li></ul><h1 id="LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS"><a href="#LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS" class="headerlink" title="LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"></a>LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</h1><p>　　大模型的低秩适配器<br>　　微软 2021年<br>　　Low-Rank Adaptation, LORA<br><a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/microsoft/LoRA" target="_blank" rel="noopener">CODE</a><br>　　冻结预训练模型的参数，在Transformer架构每一层注入一个可训练的低秩分解矩阵（rank decomposition matrices），大幅减少了下游任务的训练参数。<br>　　对比GPT-3 175B Adam微调，LoRA可以减少10000倍训练参数、3倍GPU存储。<br>　　对比RoBERTa, DeBERTa, GPT-2, GPT-3，训练参数虽少，模型微调质量更好，更高的训练吞吐量，而且不像适配器，没有额外的推理延迟。  </p><h2 id="引言-2"><a href="#引言-2" class="headerlink" title="引言"></a>引言</h2><p>　　微调会更新预训练模型的全部参数，下游任务的新模型和原模型参数一样多。许多研究者通过只更新部分参数或学习新任务的额外模块进行迁移，这样可以只保存和加载一小部分任务相关的参数即可，部署时提高了效率。但是现有方法通过延伸模型深度或者减少模型可用的序列长度会导致推理延迟。而且这些策略达不到微调的基线效果，在效率和模型质量上做了折中。<br><img src="\images\LoRA.png" alt="LoRA"></p><h2 id="问题陈述"><a href="#问题陈述" class="headerlink" title="问题陈述"></a>问题陈述</h2><p>　　给定一个预训练自回归语言模型$P_{\Phi}(y|x)$，$\Phi$为参数。将该模型用于下游任务。全量微调的话，模型将用预训练的权重$\Phi_0$作为初始化参数，并使用下方梯度计算公式来最大化目标以更新到$\Phi_0 + △\Phi$：<br>$$\underset{\Phi}{max}\sum_{(x,y)∈Z}\sum^{|y|}<em>{t=1}log(P</em>{\Phi}(y_t|x,y_{&lt;t}))$$  </p><p>　　全量微调的一个短板就是，下游任务不同就要学习不同的参数$△\Phi$，它的维度$|△\Phi|$和$|\Phi_0|$相同。而预训练模型的参数是很大的，这样就对存储和部署要求很高。<br>　　本文采用了一个更有效的方法，$△\Phi = △\Phi(\Theta)$是通过一个远小于$|△\Phi_{\Theta}|$的小尺寸参数$\Theta$编码得到。找到$△\Phi$变为了在$\Theta$上优化：<br>$$\underset{\Theta}{max}\sum_{(x,y)∈Z}\sum^{|y|}<em>{t=1}log(P</em>{\Phi}(y_t|x,y&lt;t))$$<br>　　本文提出了一种低秩表示来编码$△\Phi$。对于GPT-3 175B，$|\Theta|$的训练参数量是$|\Phi_0|$的0.01%。</p><h2 id="之前方法的缺点"><a href="#之前方法的缺点" class="headerlink" title="之前方法的缺点"></a>之前方法的缺点</h2><ul><li>适配器层引入了推理延迟  </li><li>直接优化Prompt很难<br>　　比如prefix tuning<h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><h3 id="低秩参数更新矩阵"><a href="#低秩参数更新矩阵" class="headerlink" title="低秩参数更新矩阵"></a>低秩参数更新矩阵</h3>假定$W_0∈\mathbb{R}^{d×k}$。<br>$W_0 + △W = W_0 + BA$，其中$B∈\mathbb{R}^{d×r}$，$A∈\mathbb{R}^{r×k}$，这个r的秩远小于min(d, k)。<br>训练时，冻住$W_0$，不接收梯度更新，同时A和B包含可训练参数。<br>设$h = W_0x$：<br>$$h = W_0x+△Wx = W_0x + BAx$$<br>当遇到不用的下游任务时，只需要替换BA就行，所以没有推理延迟。</li></ul><h3 id="将LoRA应用到Transformer"><a href="#将LoRA应用到Transformer" class="headerlink" title="将LoRA应用到Transformer"></a>将LoRA应用到Transformer</h3><p>在Transformer架构中，在自注意力模块有四个权重矩阵（$W_q$、$W_k$、$W_v$、$W_o$），在MLP模块有两个。本文将Transformer架构中的$W_q$（或者$W_k$，$W_v$）设为一个$d_{model}×d_{model}$的单矩阵。对于下游任务，只改变注意力权重，冻结MLP模块的。</p><h1 id="Llama-2-Open-Foundation-and-Fine-Tuned-Chat-Models"><a href="#Llama-2-Open-Foundation-and-Fine-Tuned-Chat-Models" class="headerlink" title="Llama 2: Open Foundation and Fine-Tuned Chat Models"></a>Llama 2: Open Foundation and Fine-Tuned Chat Models</h1><p>2023年7月  77页<br>GenAI, Meta<br><a href="https://arxiv.org/pdf/2307.09288.pdf" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener">CODE</a></p><h2 id="引言-3"><a href="#引言-3" class="headerlink" title="引言"></a>引言</h2><p>　　本文发布了两个模型：</p><ul><li>LLAMA 2，它是LLAMA 1的升级版本，训练语料新增40%，模型上下文长度翻倍，采用了分组查询注意力。发布了7B, 13B, 70B，34B也训了但没发布</li><li>LLAMA 2-CHAT，它是LLAMA 2用于用户对话的微调版本。发布了7B，13B，70B参数模型<br>　　提供了<a href="https://ai.meta.com/llama" target="_blank" rel="noopener">用户手册</a>和<a href="‖https://github.com/facebookresearch/llama">代码样例</a></li></ul><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>　　预训练数据：2 trillion<br>　　训练，与LLAMA 1相同之处：  </p><ul><li>标准transformer</li><li>RMSNorm预归一化</li><li>SwiGLU激活函数</li><li>旋转位置embedding RoPE<br>　　不同之处：</li><li>增加上下文长度</li><li>分组查询注意力（GQA）</li></ul><p>　　优化器：AdamW<br>　　分词器：与LLAMA 1相同，此表规模32k tokens</p><p>　　它甚至还写了LLAMA 2的碳排放情况…  </p><p>　　评价指标方面，LLAMA 2 70B在MMLU和GSM8K上与GPT-3.5相近，但是在代码基线上有差距，在几乎所有的基线上都比PaLM(540B)强。与GPT-4和PaLM-2-L相比，还有很大差距。</p><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><h3 id="SFT监督微调"><a href="#SFT监督微调" class="headerlink" title="SFT监督微调"></a>SFT监督微调</h3><p>　　使用了公开的微调数据，自己标了一些，有27540这么多就不标了，因为数量少质量高效果也能好。<br>　　对于微调过程，每一个样本包含一个提示prompt和一个答案。为了确保模型序列长度都被填充，训练集里将全部的prompt和答案相连。用了特殊token划分prompt和答案片段。使用了一种自回归目标函数，并将来自用户prompt的token计算loss归零，所以只在答案token上反向传播。模型微调轮数为2。  </p><h3 id="RLHF-人类反馈的增强学习"><a href="#RLHF-人类反馈的增强学习" class="headerlink" title="RLHF 人类反馈的增强学习"></a>RLHF 人类反馈的增强学习</h3><p>　　RLHF是一种将微调模型行为与人类偏好和指令对齐的一种模型训练过程。首先让标注员写一个提示（prompt），然后对两个样本模型的回答根据制定的标准选择更好的一个，这种数据用于训练奖励模型（reward model）。为了多样性最大化，两个模型参数和超参数不同。为了让参与者强制选择，每个回答会打标程度（很好，好，一般，不确定）。合适的标注从两个方面考虑，有用性和安全性。在其它答案是安全的情况下，不会选择不安全的回答为最佳，因为本文认为安全回答也是更好的。<br>　　收集了更多的偏好数据后，奖励模型进步了，LLAMA 2-CHAT就会训练地更好，而它更好又会改变模型数据分布。如果不用最新的样本分布，奖励模型的精确度就会很快降低。所以在微调新一轮LLAMA 2-CHAT前使用当前最新的LLAMA 2-CHAT收集数据很重要。这一步让奖励模型保持正确的分布，也能保持最新模型的准确奖励值。<br>　　本文收集了超过一百万（1 million）人工标注数据，称做Meta奖励模型数据。不同领域提示和回答的数量不同，总结和在线公式数据一般有较长的提示，然而对话型提示通常较短。平均来看，本文的数据比开源数据集有更多轮的对话，并且更长。<br>　　奖励模型将模型回复和对应的提示（包括之前的多轮上下文）作为输入，输出一个标量分数来衡量模型生成质量。将这些分数作为奖励，通过RLHF优化LLAMA 2-CHAT获得更好的效果，提升有用性和安全性。<br>　　有用性和安全性往往需要折中，所以本文训练了两个分开的奖励模型，一个用来优化可用性（Helpfulness RM），一个用来优化安全性（Safety RM）。<br>　　本文使用预训练对话模型checkpoint来初始化奖励模型，这样可以确保所有的模型从预训练中得到知识。除了将下一个token预测分类头替换为一个标量奖励值的回归头输出以外，模型架构和超参数和预训练模型一致。  </p><p>　　为了训练奖励模型，将人工标注数据转为二分类排序标签格式（比如选择&amp;拒绝），让选择的回复有更高的分数。loss是二分类排序损失函数：$$L_{ranking}=-log(\sigma(r_\theta(x, y_c)-r_\theta(x,y_r)))$$<br>　　其中，$r_\theta(x, y)$是对于提示x和回答y的标量分数输出表示，$\theta$是模型权重。$y_c$是标注员选择的回答，$y_r$是被拒绝的回答。<br>　　输出是有四个分数的标量（比如很好），为了加大各分数差距，更好的区别有用性和安全性，将损失函数改为：<br>$$L_{ranking}=-log(\sigma(r_\theta(x, y_c)-r_\theta(x,y_r) - m(r)))$$<br>　　其中，$m(r)$是评分的离散函数，两个回答越不同，这个边距越大，两个回答越相似，这个边距越近。这个边距可以提升有用性。</p><p>　　训练细节：训了一轮，本文发现训久了过拟合。</p><p>　　本文采用了两种主要的RLHF微调算法：</p><ul><li>PPO(Proximal Policy Optimization)算法</li><li>拒绝采用微调(Rejection Sampling fine-tuning)，只在70B上用了<br>在K个模型输出中使用奖励模型选择最好的，将选择出来的做梯度更新。对于每个提示，包含最好奖励分数的样本作为新的标准。然后在新的样本中微调，加强奖励。<br>两个算法主要在广度和深度上有区别。<h3 id="多轮对话一致性的系统信息"><a href="#多轮对话一致性的系统信息" class="headerlink" title="多轮对话一致性的系统信息"></a>多轮对话一致性的系统信息</h3>有些指令应该贯穿对话始终，比如“扮演xx”指令，但是原始的RLHF模型在几轮后会忘记初始指令。为此，提出Ghost注意力（GAtt）。</li></ul><h1 id="LLaMA-Open-and-Efficient-Foundation-Language-Models"><a href="#LLaMA-Open-and-Efficient-Foundation-Language-Models" class="headerlink" title="LLaMA: Open and Efficient Foundation Language Models"></a>LLaMA: Open and Efficient Foundation Language Models</h1><p>Meta AI<br><a href="https://arxiv.org/pdf/2302.13971.pdf" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener">CODE</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>　　不像Chinchilla、PaLM或者GPT-3，只使用公开可用的数据训练。<br>　　训练不是最快的，但是推理是最快的。<br>　　本文目标是打造一系列用更多token训练的最佳推理性能的大模型。LLaMA：6.7B、13.0B、32.5B、65.2B<br>　　LLaMA-13B超过GPT3。<br>　　LLaMA-65B与Chinchilla或PaLM-540B相当。</p><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><h3 id="预训练数据"><a href="#预训练数据" class="headerlink" title="预训练数据"></a>预训练数据</h3><ul><li>English CommonCrawl(67%)：使用fastText去掉非英语文本、使用一个n-gram语言模型去掉低质量内容、用一个线性模型对维基百科中用作参考文献的页面与随机抽样的页面以及未归类为参考文献的废弃页面进行分类</li><li>C4(15%)：与CCNet的主要区别是质量过滤，它主要依赖于启发式，如标点符号的存在或网页中的单词和句子的数量</li><li>Github(4.5%):基于行长度或字母数字字符比例的启发式法过滤低质量文件</li><li>Wikipedia(4.5%)</li><li>Gutenberg and Book3(4.5%)：去掉了有90%重复的书籍</li><li>ArXiv(2.5%):删除了第一部分之前的所有内容、参考书目、.tex文件中的评论、用户编写的内联扩展定义和宏</li><li>Stack Exchange(2%)：问答数据，删除了文本中的HTML标签，并根据分数对答案进行了排序(由高到低)<br>　　分词：BPE算法<br>　　整个训练集经分词后有1.4T个token(33B和65B)</li></ul><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>　　transformer架构基础上魔改。<br>　　魔改点（中括号内为灵感来源）：</p><ul><li>预归一化[GPT3]：使用RMSNorm</li><li>SwiGLU激活函数[PaLM]</li><li>旋转embedding[GPTNeo]：将绝对位置编码换成了旋转位置嵌入RoPE</li></ul><p>　　优化器：AdamW</p><p>　　训练速度优化：</p><ul><li>随机多头注意力机制，不保存注意力权重，不计算key/query分数（masked）</li><li>减少了反向传播中重复计算的激活单元的数量，只保存最耗费计算的单元，比如线性层的输出；没有使用PyTorch autograd；使用了模型和序列并行化减少模型内存占用；尽量将激活单元的计算和GPU之间的网络通信通用</li></ul><p>　　训练时间：<br>　　65B参数模型：2048张A100 GPU，80GB内存，380 tokens/sec/GPU，1.4T tokens训练了21天</p><p>　　它甚至还写了LLAMA的碳排放情况…</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;\images\paper.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt; 　　&lt;br&gt;　　&lt;strong&gt;大模型相关论文阅读笔记。 &lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;倒序排列论文，最新阅读的在最上面。&lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;2024年1月26日更新&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="大模型" scheme="https://hubojing.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="LLM" scheme="https://hubojing.github.io/tags/LLM/"/>
    
      <category term="论文" scheme="https://hubojing.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Hexo + Github Action部署博客</title>
    <link href="https://hubojing.github.io/2023/11/16/Hexo+GithubAction%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/"/>
    <id>https://hubojing.github.io/2023/11/16/Hexo+GithubAction部署博客/</id>
    <published>2023-11-16T22:20:55.000Z</published>
    <updated>2023-11-20T13:10:55.000Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="\images\blog.png" width="300" height="180" style="float:right;"><br> 　　<br>　　<strong>真正快捷的多机部署方法！</strong><br><br>　　<strong>不是在采坑，就是在采坑的路上。</strong><br><br><br> </div><a id="more"></a><h1 id="更换部署方式的原因"><a href="#更换部署方式的原因" class="headerlink" title="更换部署方式的原因"></a>更换部署方式的原因</h1><p>（不关心的朋友可以跳过这一节）<br>　　以前许多年一直是<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code>一键部署，这样导致每台机器上都需要配置环境，以前还挺喜欢折腾这些，写过<a href="https://hubojing.github.io/2017/11/19/hexo%E5%A4%9A%E6%9C%BA%E5%90%8C%E6%AD%A5/">Hexo多机同步</a> ，但是通过篇幅可以看出，太麻烦，不符合工作后能够快速更换设备的需求。近年来我逐渐注意到CI/CD的方法，先是在Gitlab上体验了一把，确实好用，后来某天惊喜发现Github出了官方的Github Action，于是有了本文的尝试。<br>　　更换后，换机再也不用安环境了，只需要把源文件项目<code>git clone</code>一下，写文，在<code>git add .</code>、<code>git commit -m &#39;update&#39;</code>、<code>git push</code>三部曲就好啦！</p><h1 id="两个项目"><a href="#两个项目" class="headerlink" title="两个项目"></a>两个项目</h1><p>　　源文件 user/blog-source  (privare)<br>　　前端显示 username/username.github.io  (public)</p><h1 id="两把钥匙"><a href="#两把钥匙" class="headerlink" title="两把钥匙"></a>两把钥匙</h1><p>　　在源文件项目git bash<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -f blog-deploy-key</span><br></pre></td></tr></table></figure></p><p>　　生成</p><ul><li>私钥 <code>blog-deploy-key</code></li><li>公钥 <code>blog-deploy-key.pub</code></li></ul><p>　　配置：</p><ul><li>源文件项目 settings - Secrets and variables - Actions 放入私钥</li><li>前端展示项目 settings - Deploy keys 放入公钥</li></ul><h1 id="workflow"><a href="#workflow" class="headerlink" title="workflow"></a>workflow</h1><p>　　创建相关文件夹和文件，源文件根路径下blog/.github/workflows/deploy.yml<br>　　参考模板在这里<a href="https://github.com/marketplace/actions/hexo-action" target="_blank" rel="noopener">https://github.com/marketplace/actions/hexo-action</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Deploy hexo blog website.</span><br><span class="line">   - name: Deploy</span><br><span class="line">     id: deploy</span><br><span class="line">     uses: sma11black/hexo-action@v1.0.3</span><br><span class="line">     with:</span><br><span class="line">       deploy_key: $&#123;&#123; secrets.HEXO_DEPLOY_PRI &#125;&#125;</span><br><span class="line">       user_name: username  # (or delete this input setting to use bot account)</span><br><span class="line">       user_email: your email  # (or delete this input setting to use bot account)</span><br><span class="line">       PUBLISH_REPOSITORY: username/username.github.io</span><br><span class="line">       BRANCH: master</span><br><span class="line">       PUBLISH_DIR: ./public</span><br><span class="line">       commit_msg: $&#123;&#123; github.event.head_commit.message &#125;&#125;  # (or delete this input setting to use hexo default settings)</span><br></pre></td></tr></table></figure></p><h1 id="关于主题的配置"><a href="#关于主题的配置" class="headerlink" title="关于主题的配置"></a>关于主题的配置</h1><p>　　这一部分写的人太少，疯狂采坑，我来详细说一下。<br>　　主题是作为submodule加入的，但是配置如果不想被看到的话，最好单独写成文件，具体看这里：<br>　　但是单独配置文件的写法需要hexo5.0以上才能实现，indigo主题才3.0+，所以最简单的方法是fork原主题后，设置为private。<br>　　要访问private项目是需要授权的。</p><h2 id="具体做法"><a href="#具体做法" class="headerlink" title="具体做法"></a>具体做法</h2><ul><li>Github-Settings-Developer settings-Personal access tokens-Tokens(classic)-Generate new token(classic)</li><li>复制该token</li><li>在源文件项目blog-source中在源文件项目blog-source中：Settings-Secrets and variables-Actions-New repository secret，粘贴该token</li><li>在workflow的配置文件中写<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with:</span><br><span class="line">        token: $&#123;&#123; secrets.PERSONAL_TOKEN &#125;&#125;</span><br><span class="line">        submodules: true</span><br></pre></td></tr></table></figure></li></ul><h1 id="完整的deploy-yml"><a href="#完整的deploy-yml" class="headerlink" title="完整的deploy.yml"></a>完整的deploy.yml</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">name: Deploy</span><br><span class="line"></span><br><span class="line">on: [push]</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  build:</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    name: A job to deploy blog.</span><br><span class="line">    steps:</span><br><span class="line">    - name: Checkout</span><br><span class="line">      uses: actions/checkout@v1</span><br><span class="line">      with:</span><br><span class="line">        token: $&#123;&#123; secrets.PERSONAL_TOKEN &#125;&#125;</span><br><span class="line">        submodules: true # Checkout private submodules(themes or something else).</span><br><span class="line">    </span><br><span class="line">    # Caching dependencies to speed up workflows. (GitHub will remove any cache entries that have not been accessed in over 7 days.)</span><br><span class="line">    - name: Cache node modules</span><br><span class="line">      uses: actions/cache@v1</span><br><span class="line">      id: cache</span><br><span class="line">      with:</span><br><span class="line">        path: node_modules</span><br><span class="line">        key: $&#123;&#123; runner.os &#125;&#125;-node-$&#123;&#123; hashFiles(&apos;**/package-lock.json&apos;) &#125;&#125;</span><br><span class="line">        restore-keys: |</span><br><span class="line">          $&#123;&#123; runner.os &#125;&#125;-node-</span><br><span class="line">    - name: Install Dependencies</span><br><span class="line">      if: steps.cache.outputs.cache-hit != &apos;true&apos;</span><br><span class="line">      run: npm ci</span><br><span class="line">    </span><br><span class="line">    # Deploy hexo blog website.</span><br><span class="line">    - name: Deploy</span><br><span class="line">      id: deploy</span><br><span class="line">      uses: sma11black/hexo-action@v1.0.3</span><br><span class="line">      with:</span><br><span class="line">        deploy_key: $&#123;&#123; secrets.HEXO_DEPLOY_PRI &#125;&#125;</span><br><span class="line">        user_name: username  # (or delete this input setting to use bot account)</span><br><span class="line">        user_email: your email  # (or delete this input setting to use bot account)</span><br><span class="line">        PUBLISH_REPOSITORY: username/username.github.io</span><br><span class="line">        BRANCH: master</span><br><span class="line">        PUBLISH_DIR: ./public</span><br><span class="line">        commit_msg: $&#123;&#123; github.event.head_commit.message &#125;&#125;  # (or delete this input setting to use hexo default settings)</span><br><span class="line">    # Use the output from the `deploy` step(use for test action)</span><br><span class="line">    - name: Get the output</span><br><span class="line">      run: |</span><br><span class="line">        echo &quot;$&#123;&#123; steps.deploy.outputs.notify &#125;&#125;&quot;</span><br></pre></td></tr></table></figure><h1 id="无伤大雅的缺点"><a href="#无伤大雅的缺点" class="headerlink" title="无伤大雅的缺点"></a>无伤大雅的缺点</h1><p>　　这个应该是git操作的哪个部分出问题了，导致<code>git push</code>后前端项目历史push记录丢失了，损失就是github页的绿点没了，其它的没什么影响，毕竟前端页面的渲染结果没源文件有用。<code>git reflog</code>只剩下最新的一条，历史的都丢失了，不过不影响博客就算了。这个问题这些年也遇到好几次了，如果有大佬遇到过一样的问题，如愿赐教，将非常感谢。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/lastkiss/p/16955696.html" target="_blank" rel="noopener">Hexo&amp;github action持续部署</a><br><a href="https://www.taniarascia.com/git-submodules-private-content/" target="_blank" rel="noopener">Using Git Submodules for Private Content</a><br><a href="https://zhuanlan.zhihu.com/p/626270948" target="_blank" rel="noopener">github action 部署 hexo踩坑记录</a><br><a href="https://zhuanlan.zhihu.com/p/408319831" target="_blank" rel="noopener">Github Actions: submodule 下公私有仓库授权和通信</a><br><a href="https://sanonz.github.io/2020/deploy-a-hexo-blog-from-github-actions/" target="_blank" rel="noopener">利用 Github Actions 自动部署 Hexo 博客</a></p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;\images\blog.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt; 　　&lt;br&gt;　　&lt;strong&gt;真正快捷的多机部署方法！&lt;/strong&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;不是在采坑，就是在采坑的路上。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="博客" scheme="https://hubojing.github.io/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="博客" scheme="https://hubojing.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="Hexo" scheme="https://hubojing.github.io/tags/Hexo/"/>
    
      <category term="Github Action" scheme="https://hubojing.github.io/tags/Github-Action/"/>
    
  </entry>
  
  <entry>
    <title>InstructGPT笔记</title>
    <link href="https://hubojing.github.io/2023/03/14/InstructGPT/"/>
    <id>https://hubojing.github.io/2023/03/14/InstructGPT/</id>
    <published>2023-03-14T21:05:47.000Z</published>
    <updated>2023-03-14T21:05:47.000Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>简记。</strong><br><br><br> </div><a id="more"></a><h1 id="论文概况"><a href="#论文概况" class="headerlink" title="论文概况"></a>论文概况</h1><p>Training language models to follow instructions with human feedback</p><p>根据人类反馈指示来训练语言模型</p><p>OpenAI 2022</p><h1 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h1><p>该系统包含主要三个步骤实现：</p><p>1、使用一组广泛分布的互联网数据对GPT-3模型进行预训练。然后，针对典型的一组human prompts，让laber写下正确的答案并用这组12,725的监督数据对模型进行精调；</p><p>2、随机选择一组human prompts，并用模型对每个prompt产生多个输出的答案。让labeler对这些回答进行排序，并根据排序训练一个奖励模型 （reward model）。这组用来训练reward model的数据包含有33,207个prompts以及在不同回答组合下产生的10倍于此的答案；</p><p>3、再次随机采样human prompts，并基于PPO的强化学习算法（Proximal Policy Optimization Algorithm）对监督训练后精调过的模型进行再次fine-tune。每个采样的prompt输入PPO模型，并用reward model给出的奖励信号用31,144个prompts对模型进行训练。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;简记。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="GPT" scheme="https://hubojing.github.io/tags/GPT/"/>
    
  </entry>
  
  <entry>
    <title>命名实体识别（NER）论文泛读</title>
    <link href="https://hubojing.github.io/2023/03/08/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/"/>
    <id>https://hubojing.github.io/2023/03/08/命名实体识别（NER）论文泛读/</id>
    <published>2023-03-08T23:54:02.000Z</published>
    <updated>2023-03-08T23:59:44.000Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>论文泛读不定期更新。</strong><br><br><br> </div><a id="more"></a><h1 id="SpanBert"><a href="#SpanBert" class="headerlink" title="SpanBert"></a>SpanBert</h1><p><a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00300/1923170/tacl_a_00300.pdf" target="_blank" rel="noopener">PDF</a></p><h1 id="Global-Pointer-Novel-Efficient-Span-based-Approach-for-Named-Entity-Recognition"><a href="#Global-Pointer-Novel-Efficient-Span-based-Approach-for-Named-Entity-Recognition" class="headerlink" title="Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition"></a>Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition</h1><p>2022-12-11阅读</p><h2 id="论文概况"><a href="#论文概况" class="headerlink" title="论文概况"></a>论文概况</h2><p>苏剑林 2022年8月</p><h1 id="A-Unified-MRC-Framework-for-Named-Entity-Recognition"><a href="#A-Unified-MRC-Framework-for-Named-Entity-Recognition" class="headerlink" title="A Unified MRC Framework for Named Entity Recognition"></a>A Unified MRC Framework for Named Entity Recognition</h1><p>2022-12-07阅读</p><h2 id="论文概况-1"><a href="#论文概况-1" class="headerlink" title="论文概况"></a>论文概况</h2><p>ACL 2020<br><a href="https://arxiv.org/abs/1910.11476" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/ShannonAI/mrc-for-flat-nested-ner" target="_blank" rel="noopener">CODE</a></p><p>##笔记<br>使用MRC（Machine Reading Comprehension）思想，将NER任务转换为MRC任务。它能引入query先验知识，对重叠的NER实体相当于回答不同的问题，所以它能同时解决flat和nested NER问题。</p><h1 id="Named-Entity-Recognition-as-Dependency-Parsing"><a href="#Named-Entity-Recognition-as-Dependency-Parsing" class="headerlink" title="Named Entity Recognition as Dependency Parsing"></a>Named Entity Recognition as Dependency Parsing</h1><p>2022-12-05阅读</p><h2 id="论文概况-2"><a href="#论文概况-2" class="headerlink" title="论文概况"></a>论文概况</h2><p>ACL 2020<br><a href="https://aclanthology.org/2020.acl-main.577/" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/juntaoy/biaffine-ner" target="_blank" rel="noopener">CODE</a></p><h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><p>以前NER常见模式是BiLSTM+CRF，BiLSTM用于输入编码，CRF用于分类。本文提出一种双仿射模型替代CRF用于分类。<br><img src alt="架构图"></p><h1 id="Boundary-Enhanced-Neural-SpanClassification-for-Nested-Named-Entity-Recognition"><a href="#Boundary-Enhanced-Neural-SpanClassification-for-Nested-Named-Entity-Recognition" class="headerlink" title="Boundary Enhanced Neural SpanClassification for Nested Named Entity Recognition"></a>Boundary Enhanced Neural SpanClassification for Nested Named Entity Recognition</h1><p>边界增强神经跨度分类用于嵌套命名实体识别</p><p>阅读时间：2022-09-19</p><p>论文概况</p><p>AAAI 2020</p><p>阿里巴巴</p><p>Chuanqi Tan, Wei Qiu, Mosha Chen, Rui Wang, Fei Huang</p><p>问题提出</p><p>针对嵌套命名实体识别，基于span的方法有两个问题：</p><ol><li>对所有子序列进行分类在计算上是十分昂贵的，效率低下。</li><li>基于span的方法主要侧重于学习跨度表示，但缺乏明确的边界监督。</li></ol><p>为此，本文提出一种边界增强的神经跨度分类模型（BENSC），除了对span进行分类之外，本文还结合一个额外的边界检测任务来预测那些作为实体边界的单词。这两个任务在多任务学习框架下联合训练，通过额外的边界监督增强了跨度表示。被视为实体的span应该在span级别和边界级别都具有高概率。另外，边界检测模型具有生成高质量候选span的能力，大大降低了推理过程中的时间复杂度到几乎线性时间。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src alt></p><h1 id="Chinese-named-entity-recognition-The-state-of-the-art"><a href="#Chinese-named-entity-recognition-The-state-of-the-art" class="headerlink" title="Chinese named entity recognition: The state of the art"></a>Chinese named entity recognition: The state of the art</h1><p>中文命名实体识别：最新技术</p><p>阅读时间：2022-08-11</p><h2 id="论文概况-3"><a href="#论文概况-3" class="headerlink" title="论文概况"></a>论文概况</h2><p>2022年2月 Neurocomputing</p><p><a href="https://www.sciencedirect.com/science/article/pii/S0925231221016581/pdfft?md5=ab00af6205a671b5d2b841acf7111fd0&amp;pid=1-s2.0-S0925231221016581-main.pdf" target="_blank" rel="noopener">PDF</a></p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;论文泛读不定期更新。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="命名实体识别" scheme="https://hubojing.github.io/tags/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"/>
    
      <category term="NER" scheme="https://hubojing.github.io/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>向量检索技术</title>
    <link href="https://hubojing.github.io/2023/02/28/%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E6%8A%80%E6%9C%AF/"/>
    <id>https://hubojing.github.io/2023/02/28/向量检索技术/</id>
    <published>2023-02-28T21:48:28.000Z</published>
    <updated>2024-02-12T13:30:22.754Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="\images\假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>笔记。</strong><br><br><br> </div><a id="more"></a><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>大数据领域检索分两类：</p><ul><li>结构化数据检索，如ElasticSearch、Solr、关系型数据库等</li><li>非结构化数据检索，如图片、音频、视频等</li></ul><p>向量检索第一步：对非结构化的数据进行向量化表示</p><p>即物品的向量要满足相似物品的距离近，不相似的距离远，这种对物品进行特征表示的方法称为<strong>度量学习（metric learning)</strong>。</p><p>传统度量学习方法：线性投影  核方法</p><p>缺点：无法解决非线性特征</p><p>深度度量学习：通过激活函数提供非线性变换能力</p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>向量检索定义：在一个给定向量数据集中，按照某种度量方式，检索出与查询向量相近的K个向量（K-Nearest Neighbor, KNN），但KNN计算量大，通常只关注近似近邻（Approximate Nearest Neighbor, ANN）问题。</p><p>向量检索算法需解决两个问题：</p><ul><li>减少候选向量集：通过各种策略，比如构建索引结构，实现检索时绕开不相关向量；</li><li>降低单个向量计算的复杂度：找到候选向量后，要对单个向量的相似度进行计算，但复杂度搞，需要处理。</li></ul><p>经典检索算法有三个：</p><ul><li><p>NSW</p><p>关键是在构图过程中通过贪婪搜索算法记录下搜索最优路径。</p></li><li><p>HNSW</p><p>对NSW的升级，使用跳表结构代替NSW的链表结构通过空间换时间的方法将向量检索的复杂度从多重对数复杂度降至对数复杂度。</p></li><li><p>IVF_PQ</p><p>通过乘积量化（PQ）将向量进行压缩，降低计算复杂度；通过聚类加倒排（IVF）减少检索候选集。</p></li></ul><p>其它：</p><p>IVFSQ8、IVF_FLAT是IVF算法变种，分别在召回率、内存占用和响应时间做了折中处理；适用于测试生成groud truth集合的FLAT纯暴力检索算法。</p><p>常见的四种向量度量方式：</p><p>欧氏距离（L2）、余弦、内积（IP）、杰卡德距离</p><p>通常欧式距离用于图片检索，余弦用于人脸识别，内积、杰卡德距离多用于推荐。</p><h1 id="一些结论"><a href="#一些结论" class="headerlink" title="一些结论"></a>一些结论</h1><p>高召回率从高到底</p><p>FLAT（仅供测试使用） &gt; HNSW &gt; IVFFLAT &gt; IVF*SQ8 *&gt; IVF_PQ</p><p>查询响应时间从低到高</p><p>HNSW &gt; IVF*PQ *&gt; IVF_SQ8 &gt; IVF_FLAT &gt; FLAT</p><p>资源占用从高到底</p><p>IVF_PQ &gt; IVF_SQ8 &gt; HNSW</p><p>如，内存和磁盘足够，百万~千万级，选HNSW算法；召回率要求不高，相应时间要求较高，集群资源有限，数据集较大（亿级），选IVF_SQ8算法。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;\images\假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;笔记。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="软件开发" scheme="https://hubojing.github.io/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="向量检索" scheme="https://hubojing.github.io/tags/%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>2022年终总结</title>
    <link href="https://hubojing.github.io/2023/01/01/2022%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    <id>https://hubojing.github.io/2023/01/01/2022年终总结/</id>
    <published>2023-01-01T21:20:04.000Z</published>
    <updated>2024-02-12T13:30:22.746Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="\images\假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>短短总结一下。</strong><br><br><br> </div><a id="more"></a><p>2022年是生命长河里记忆深刻的一年。<br>我再次离开校园，走向职场。</p><p>技术博客只谈技术。</p><h1 id="上半场"><a href="#上半场" class="headerlink" title="上半场"></a>上半场</h1><p>在学校时，我的研究方向是推荐算法、时空数据挖掘，小方向是兴趣点推荐。虽然现在没有从事推荐算法的工作，但我依然经常阅读和跟进推荐算法相关的技术，作为一种爱好吧。<br><a href="https://github.com/hubojing/POI-Recommendation" target="_blank" rel="noopener">POI Recommendation</a><br>记得在学校时，有同学研究和交通有关的课题，那时的我对交通方面是没什么兴趣的。现在，当我人在外地时，想着回家可以快点、再快点时，突然明白了交通的重要性，明白了提速对于社会进步的重要性。</p><h1 id="下半场"><a href="#下半场" class="headerlink" title="下半场"></a>下半场</h1><p>进入职场后，我的研究方向是NLP算法，信息抽取方向。<br>对我来说，信息抽取是一个新方向，虽然之前接触到一些旁支，比如知识图谱相关的工作，但是没有专门研究过这一块。<br>所以一切都从最初快速学起。<br><a href="https://github.com/hubojing/Information-Extraction-Papers" target="_blank" rel="noopener">Information-Extraction-Papers</a><br>在团队里认识了很多技术大佬，他们技术扎实，研究能力强，大家每周围在一起讨论的感觉很有读研时的样子。我可以感受到他们对技术和研究的热爱，当提到竞赛、模型和算法时，他们的眼神都会亮起来。热爱是能力提升的最大动力，我很荣幸能够有机会与他们共事。这种研究的氛围我很喜欢。</p><h1 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h1><p>技术展望的话，这一年我突然觉得机器人方向才是未来最有潜力的方向。<br>随着最近ChatGPT的大火，大模型或有一统NLP的趋势。</p><p>博客的更新速度这半年没有跟上。一方面，由于人在外地，日常通过平板远程到自用笔记本，键鼠操作的延迟不是很方便，导致自己不太愿意去写。另一方面，刚开始花费了一定的精力和时间去体验一个人在外的感觉，后来因为疫情又担心在家的爸妈，再后来，自己也阳了……每日工作后略感疲惫，心有余而力不足。找起借口来，总是有话说。总之，我对目前自身的技术现状是不够满意的。</p><p>一晃工作也半年了，业务、团队和环境都逐渐熟悉和适应了，身体也逐渐康复，理应在2023年里，在技术层面有一个新的提升。心里默默立了一些Flag，就不写出来献丑了。</p><p>希望自己能把技术打扎实，不要一直是浮在表面的菜狗一条。</p><p>本来想写的全面一点，但是阳康后不能熬夜，就先这样吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;\images\假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;短短总结一下。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="杂谈" scheme="https://hubojing.github.io/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="年终总结" scheme="https://hubojing.github.io/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Python深拷贝与浅拷贝bug实例浅析</title>
    <link href="https://hubojing.github.io/2022/12/03/Python%E6%B7%B1%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B5%85%E6%8B%B7%E8%B4%9Dbug%E5%AE%9E%E4%BE%8B%E6%B5%85%E6%9E%90/"/>
    <id>https://hubojing.github.io/2022/12/03/Python深拷贝与浅拷贝bug实例浅析/</id>
    <published>2022-12-03T23:26:08.000Z</published>
    <updated>2024-02-12T13:30:22.750Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>一个bug引起的……thinking</strong><br><br><br> </div><a id="more"></a><h1 id="需求简述"><a href="#需求简述" class="headerlink" title="需求简述"></a>需求简述</h1><p>　　将excel学生信息表转换为json格式。<br>　　其中代码有一步要将excel每行的数据按照json模板格式替换掉默认值。</p><h1 id="原代码"><a href="#原代码" class="headerlink" title="原代码"></a>原代码</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetStudentInfo</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, student_info_path)</span>:</span></span><br><span class="line">        self.student_info_path = student_info_path</span><br><span class="line">        self.template = &#123;</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"ZhangSan"</span>,</span><br><span class="line">            <span class="string">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">            <span class="string">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">            <span class="string">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">            <span class="string">"id"</span>: <span class="string">"0"</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_new_student</span><span class="params">(self, name, student_id)</span>:</span></span><br><span class="line">        new_student = self.template</span><br><span class="line">        new_student[<span class="string">'name'</span>] = name</span><br><span class="line">        new_student[<span class="string">'id'</span>] = student_id</span><br><span class="line">        <span class="keyword">return</span> new_student</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_whole_stu_info</span><span class="params">(self)</span>:</span></span><br><span class="line">        students = &#123;&#125;</span><br><span class="line">        tables = xlrd.open_workbook(self.student_info_path)</span><br><span class="line">        table = tables.sheets()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">0</span>, table.nrows - <span class="number">1</span>):</span><br><span class="line">            name = table.cell_value(row + <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">            student_id = table.cell_value(row + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            new_student = self.create_new_student(name, student_id)</span><br><span class="line">            students[str(row)] = new_student</span><br><span class="line">        self.get_new_file(students)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_new_file</span><span class="params">(self, students)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'./output.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">            json.dump(students, file, indent=<span class="number">4</span>, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==  <span class="string">'__main__'</span>:</span><br><span class="line">    student_info_path = <span class="string">'./student_info.xlsx'</span></span><br><span class="line">    data = GetStudentInfo(student_info_path)</span><br><span class="line">    data.get_whole_stu_info()</span><br></pre></td></tr></table></figure><p>　　输出文件为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"0"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"GouDong"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">4.0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"1"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"GouDong"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">4.0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"2"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"GouDong"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">4.0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"3"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"GouDong"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">4.0</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Bug定位"><a href="#Bug定位" class="headerlink" title="Bug定位"></a>Bug定位</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_new_student</span><span class="params">(self, name, student_id)</span>:</span></span><br><span class="line">    new_student = self.template <span class="comment"># 这一行有问题！</span></span><br><span class="line">    print(id(new_student))</span><br><span class="line">    new_student[<span class="string">'name'</span>] = name</span><br><span class="line">    new_student[<span class="string">'id'</span>] = student_id</span><br><span class="line">    <span class="keyword">return</span> new_student</span><br></pre></td></tr></table></figure><p>　　发现每一次的<code>new_student</code>的id是一样的<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2110590375320</span></span><br><span class="line"><span class="number">2110590375320</span></span><br><span class="line"><span class="number">2110590375320</span></span><br><span class="line"><span class="number">2110590375320</span></span><br></pre></td></tr></table></figure></p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>修改为</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_new_student</span><span class="params">(self, name, student_id)</span>:</span></span><br><span class="line">    new_student = copy.deepcopy(self.template) <span class="comment"># 修改后（法一）</span></span><br><span class="line">    <span class="comment"># new_student = copy.copy(self.template) # 修改后（法二）</span></span><br><span class="line">    print(id(new_student))</span><br><span class="line">    new_student[<span class="string">'name'</span>] = name</span><br><span class="line">    new_student[<span class="string">'id'</span>] = student_id</span><br><span class="line">    <span class="keyword">return</span> new_student</span><br></pre></td></tr></table></figure><p>　　此时输出的id不同了：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2392740865832</span><br><span class="line">2392740866072</span><br><span class="line">2392740866152</span><br><span class="line">2392740865752</span><br></pre></td></tr></table></figure></p><p>　　新的输出文件为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"0"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"LiuBo"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">1.0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"1"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"BoCai"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">2.0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"2"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"CaiGou"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">3.0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"3"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"GouDong"</span>,</span><br><span class="line">        <span class="attr">"sex"</span>: <span class="string">"female"</span>,</span><br><span class="line">        <span class="attr">"grade"</span>: <span class="string">"6"</span>,</span><br><span class="line">        <span class="attr">"age"</span>: <span class="string">"12"</span>,</span><br><span class="line">        <span class="attr">"id"</span>: <span class="number">4.0</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>　　原代码中，<code>new_student = self.template</code>每一次都将*<code>new_student</code>指向<code>self.template</code>，<code>students[str(row)] = new_student</code>每一次都将<code>students[str(row)]</code>指向<code>new_student</code>。所以每次<code>new_student</code>修改后，<code>students[str(row)]</code>的全部值都会更改为最新版。</p><p>　　若要避免该问题，就涉及到浅拷贝和深拷贝的问题。</p><ul><li><strong>赋值</strong>：仅仅是个别名，引用，指向原有地址，id的地址和原有地址相同。（就像快捷方式。）</li><li><strong>浅拷贝</strong>：第一层拷贝了，里面子文件全是引用。（先建一个新对象，对象地址是新的，里面放原数据的地址，就像一个文件夹里放的全是快捷方式。）</li><li><strong>深拷贝</strong>：新对象的内存地址也会重新分配，跟原来的内存地址不一样。完全弄一个克隆版，克隆体和本体没有关系了，本体改了克隆体不变。（先建一个新对象，对象地址是新的，里面放的全是克隆体，其地址也是新的。就像一个文件夹里放的全是文件，而不是快捷方式。）</li></ul><p>　　再要分清Python里，“=”号、<code>copy.copy</code>和<code>copy.deepcopy</code>三者的区别。</p><ul><li>“=”号：对应赋值<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line">b = a</span><br><span class="line">print(<span class="string">'原来的a'</span>, a, <span class="string">'地址'</span>, id(a))</span><br><span class="line">print(<span class="string">'b'</span>, b, <span class="string">'地址'</span>, id(b))</span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">print(<span class="string">'此时的a'</span>, a, <span class="string">'地址'</span>, id(a))</span><br><span class="line">print(<span class="string">'修改后的b'</span>, b, <span class="string">'地址'</span>, id(b))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># 原来的a 1 地址 140720364485696</span></span><br><span class="line"><span class="comment"># b 1 地址 140720364485696</span></span><br><span class="line"><span class="comment"># 此时的a 1 地址 140720364485696</span></span><br><span class="line"><span class="comment"># 修改后的b 2 地址 140720364485728</span></span><br></pre></td></tr></table></figure></li></ul><p>修改后b此时地址变了，因为赋给一个全新完整的变量会重新生成新地址。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">c = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">print(<span class="string">'原来的c'</span>, c, <span class="string">'地址'</span>, id(c))</span><br><span class="line">d = c</span><br><span class="line">print(<span class="string">'d'</span>, d, <span class="string">'地址'</span>, id(d))</span><br><span class="line">d[<span class="number">0</span>] = <span class="number">3</span></span><br><span class="line">print(<span class="string">'此时的c'</span>, c, <span class="string">'地址'</span>, id(c))</span><br><span class="line">print(<span class="string">'修改后的d'</span>, d, <span class="string">'地址'</span>, id(d))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># 原来的c [1, 2] 地址 2758022890888</span></span><br><span class="line"><span class="comment"># d [1, 2] 地址 2758022890888</span></span><br><span class="line"><span class="comment"># 此时的c [3, 2] 地址 2758022890888</span></span><br><span class="line"><span class="comment"># 修改后的d [3, 2] 地址 2758022890888</span></span><br></pre></td></tr></table></figure></p><p>　　修改后d地址没变，因为只修改了d内的部分值。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">e = &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"ZhangSan"</span>,</span><br><span class="line">    <span class="string">"id"</span>: <span class="string">"0"</span></span><br><span class="line">&#125;</span><br><span class="line">print(<span class="string">'原来的e'</span>, e, <span class="string">'地址'</span>, id(e))</span><br><span class="line">f = e</span><br><span class="line">print(<span class="string">'f'</span>, f, <span class="string">'地址'</span>, id(f))</span><br><span class="line">f[<span class="string">'id'</span>] = <span class="string">'1'</span></span><br><span class="line">print(<span class="string">'此时的e'</span>, e, <span class="string">'地址'</span>, id(e))</span><br><span class="line">print(<span class="string">'修改后的f'</span>, f, <span class="string">'地址'</span>, id(f))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># 原来的e &#123;'name': 'ZhangSan', 'id': '0'&#125; 地址 2001978290072</span></span><br><span class="line"><span class="comment"># f &#123;'name': 'ZhangSan', 'id': '0'&#125; 地址 2001978290072</span></span><br><span class="line"><span class="comment"># 此时的e &#123;'name': 'ZhangSan', 'id': '1'&#125; 地址 2001978290072</span></span><br><span class="line"><span class="comment"># 修改后的f &#123;'name': 'ZhangSan', 'id': '1'&#125; 地址 2001978290072</span></span><br></pre></td></tr></table></figure></p><p>　　修改后f地址没变，因为只修改了f内的部分值。</p><ul><li><code>copy.copy</code>：对应浅拷贝</li><li><code>copy.deepcopy</code>：对应深拷贝<br>　　官方文档：<a href="https://docs.python.org/3/library/copy.html" target="_blank" rel="noopener">copy函数</a><blockquote><p>The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances):<br>A shallow copy constructs a new compound object and then (to the extent possible) inserts <strong>references</strong> into it to the objects found in the original.<br>A deep copy constructs a new compound object and then, recursively, inserts <strong>copies</strong> into it of the objects found in the original.</p></blockquote></li></ul><p>　　注意加粗字体，区别在于一个是引用，一个是复制体本身。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;一个bug引起的……thinking&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="编程语言" scheme="https://hubojing.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="Python" scheme="https://hubojing.github.io/tags/Python/"/>
    
      <category term="浅拷贝" scheme="https://hubojing.github.io/tags/%E6%B5%85%E6%8B%B7%E8%B4%9D/"/>
    
      <category term="深拷贝" scheme="https://hubojing.github.io/tags/%E6%B7%B1%E6%8B%B7%E8%B4%9D/"/>
    
  </entry>
  
  <entry>
    <title>Python数据处理代码笔记</title>
    <link href="https://hubojing.github.io/2022/10/06/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
    <id>https://hubojing.github.io/2022/10/06/Python数据处理代码笔记/</id>
    <published>2022-10-06T14:59:36.000Z</published>
    <updated>2024-02-12T13:30:22.750Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>自用，一些工具代码。</strong><br><br><br> </div><a id="more"></a><h1 id="pd读取excel"><a href="#pd读取excel" class="headerlink" title="pd读取excel"></a>pd读取excel</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = pd.DataFrame(pd.read_excel(excel_path))</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> data.itertuples():</span><br><span class="line">    id = getattr(row, <span class="string">'uid'</span>)</span><br><span class="line">    text = getattr(row, <span class="string">'文本'</span>)</span><br><span class="line">    <span class="keyword">if</span> pd.isna(text):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    info = re.findall(<span class="string">r'"重要"："([^"]+)",'</span>, str(text))</span><br></pre></td></tr></table></figure><h1 id="pd输出excel"><a href="#pd输出excel" class="headerlink" title="pd输出excel"></a>pd输出excel</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_excel</span><span class="params">(outputdata, result_path)</span>:</span></span><br><span class="line">    title = [<span class="string">'姓名'</span>, <span class="string">'性别'</span>, <span class="string">'年龄'</span>]</span><br><span class="line">    writer = pd.ExcelWriter(result_path)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(outputdata, columns=title)</span><br><span class="line">    df.to_excel(writer, sheet_name=<span class="string">'Sheet1'</span>, index=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    writer.save()</span><br></pre></td></tr></table></figure><h1 id="读取json"><a href="#读取json" class="headerlink" title="读取json"></a>读取json</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">files = os.listdir(data_dir)</span><br><span class="line"><span class="keyword">for</span> jsonfile <span class="keyword">in</span> files:</span><br><span class="line">    json_data = json.load(open(data_dir + jsonfile, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure><h1 id="输出json"><a href="#输出json" class="headerlink" title="输出json"></a>输出json</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_json</span><span class="params">(outputdata, result_path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(result_path, <span class="string">'w+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        json.dump(outputdata, file, indent=<span class="number">4</span>, ensure_ascii=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><h1 id="读写excel"><a href="#读写excel" class="headerlink" title="读写excel"></a>读写excel</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line">file = xlrd.open_workbook(<span class="string">'test.xlsx'</span>)</span><br><span class="line">sheet = flie.sheets()[<span class="number">0</span>]</span><br><span class="line">rows = sheet.nrows</span><br><span class="line">cols = sheet.ncols</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(rows):</span><br><span class="line">    val = sheet.cell_value(i, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h1 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">User = dict()</span><br><span class="line">User[id] = []</span><br><span class="line">User[id].append([val1, val2])</span><br><span class="line">userdic = dict()</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> data.readlines():</span><br><span class="line">    linestr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">    tmp1 = int(linestr[<span class="number">0</span>])</span><br><span class="line">    tmp2 = int(linestr[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> tmp1 <span class="keyword">not</span> <span class="keyword">in</span> userdic:</span><br><span class="line">        userdic[tmp1] = set()</span><br><span class="line">    userdic[tmp1].add(tmp2)</span><br></pre></td></tr></table></figure><h1 id="读写word"><a href="#读写word" class="headerlink" title="读写word"></a>读写word</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> win32com <span class="keyword">import</span> client <span class="keyword">as</span> wc</span><br><span class="line"></span><br><span class="line">word = wc.Dispatch(<span class="string">"Word.Application"</span>)</span><br><span class="line"></span><br><span class="line">bookList = os.listdir(<span class="string">r'./result_new_2/'</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> bookList:</span><br><span class="line">    <span class="comment">#print(file)</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        doc = word.Documents.Open(<span class="string">"C:\\docdir\\"</span> + file)</span><br><span class="line">        doc.SaveAs(<span class="string">"&#123;&#125;x"</span>.format(<span class="string">"C:\\docxdir\\"</span> + file), <span class="number">12</span>)<span class="comment">#另存为后缀为".docx"的文件，其中参数12指docx文件</span></span><br><span class="line">        doc.Close()</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(file)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">word.Quit()</span><br><span class="line">print(<span class="string">"完成！"</span>)</span><br></pre></td></tr></table></figure><h1 id="re"><a href="#re" class="headerlink" title="re"></a>re</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text = re.findall(<span class="string">r'"type":"([^"]+)",'</span>, str(longtext))</span><br></pre></td></tr></table></figure><h1 id="以前的笔记"><a href="#以前的笔记" class="headerlink" title="以前的笔记"></a>以前的笔记</h1><p><a href="https://hubojing.github.io/2021/07/11/Pandas%E6%9D%82%E8%AE%B0/">Pandas自用笔记</a></p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;自用，一些工具代码。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Waline评论服务端转移至Deta</title>
    <link href="https://hubojing.github.io/2022/09/12/Waline%E8%AF%84%E8%AE%BA%E6%9C%8D%E5%8A%A1%E7%AB%AF%E8%BD%AC%E7%A7%BB%E8%87%B3Deta/"/>
    <id>https://hubojing.github.io/2022/09/12/Waline评论服务端转移至Deta/</id>
    <published>2022-09-12T11:24:26.000Z</published>
    <updated>2024-02-12T13:30:22.750Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>和评论系统battle的第N年</strong><br>　　<strong>折腾不息</strong><br><br><br> </div><a id="more"></a><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>前阵子评论系统又挂了，原因是*.vercel.app域名被污染。</p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>法一：服务端换个域名<br>法二：换个服务端部署</p><p>我选法二。</p><h1 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h1><p>DETA官网：<a href="https://www.deta.sh/" target="_blank" rel="noopener">https://www.deta.sh/</a></p><blockquote><p>Deta is free for ever.</p></blockquote><p>这句话很不错有木有~</p><ol><li>注册</li><li>根据注册后的引导新建一个默认的project</li><li>点击 <a href="https://web.deta.sh/deploy?path=https://github.com/walinejs/deta-starter" target="_blank" rel="noopener">https://web.deta.sh/deploy?path=https://github.com/walinejs/deta-starter</a> 将Waline快速部署到deta平台</li><li>将部署url写入前端脚本的 serverURL 配置中，此时更新就可看到评论</li><li><p>将项目放到本地管理</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mac or Linux</span></span><br><span class="line">curl -fsSL https://get.deta.dev/cli.sh | sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Windows for powershell</span></span><br><span class="line">iwr https://get.deta.dev/cli.ps1 -useb | iex</span><br></pre></td></tr></table></figure></li><li><p>cmd<code>deta login</code>登录</p></li><li>复制并执行页面中的<code>deta clone</code>命令，将项目下载到本地</li><li><code>deta deploy</code>实现部署</li><li>本地项目新增<code>.env</code>文件，将需要修改的环境变量使用<code>VAR_NAME=VALUE</code>的形式一行一个写在文件中</li><li>使用<code>deta update -e .env</code>进行环境变量更新</li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://waline.js.org/guide/server/deta.html" target="_blank" rel="noopener">Waline手册-Deta部署</a></li><li><a href="https://docs.deta.sh/docs/cli/install/" target="_blank" rel="noopener">Deta CLI</a></li><li><a href="https://docs.deta.sh/docs/micros/env_vars/#setting-environment-variables" target="_blank" rel="noopener">Deta环境变量配置</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;和评论系统battle的第N年&lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;折腾不息&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="博客" scheme="https://hubojing.github.io/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="博客" scheme="https://hubojing.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="Waline" scheme="https://hubojing.github.io/tags/Waline/"/>
    
      <category term="Deta" scheme="https://hubojing.github.io/tags/Deta/"/>
    
  </entry>
  
  <entry>
    <title>关系抽取（RE）论文泛读</title>
    <link href="https://hubojing.github.io/2022/08/16/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%EF%BC%88RE%EF%BC%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/"/>
    <id>https://hubojing.github.io/2022/08/16/关系抽取（RE）论文泛读/</id>
    <published>2022-08-16T19:37:35.000Z</published>
    <updated>2024-02-12T13:30:22.754Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/沉醉于知识的芬芳.png" width="300" height="180" style="float:right;"><br><br><br>　　<strong>论文泛读不定期更新。</strong><br><br><br> </div><a id="more"></a><h1 id="Document-Level-Relation-Extraction-with-Adaptive-Focal-Loss-and-Knowledge-Distillation"><a href="#Document-Level-Relation-Extraction-with-Adaptive-Focal-Loss-and-Knowledge-Distillation" class="headerlink" title="Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation"></a>Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</h1><p>具有自适应焦点损失和知识蒸馏的文档级关系抽取<br>阅读时间：2022-08-15</p><h2 id="论文概况"><a href="#论文概况" class="headerlink" title="论文概况"></a>论文概况</h2><p>ACL 2022<br>阿里达摩院<br>Qingyu Tan, Ruidan He, Lidong Bing, Hwee Tou Ng<br><a href="https://arxiv.org/pdf/2203.10900" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/tonytan48/KD-DocRE" target="_blank" rel="noopener">CODE</a></p><h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><p>文档级关系抽取要同时从多个句子中提取关系。本文提出DocRE算法，一个用于文档级别的关系抽取半监督算法，它有三个新组件。第一，用轴向注意力模块学习实体对之间的依赖关系。第二，提出了一个自适应的焦点损失来解决DocRE中类的不平衡问题。最后，利用知识蒸馏来克服人工标注数据与远程监督数据之间的差异。<br>现有问题：现存的方法关注实体对的句法特征，而忽略了实体对之间的交互作用；目前还没有工作可以直接的解决类的不平衡问题。现存的工作仅仅关注阈值学习来平衡正例和负例，但正例内部的类不平衡问题并没有得到解决；关于将远程监督数据应用于DocRE任务的研究很少。<br>贡献点：轴向注意力（提升two-hop关系的推理能力）、自适应焦点损失（解决标签分配不平衡的问题，长尾类在总的损失中占比较多）、知识蒸馏（克服标注数据和远程监督数据之间的差异）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/KD-DocRE.png" alt="KD-DocRE" title>                </div>                <div class="image-caption">KD-DocRE</div>            </figure></p><h1 id="Packed-Levitated-Marker-for-Entity-and-Relation-Extraction"><a href="#Packed-Levitated-Marker-for-Entity-and-Relation-Extraction" class="headerlink" title="Packed Levitated Marker for Entity and Relation Extraction"></a>Packed Levitated Marker for Entity and Relation Extraction</h1><p>打包悬浮标记用于实体和关系抽取<br>阅读时间：2022-08-15</p><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><p>ACL 2022<br>Deming Ye, Yankai Lin, Peng Li, Maosong Sun<br>清华大学与腾讯微信模式识别中心合作<br><a href="https://aclanthology.org/2022.acl-long.337/" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/thunlp/PL-Marker" target="_blank" rel="noopener">CODE</a></p><h2 id="笔记-1"><a href="#笔记-1" class="headerlink" title="笔记"></a>笔记</h2><p>最近的命名实体识别和关系抽取工作专注于研究如何从预训练模型中获得更好的span表示。然而，许多工作忽略了span之间的相互关系。本文提出了一种基于悬浮标记的span表示方法，在编码过程中通过特定策略打包标记来考虑span之间的相互关系。对于命名实体识别任务，提出了一种面向邻居span的打包策略，以更好地建模实体边界信息。对于关系抽取任务，设计了一种面向头实体的打包策略，将每个头实体以及可能的尾实体打包，以共同建模同头实体的span对。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/PL-Marker.png" alt="PL-Marker" title>                </div>                <div class="image-caption">PL-Marker</div>            </figure></p><h1 id="Consistent-Representation-Learning-for-Continual-Relation-Extraction"><a href="#Consistent-Representation-Learning-for-Continual-Relation-Extraction" class="headerlink" title="Consistent Representation Learning for Continual Relation Extraction"></a>Consistent Representation Learning for Continual Relation Extraction</h1><p>一致表示学习用于连续关系抽取<br>阅读时间：2022-08-12</p><h2 id="论文概况-1"><a href="#论文概况-1" class="headerlink" title="论文概况"></a>论文概况</h2><p>ACL 2022<br>Kang Zhao, Hua Xu, Jiangong Yang, Kai Gao<br><a href="https://arxiv.org/pdf/2203.02721" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/thuiar/CRL" target="_blank" rel="noopener">CODE</a></p><h2 id="笔记-2"><a href="#笔记-2" class="headerlink" title="笔记"></a>笔记</h2><p>通过对比学习和回放记忆时的知识蒸馏，提出一种新颖的一致性表示学习方法。使用基于记忆库的监督对比学习来训练每一个新的任务，以使模型高效学习特征表示。为了防止对老任务的遗忘，构造了记忆样本的连续回放，同时让模型保留在知识蒸馏中历史任务之间的关系。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/CRL.png" alt="CRL" title>                </div>                <div class="image-caption">CRL</div>            </figure></p><h1 id="Pre-training-to-Match-for-Unified-Low-shot-Relation-Extraction"><a href="#Pre-training-to-Match-for-Unified-Low-shot-Relation-Extraction" class="headerlink" title="Pre-training to Match for Unified Low-shot Relation Extraction"></a>Pre-training to Match for Unified Low-shot Relation Extraction</h1><p>预训练用于匹配统一少样本关系抽取<br>阅读时间：2022-08-12</p><h2 id="论文概况-2"><a href="#论文概况-2" class="headerlink" title="论文概况"></a>论文概况</h2><p>ACL 2022<br>Fangchao Liu, Hongyu Lin, Xianpei Han, Boxi Cao, Le Sun<br><a href="https://arxiv.org/pdf/2203.12274" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/fc-liu/MCMN" target="_blank" rel="noopener">CODE</a></p><h2 id="笔记-3"><a href="#笔记-3" class="headerlink" title="笔记"></a>笔记</h2><p>低样本关系抽取旨在少样本甚至零样本场景下的关系抽取。由于低样本关系抽取所包含任务形式多样，传统方法难以统一处理。本文针对这一问题，提出了一种统一的低样本匹配网络：（1）基于语义提示（prompt）范式，构造了从关系描述到句子实例的匹配网络模型；（2）针对匹配网络模型学习，设计了三元组-复述的预训练方法，以增强模型对关系描述与实例之间语义匹配的泛化性。在零样本、小样本以及带负例的小样本关系抽取评测基准上的实验结果表明，该方法能有效提升低样本场景下关系抽取的性能，并且具备了较好的任务自适应能力。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/MCMN.png" alt="MCMN" title>                </div>                <div class="image-caption">MCMN</div>            </figure></p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/沉醉于知识的芬芳.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;论文泛读不定期更新。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="关系抽取" scheme="https://hubojing.github.io/tags/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/"/>
    
      <category term="RE" scheme="https://hubojing.github.io/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>兴趣点推荐（POI Recommendation）论文泛读</title>
    <link href="https://hubojing.github.io/2022/08/14/%E5%85%B4%E8%B6%A3%E7%82%B9%E6%8E%A8%E8%8D%90%EF%BC%88POI%20Recommendation%EF%BC%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/"/>
    <id>https://hubojing.github.io/2022/08/14/兴趣点推荐（POI Recommendation）论文泛读/</id>
    <published>2022-08-14T22:34:04.000Z</published>
    <updated>2024-02-12T13:30:22.754Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/沉醉于知识的芬芳.png" width="300" height="180" style="float:right;"><br><br><br>　　<strong>论文泛读不定期更新。</strong><br><br><br> </div><a id="more"></a><h1 id="Hierarchical-Multi-Task-Graph-Recurrent-Network-for-Next-POI-Recommendation"><a href="#Hierarchical-Multi-Task-Graph-Recurrent-Network-for-Next-POI-Recommendation" class="headerlink" title="Hierarchical Multi-Task Graph Recurrent Network for Next POI Recommendation"></a>Hierarchical Multi-Task Graph Recurrent Network for Next POI Recommendation</h1><p>分层多任务图循环网络用于下一个兴趣点推荐<br>阅读时间：2022-08-11</p><h2 id="论文概况"><a href="#论文概况" class="headerlink" title="论文概况"></a>论文概况</h2><p>SIGIR2022<br>Nicholas Lim, Bryan Hooi, See-Kiong Ng,Yong Liang Goh, Renrong Weng, Rui Tan<br><a href="https://bhooi.github.io/papers/hmt_sigir22.pdf" target="_blank" rel="noopener">PDF</a></p><h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><p>解决问题：数据稀疏（用户-兴趣点矩阵稀疏）<br>提出HMT-GRN算法，该方法通过在多任务设置中学习不同的低稀疏用户区域矩阵来缓解数据稀疏问题，GRN模块同时对顺序依赖关系和全局时空POI-POI关系进行建模，然后对不同的区域和兴趣点分布采用分层束搜索（HBS），随着空间粒度增加来分层减少搜索空间并且预测下一个兴趣点。本文HBS通过减少搜索空间来提高效率，与穷举法相比速度提升5~7倍。本文还提出了一种新颖的选择层来预测下一个兴趣点用户是否曾经访问过，在个性化和探索之间取得平衡。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/HMT-GRN.png" alt="HMT-GRN" title>                </div>                <div class="image-caption">HMT-GRN</div>            </figure><p>个人备注：把bean search运用在POI推荐中；对于已访问过的POI设置了一个选择概率。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/沉醉于知识的芬芳.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;论文泛读不定期更新。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="POI" scheme="https://hubojing.github.io/tags/POI/"/>
    
      <category term="兴趣点推荐" scheme="https://hubojing.github.io/tags/%E5%85%B4%E8%B6%A3%E7%82%B9%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>实体链接（EL）论文泛读</title>
    <link href="https://hubojing.github.io/2022/08/14/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5%EF%BC%88EL%EF%BC%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/"/>
    <id>https://hubojing.github.io/2022/08/14/实体链接（EL）论文泛读/</id>
    <published>2022-08-14T16:26:09.000Z</published>
    <updated>2024-02-12T13:30:22.754Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/沉醉于知识的芬芳.png" width="300" height="180" style="float:right;"><br><br><br>　　<strong>论文泛读不定期更新。</strong><br><br><br> </div><a id="more"></a><h1 id="Entity-linking-meets-deep-learning-Techniques-and-solutions"><a href="#Entity-linking-meets-deep-learning-Techniques-and-solutions" class="headerlink" title="Entity linking meets deep learning: Techniques and solutions"></a>Entity linking meets deep learning: Techniques and solutions</h1><p>实体链接遇到深度学习：技术和解决方法<br>阅读时间：2022-08-11</p><h2 id="论文概况"><a href="#论文概况" class="headerlink" title="论文概况"></a>论文概况</h2><p>2021年 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING<br>CCF-A<br>Wei Shen, Yuhan Li, Yinan Liu, Jiawei Han,Fellow, IEEE, Jianyong Wang,Fellow, IEEE, Xiaojie Yuan<br><a href="https://arxiv.org/pdf/2109.12520" target="_blank" rel="noopener">PDF</a></p><h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><p>从三个方面展开：嵌入（Embedding）、特征（Feature）、算法（Algorithm）<br>Embedding包括字（word）嵌入、Mention嵌入、实体（entity）嵌入、对齐（aligenment）嵌入。<br>特征包括先验流行度、表面形式相似度、类型相似度、上下文相似度、主题连贯性。<br>算法包括MLP、基于图的算法、强化学习。<br>给出了十种广泛使用的实体链接数据集。<br>未来方向：多源异质文本数据、NER和EL联合、更高级的语言模型、EL模型鲁棒性</p><h1 id="Multilingual-Autoregressive-Entity-Linking"><a href="#Multilingual-Autoregressive-Entity-Linking" class="headerlink" title="Multilingual Autoregressive Entity Linking"></a>Multilingual Autoregressive Entity Linking</h1><p>多语言自回归实体链接<br>阅读时间：2022-08-11</p><h2 id="论文概况-1"><a href="#论文概况-1" class="headerlink" title="论文概况"></a>论文概况</h2><p>2022年3月 Transactions of the Association for Computational Linguistics SCI Q1<br>Nicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe,Naman Goyal, Mikhail Plekhanov, Luke Zettlemoyer,Nicola Cancedda, Sebastian Riedel1,6, Fabio Petroni<br>Facebook AI<br><a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00460/2004070/tacl_a_00460.pdf" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/facebookresearch/GENRE" target="_blank" rel="noopener">CODE</a></p><h2 id="笔记-1"><a href="#笔记-1" class="headerlink" title="笔记"></a>笔记</h2><p>提出mGENRE系统，它是一个用于多语言实体链接问题的序列到序列的系统，用于解析特定语言mention到多语言知识库。对于特定语言的mention，mGENRE以自回归的方式从左到右逐个（left-to-right, token-by-token）标记预测目标实体的名称。自回归公式有效地交叉编码关于字符串和实体名称，用来捕获比标准点积更多的交互。它还可以在大知识库中进行快速搜索，即使对于没出现在mention表中和不用大规模向量索引的mention也是如此。虽然先前的MEL工作对每个实体使用单一表示，但我们匹配尽可能多的多语言的实体名称，这允许利用源输入和目标名称之间的语言连接。此外，在完全没有训练数据的语言的零样本设置中，mGENRE将目标语言视为在预测时被边缘化的潜在变量。这使平均准确度提高了50%以上。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/mGENRE.png" alt="mGENRE" title>                </div>                <div class="image-caption">mGENRE</div>            </figure></p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/沉醉于知识的芬芳.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;论文泛读不定期更新。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="实体链接" scheme="https://hubojing.github.io/tags/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5/"/>
    
  </entry>
  
  <entry>
    <title>BERT</title>
    <link href="https://hubojing.github.io/2022/08/09/BERT/"/>
    <id>https://hubojing.github.io/2022/08/09/BERT/</id>
    <published>2022-08-09T20:59:07.000Z</published>
    <updated>2024-02-12T13:30:22.746Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/BERT架构图.png" width="300" height="180" style="float:right;"><br><br><br>　　<strong>笔记</strong><br><br><br> </div><a id="more"></a><h1 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h1><p>2019年 谷歌 Jacob Devlin<br>NAACL-HLT会议-NLP顶会<br><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">PDF</a><br><a href="GitHub - google-research/bert: TensorFlow code and pre-trained models for BERT">CODE</a></p><h1 id="摘要重点"><a href="#摘要重点" class="headerlink" title="摘要重点"></a>摘要重点</h1><p>　　BERT(Bidirectional Encoder Rpresentation from Transformers)<br>　　BERT 旨在通过联合调节所有层的左右上下文，从未标记的文本中预训练深度双向表示。因此，预训练的 BERT 模型可以通过一个额外的输出层进行微调，从而为各种任务（例如问答和语言推理）创建最先进的模型，而无需大量特定任务架构修改。</p><h1 id="问题提出"><a href="#问题提出" class="headerlink" title="问题提出"></a>问题提出</h1><p>　　在下游任务中有两种方法使用预训练语言表示模型，一种是基于特征（feature-based）方法，一种是微调（fine-tuning）。本文认为现有技术限制了预训练表示的能力，尤其是微调方法。主要限制是标准语言模型是单向的，这限制了预训练可以使用的架构。例如，在 OpenAI GPT 中，作者使用从左到右的架构，其中每个标记只能关注Transformer的自注意力层中的先前标记。 这样的限制对于句子级任务来说是次优的，并且在将基于微调的方法应用于令牌级任务（例如问答）时可能非常有害，在这些任务中，从两个方向整合上下文至关重要。</p><h1 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h1><ul><li>我们解释了语言表示中双向预训练的重要性。不像之前的模型在预训练中使用双向语言模型，BERT使用掩码语言模型（masked language model）来预训练深度双向表示。也和以前使用从左到右和从右到左LM独立训练再浅层连结的方法不同。</li><li>预训练表示减少了许多特定任务的繁重工程架构。BERT是第一个基于表示模型微调并在一系列句子级别和token级别任务中实现SOTA性能的。</li><li>BERT在11个NLP任务中实现SOTA性能，代码开源。</li></ul><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>基于特征的无监督方法（如ELMo）、微调无监督方法（如OpenAI GPT）</p><h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><p>　　两阶段：预训练+微调。<br>　　预训练阶段，模型在不同的预训练任务中使用未标注数据训练。<br>　　微调阶段，首先使用预训练参数初始化，所有的参数使用下游任务中的标注数据进行微调。即使每一个下游任务使用相同的预训练参数初始化，它们还是有单独的微调模型。如图为一个问答示例。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/BERT架构图.png" alt="BERT" title>                </div>                <div class="image-caption">BERT</div>            </figure><br>　　记层数（Transformer块）为L，隐藏层为H，自注意力头数量为A。实验有两种模型规模：一种是$BERT_{BASE}$（L=12，H=768，A=12，总参数为110M）；另一种是$BERT_{LARGE}$（L=24，H=1024，A=16，总参数为340M）。<br>　　为压缩目的，$BERT_{BASE}$选择了和Open AI一样的模型规模。但是BERT Transformer使用了双向自注意力，GPT Transformer使用的是受限的自注意力，它的每个token只能获取它左边的上下文。<br>　　每一个token都以[CLS]开头。句子对会一起打包到一个序列中，分割句子使用两种方式。一是使用token[SEP]，二是在每个token中加入一个学习过的embedding表示它属于句子A还是句子B。如图1所示，输入embedding记为E，最后隐藏向量的[CLS]记为C，第i个输入token的最后的隐藏向量记为$T_i$。<br>对于给定的标记，其输入表示是通过对相应的标记、段和位置嵌入求和来构建的。这种结构的可视化可以在图2中看到。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/BERT输入表示.png" alt="BERT输入表示" title>                </div>                <div class="image-caption">BERT输入表示</div>            </figure></p><h2 id="预训练BERT"><a href="#预训练BERT" class="headerlink" title="预训练BERT"></a>预训练BERT</h2><h3 id="任务1：Masked-LM"><a href="#任务1：Masked-LM" class="headerlink" title="任务1：Masked LM"></a>任务1：Masked LM</h3><p>　　随机掩盖掉部分输入的tokens，然后预测这些被掩盖的tokens。这个过程记为“masked LM”(MLM)，类似于文学里的完形填空。与掩码标记对应的最终隐藏向量被送到词汇表上的softmax输出，就像在标准 LM 中一样。本文选择15%的tokens进行掩盖。由于[MASK] token在微调阶段不存在，所以预训练阶段和微调阶段不匹配。为了减轻影响，当选中第i个token时，按三条规则进行掩码：<br>（1） 80%时间使用[MASK] token<br>（2）10%时间选择随机token<br>（3）10%时间token不变<br>然后，使用交叉熵损失和$T_i$预测原始token。</p><h3 id="任务2：下一个句子预测（NSP）"><a href="#任务2：下一个句子预测（NSP）" class="headerlink" title="任务2：下一个句子预测（NSP）"></a>任务2：下一个句子预测（NSP）</h3><p>　　问答QA和自然语言推理（NLI）都是基于对两个句子关系的理解做的，而语言模型不能直接捕捉它。为了训练一个能理解句子关系的模型，我们预先训练一个二值化的下一个句子预测任务，该任务可以从任何单语语料库中轻松生成。在之前的工作中，只有句子嵌入被转移到下游任务， BERT 转移所有参数来初始化最终任务模型参数。</p><h2 id="微调BERT"><a href="#微调BERT" class="headerlink" title="微调BERT"></a>微调BERT</h2><p>　　微调很简单，因为Transformer中的自注意力机制允许 BERT 通过交换适当的输入和输出来对许多下游任务进行建模——无论它们涉及单个文本还是文本对。对于涉及文本对的应用程序，一种常见的模式是在应用双向交叉注意力之前独立编码文本对。BERT使用自我注意力机制来统一这两个阶段，因为使用自我注意对连接的文本对进行编码有效地包括了两个句子之间的双向交叉注意力。<br>　　对于每个任务，我们只需将任务特定的输入和输出插入BERT，端到端微调所有参数。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/BERT架构图.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;笔记&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="BERT" scheme="https://hubojing.github.io/tags/BERT/"/>
    
      <category term="NLP" scheme="https://hubojing.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>samba映射</title>
    <link href="https://hubojing.github.io/2022/07/31/samba%E6%98%A0%E5%B0%84/"/>
    <id>https://hubojing.github.io/2022/07/31/samba映射/</id>
    <published>2022-07-31T18:02:47.000Z</published>
    <updated>2024-02-12T13:30:22.750Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>更方便地使用服务器。</strong><br><br><br> </div><a id="more"></a><h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>　　将服务器资源可通过windows进行管理，很方便。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>　　在ubuntu服务器上<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get install samba</span><br><span class="line">samba --version</span><br></pre></td></tr></table></figure></p><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/samba/smb.conf</span><br></pre></td></tr></table></figure><p>　　在文件末尾追加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">security = share</span><br><span class="line"></span><br><span class="line">[share]</span><br><span class="line">path = /home/想映射的文件夹/</span><br><span class="line">available = yes</span><br><span class="line">browsealbe = yes</span><br><span class="line">public = yes</span><br><span class="line">guest ok = yes</span><br><span class="line">writable = yes</span><br><span class="line">create mask = 0664</span><br><span class="line">directory mask = 0664</span><br><span class="line">force user =root</span><br><span class="line">valid users = 用户名</span><br></pre></td></tr></table></figure></p><p>　　配置samba密码：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /etc/samba/smbpasswd</span><br><span class="line">sudo smbpasswd -a 用户名</span><br></pre></td></tr></table></figure></p><p>　　之后输入密码。<br>　　配置samba服务：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/samba restart</span><br><span class="line">sudo service smbd restart</span><br></pre></td></tr></table></figure></p><h1 id="windows映射操作"><a href="#windows映射操作" class="headerlink" title="windows映射操作"></a>windows映射操作</h1><p>　　此电脑-计算机-映射网络驱动器<br>　　填写”\服务器ip\文件夹名称”，勾选使用其他凭据连接，输入用户名和密码。</p><p>　　完成。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;更方便地使用服务器。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="互联网" scheme="https://hubojing.github.io/categories/%E4%BA%92%E8%81%94%E7%BD%91/"/>
    
    
      <category term="samba" scheme="https://hubojing.github.io/tags/samba/"/>
    
  </entry>
  
  <entry>
    <title>JingSLink</title>
    <link href="https://hubojing.github.io/2022/07/03/JingSLink/"/>
    <id>https://hubojing.github.io/2022/07/03/JingSLink/</id>
    <published>2022-07-03T21:26:39.000Z</published>
    <updated>2024-02-12T13:30:22.746Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br>　　<strong>简单开发一个截图保存并返回链接的小工具JingSLink</strong><br>　　<strong>JingPic迎来重构版JingSLink</strong><br>　　<strong>自用简陋小工具系列</strong><br><br><br> </div><a id="more"></a><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>　　写markdown文章时，贴图这件事儿真的很麻烦。我需要把图片手动保存到对应文件夹，再手动在文章里引入图片链接。所以很多时候我能不配图就不配图。多年前为了方便插入图床链接，用MFC做了个自用小工具<a href="https://github.com/hubojing/JingPic" target="_blank" rel="noopener">JingPic</a>。但是它是基于保存了的图片进行的操作，将图片转移到需要的位置。后期也没有维护，操作上依然不够简洁。比如我更喜欢直接截图能自动保存到指定位置，这样就减少一步自己保存的操作。</p><p>　　自己定制还是最爽的，我想要什么功能我自己开发好了。而且这个小工具开发起来也很简单，几十行很快就写完了。</p><p>　　首先，要起个名字，以前的叫JingPic，这个重构版就叫JingSLink（Jinger Screenshot for Link）吧。</p><h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><ul><li>能够截图</li><li>截图能保存到指定文件夹中</li><li>截图完成后，自动将图片相对路径复制到剪贴板</li><li>有exe可执行文件</li></ul><h1 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h1><p>　　语言：Python<br>　　用到的库：tkinter, PIL, keyboard, pyperclip</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>　　一开始我是想自己实现截图操作的，后来一想，我平日用QQ截图最多，它功能齐全，那我直接调用它的截图不就好了。<br>　　所以，大体实现思路如下：</p><ul><li>监听键盘，捕捉QQ截图快捷键ctrl+alt+a，并设置一个结束键ctrl。</li><li>使用PIL库的ImageGrab读取截图。</li><li>弹出一个输入框，输入图片名称。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/JingSLink.png" alt="输入框" title>                </div>                <div class="image-caption">输入框</div>            </figure></li><li>拼凑出图片完整链接，将图片保存到该地址。</li><li>拼凑出hexo博文中所需的图片插入相对地址，复制到剪贴板。<br>　　当然，为了不每次都打开pycharm编译器，得导出一个exe文件。</li></ul><p>　　打包exe：<br>　　安装pyinstaller。<br>　　在pycharm中，<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller -F main.py</span><br><span class="line">pyinstaller -F -w main.py<span class="comment">#（-w是取消dos窗口）</span></span><br><span class="line">pyinstaller -F -w --icon=ico main.py</span><br><span class="line"><span class="comment"># (ico为图标的文件名，与dist目录为同目录)</span></span><br></pre></td></tr></table></figure></p><p>　　最后生成的exe文件在dist文件夹下。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/hubojing/JingSLink" target="_blank" rel="noopener">https://github.com/hubojing/JingSLink</a></p><h1 id="后期优化"><a href="#后期优化" class="headerlink" title="后期优化"></a>后期优化</h1><p>　　如有必要：</p><ul><li>一些异常情况的提示</li><li>右下角有最小化托盘，右键有设置和退出菜单</li><li>可扩展性：可以更改文件夹路径</li><li>可扩展性：可以更改图片路径格式（绝对路径/相对路径）</li><li>换pyqt框架</li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="http://tkdocs.com/tutorial/index.html" target="_blank" rel="noopener">http://tkdocs.com/tutorial/index.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;简单开发一个截图保存并返回链接的小工具JingSLink&lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;JingPic迎来重构版JingSLink&lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;自用简陋小工具系列&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="软件开发" scheme="https://hubojing.github.io/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="截图" scheme="https://hubojing.github.io/tags/%E6%88%AA%E5%9B%BE/"/>
    
      <category term="链接" scheme="https://hubojing.github.io/tags/%E9%93%BE%E6%8E%A5/"/>
    
      <category term="markdown" scheme="https://hubojing.github.io/tags/markdown/"/>
    
      <category term="软件开发" scheme="https://hubojing.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>TEMN</title>
    <link href="https://hubojing.github.io/2022/07/03/Paper_TEMN/"/>
    <id>https://hubojing.github.io/2022/07/03/Paper_TEMN/</id>
    <published>2022-07-03T13:03:28.000Z</published>
    <updated>2024-02-12T13:30:22.750Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/TEMN架构图.png" width="300" height="180" style="float:right;"><br><br>　　<strong>陆续上架前几个月写的库存</strong><br>　　<strong>主题增强记忆网络个性化兴趣点推荐</strong><br><br><br> </div><a id="more"></a><h1 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h1><p>　　Topic-Enhanced Memory Networks for Personalised Point-of-Interest Recommendation<br>　　主题增强记忆网络个性化兴趣点推荐<br>　　KDD 2019<br><a href="https://arxiv.org/pdf/1905.13127.pdf" target="_blank" rel="noopener">PDF</a><br><a href="https://github.com/XiaoZHOUCAM/TEMN" target="_blank" rel="noopener">CODE</a></p><p>　　关键词：推荐系统；神经网络；主题建模</p><h1 id="问题提出"><a href="#问题提出" class="headerlink" title="问题提出"></a>问题提出</h1><p>　　现有问题：数据稀疏；现有算法使用一个单一向量刻画用户偏好限制了表达和可解释性。</p><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p>　　Topic-Enhanced Memory Network (TEMN)<br>　　TEMN是一个统一的混合模型，利用TLDA和外部记忆网络以及神经注意机制来捕捉用户的全局和细粒度偏好。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/TEMN架构图.png" alt="TEMN" title>                </div>                <div class="image-caption">TEMN</div>            </figure><br>　　三部分组成：记忆网络，TLDA和地理建模部分。<br>　　前两部分相互联系，用于建模从基于领域的记忆网络中学到的非线性交互（通过历史记录）以及从主题模型中学到的全局偏好。</p><p>　　每一部分分别对应不同的损失函数，进行联合训练。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>　　微信朋友圈签到数据集（未开源）<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/TEMN数据集.png" alt="数据集" title>                </div>                <div class="image-caption">数据集</div>            </figure></p><h2 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h2><ul><li>MF</li><li>BPR</li><li>LDA</li><li>CML</li><li>LRML</li><li>TEMN(GPR) 保留了记忆模块，将TLDA替换为LDA，去掉了地理模块。</li><li>LORE</li><li>ST-RNN</li><li>TEMN(SPR) 完整TEMN模型使用微信（SPR）数据</li><li>GeoMF</li><li>TLDA</li><li>TEMN(CPR) 完整TEMN模型使用微信（GPR）数据<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/TEMN性能.png" alt="性能" title>                </div>                <div class="image-caption">性能</div>            </figure></li></ul><h1 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h1><ul><li>提出一种融合基于领域和全局的用户偏好的端到端深度学习框架。</li><li>在兴趣点推荐中设计了能融合多种上下文信息的灵活架构，并使之能在多种推荐场景应用。</li><li>提出一种结合监督和非监督学习的混合模型，并利用了记忆网络和主题模型。通过相互学习机制，模型还能得出用户在受记忆网络影响的主题上的概率分布。</li><li>在微信数据集上进行模型验证，超过基线模型。</li><li>通过在TEMN中引入神经注意机制和主题模型，POI推荐的可解释性得到了显著提高。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/TEMN架构图.png&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;陆续上架前几个月写的库存&lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;主题增强记忆网络个性化兴趣点推荐&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="论文" scheme="https://hubojing.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="推荐算法" scheme="https://hubojing.github.io/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="兴趣点推荐" scheme="https://hubojing.github.io/tags/%E5%85%B4%E8%B6%A3%E7%82%B9%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>Office三件套笔记</title>
    <link href="https://hubojing.github.io/2022/07/02/office%E4%B8%89%E4%BB%B6%E5%A5%97%E7%AC%94%E8%AE%B0/"/>
    <id>https://hubojing.github.io/2022/07/02/office三件套笔记/</id>
    <published>2022-07-02T09:42:53.000Z</published>
    <updated>2022-07-03T22:53:23.000Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br>　　<strong>陆续上架前几个月写的库存</strong><br><br>　　<strong>没想到有一天还会写这篇笔记</strong><br>　　<strong>记录一些稍高阶的玩法</strong><br>　　<strong>写论文后遗症</strong><br> </div><a id="more"></a><h1 id="Word"><a href="#Word" class="headerlink" title="Word"></a>Word</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>样式修改：<br>引用-目录-自定义目录-修改（选项旁边），为不同级别目录进行格式自定义。</p><h2 id="域代码"><a href="#域代码" class="headerlink" title="域代码"></a>域代码</h2><p>快捷键： Alt+F9。</p><p>别在百度里搜陈年包浆老帖了，官方文档不香吗<br><a href="https://support.microsoft.com/zh-cn/office/word-%E4%B8%AD%E7%9A%84%E5%9F%9F%E4%BB%A3%E7%A0%81%E5%88%97%E8%A1%A8-1ad6d91a-55a7-4a8d-b535-cf7888659a51" target="_blank" rel="noopener">官方文档</a></p><h3 id="图注"><a href="#图注" class="headerlink" title="图注"></a>图注</h3><p>Seq（序列）域<br>图注序号标注 域代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">图&#123;SEQ 图 \r 1&#125;</span><br></pre></td></tr></table></figure></p><p>即 “图1”</p><h2 id="交叉引用"><a href="#交叉引用" class="headerlink" title="交叉引用"></a>交叉引用</h2><p>交叉引用后要添加或者删除怎么更新：</p><ol><li>在要插入的前一行最后的<strong>换行符前</strong>回车，添加文献</li><li>ctrl+A，右键“更新域”。</li></ol><p>多引用（形如格式[2, 5]）<br>原域代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;REF_Ref56957053 \r \h  *MERGRFORMAT &#125;&#123;REF_Ref56957053 \r \h *MERGRFORMAT &#125;</span><br></pre></td></tr></table></figure></p><p>修改域代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;REF_Ref56957053 \r \h  \#&quot;[0&quot;  \*MERGRFORMAT &#125;&#123;REF_Ref56957053 \r \h  \#&quot;0]&quot;  \*MERGRFORMAT &#125;</span><br></pre></td></tr></table></figure></p><p>手动加中间的逗号。</p><h1 id="Excel"><a href="#Excel" class="headerlink" title="Excel"></a>Excel</h1><ul><li>excel 如何合并相同项并累加<br><a href="https://zhidao.baidu.com/question/75402048.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/75402048.html</a></li><li>如何把2个excel的数据表连起来<br><a href="https://zhidao.baidu.com/question/577259682.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/577259682.html</a></li></ul><h1 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h1><p>待续..</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;陆续上架前几个月写的库存&lt;/strong&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;没想到有一天还会写这篇笔记&lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;记录一些稍高阶的玩法&lt;/strong&gt;&lt;br&gt;　　&lt;strong&gt;写论文后遗症&lt;/strong&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
    
      <category term="office" scheme="https://hubojing.github.io/tags/office/"/>
    
      <category term="word" scheme="https://hubojing.github.io/tags/word/"/>
    
      <category term="excel" scheme="https://hubojing.github.io/tags/excel/"/>
    
      <category term="ppt" scheme="https://hubojing.github.io/tags/ppt/"/>
    
  </entry>
  
  <entry>
    <title>毕业</title>
    <link href="https://hubojing.github.io/2022/06/29/%E6%AF%95%E4%B8%9A/"/>
    <id>https://hubojing.github.io/2022/06/29/毕业/</id>
    <published>2022-06-29T10:29:24.000Z</published>
    <updated>2022-08-15T00:23:53.000Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="/images/假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>一篇六月底就写好、八月中才发布的毕业总结。</strong><br><br><br> </div><a id="more"></a><p>　　技术博客只谈学习和技术。每次我在这里写这种总结文的时候，必须时刻提醒自己这句话，否则我的思路一旦打开，就不知道飞到哪里去了。</p><p>　　对比本科结束时写的<a href="https://hubojing.github.io/2017/03/06/%E5%8F%8D%E7%9C%81/">总结</a>，首先得给自己这三年打个分。<br>　　<br>　　从学习的角度来说，给个65分吧。(其它方面可能要远高于本科时期)</p><p>　　相比于本科时期自己200%投入到学习中的那种专注和疯狂，读研期间的我看起来要“正常”许多。那时候需要用忙碌和疲惫来使自己放松（这句话也许听起来十分奇怪），而读研后的我尝试着逐步和自己和解了。</p><h1 id="最大收获"><a href="#最大收获" class="headerlink" title="最大收获"></a>最大收获</h1><p>　　如果说本科期间我的最大收获是学会了自学的话，读研期间的最大收获有二：学会了读书、科研入了门。<br>　　本科时，每个学期都会给自己强行列个读书方向，比如这学期重点是推理方面、日本作家系列，下学期重点是历史方面、国内作家系列，再下个学期看点俄国文学、美国文学之类的。书是看了一些，但是往往得到最多的是完成任务的快感，并没有真正得到读书的快乐。<br>　　读研时，再没有给自己布置强制性的读书任务。而是在微信读书上凭着兴趣，任意读之。慢慢地，我竟喜欢上了这种读书的感觉，只要是在空隙时间，我都会打开看看书。就这样，我读完了一本又一本书，读完书后的感受各不相同，也许会受主人公的命运而牵绊，也许会通过书籍体验了不同的人生而倍感快乐。我也遇到了一本改变我三观的重要书籍：三体。<br>　　读三体完全是因为看新闻说这本书要被改编成影视了，为了防止影视先入为主的剧情，我决定在影视剧开播前把原著给看完了。三体的三大本着实很厚，但是30个小时读下来也一点不亏。从大的角度上，我对整个人类的观点产生了巨大变化，从个人方面，也改变了我对生活的态度。<br>　　罗马假日里说，要么读书，要么旅行，身体和心灵总有一个要在路上。(You can either travel or read, but either your body or soul must be on the way.)<br>　　我比较宅，对旅游无感，所幸还可以通过读书来认识世界。三年读了几十本书，虽然不多，但是每本都留下了一些笔记和感悟，很多书里的语句也在影响和改变我的行为和看法。当然也有不少技术书，大都是电子版（省钱哈哈哈），希望自己对技术的追求永不停止吧。</p><p>　　学术方面，现在回想起来本科毕业时自己的科研底子很薄弱，是一个科研小白，和现在的学弟学妹们的基础比不了。本科时的学习基本上也都是自己的单打独斗，总体上是一段孤独的旅程。读研给了我一个接触学术和前沿的机会，而三年后的我能够在科研方面入了门，离不开我导的指引、组内的讨论、实习实践和网上大神们的知识共享。</p><h1 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h1><p>　　（更新插一句）这篇文章其它部分六月底就写好了，唯独技术这一节我总觉得写地短了浅了点，拖到如今才完善这一段。<br>　　从通信转到软工，我终于算个科班出身啦哈哈哈。为了缩小和大家的差距，三年里蹭了些课、看了些书、刷了些题，但总归还是觉得自己水平不咋地。但是一想到自己未来还有一辈子可以去学这些东西就令人神往。<br>　　本科关注开发技术，读研关注算法技术。读研之前，在方向的求索问题上，我走了不少弯路，进行过很多简单的尝试。那段时间我依然存在着新手都有的困惑，纠结于编程语言的选择。虽然干着C++开发的活，但因为市场上C++求职岗位比Java岗位数量少，而担忧起C++的未来发展。现在回头来想，还是杞人忧天了些，C++随着AI浪潮的崛起，反而迎来了新的上升期。而我现在也不再纠结于编码语言本身，摆正了对它的看法，终于明白了“编程语言只是工具”这句话的含义。读研之后，由于学术培养，我开始了算法研究之路。科研初期也是跌跌撞撞的，由于比很多同学起步晚，只有自己多看多学才能弥补差距。一开始阅读英文文献也很不习惯很慢，但好在考研期间坚持了几年的英语电台每日一句录制，英语的基础还没有荒废，随着习惯逐渐养成，基本摆脱了中文翻译的操作（其实主要是懒得打开翻译器……所以懒有时候也有帮助哈哈哈哈）。在方向的选择上，研一上学期在看了不多的文献后，大致了解了数据挖掘的基础情况，结合我们组内的具体科研状况、对当时就业市场的观察（CV卷得惨绝人寰、NLP落地尚不明朗、搜广推相对靠近业务且竞争小于前两者，当然，2022年的搜广推已经不是这样了！）以及个人兴趣推动（非常好奇网易云私人FM推荐是如何做的），我鼓足勇气和导师提出了推荐算法方向，没想到的是，我导当场就同意了。结合我们组的特点，我做的是兴趣点推荐算法。想想读研以前，我深感自己没有一个特定的领域导致无法深耕，而现在，我终于拥有了一个自己的研究方向，终于可以在知乎上写上“xxx方向”了哈哈哈。<br>　　兴趣点推荐方向本身，隶属于推荐算法大家庭，所以，为了更好地学习推荐算法，我也不断学习机器学习（包括深度学习）的基础知识，积极了解前沿学术和工业界动态。在这里还有个小插曲，当年入学前，有好几个已经读研的同学们纷纷给我提醒：慎入深度学习深坑！（后来我知道了他们都是搞CV的，求职的时候被卷死……后来怒转开发）但是在我心中已经有了这种印象——远离深度学习，否则会变得不幸，哈哈哈哈哈。幸好我当时拉跨的AI基础让我觉得推荐算法和深度学习这个词看起来很远，所以选了推荐算法时还在沾沾自喜我应该没有入坑吧？属实是TOO YOUNG TOO NAIVE，直到后来阅读每一篇顶会论文都是深度学习时，我才明白天下就是深度学习的天下啊……其实现在的我回看这个观点，我不会认为对，但也并没有全错。深度学习本身只是一种方法，它只有结合到某种场景、某个业务或者某个领域才会产生价值。<br>　　推荐算法目前来说依然是个不错的方向，至少现在它是一个能够落地、有实际应用的方向。但随着互联网增速放缓进入存量时代，搜广推是否仍和之前一样蕴含巨大价值也许值得讨论。在我秋招的那段日子里，我看到过一句话：“这个世界上最聪明的一些头脑，都在哄骗他人去点击广告。（大意如此）”那天这句话对我还是有不小的触动，也许我们做的这些算法是否已经足够甚至过量了呢？信息茧房的危害正在蚕食互联网中的我们，同质化的算法打着个性化的招牌过分挖掘用户隐私的同时，也使人们沉迷在算法里，渐渐抹杀掉很多的创造力。比如网易云，当初是因为喜欢它去研究推荐算法，可是有一天开始，它的私人FM让我觉得不准了，远远不如18年那会儿了，但去搜它官方公开的技术模型，反而迭代地更加复杂，更加高深了。这件事让我常常想，我们的推荐算法会不会早就“过拟合”了？不过，技术本身是不应该附带偏见的，对推荐算法本身我始终以纯粹的技术观点去分析和学习。<br>　　秋招拿的一些offer大都是推荐算法方面的，但是最后我选择了NLP算法。除了一些现实因素的考虑（离家较近）外，选择NLP也是选择了更多的挑战，而我比较享受未知。当然，和推荐算法关联最大的就是NLP算法，两者关联紧密，而NLP算法的应用场景会更大更多，未来总归是NLP的天下（虽然现在是CV的）。所以，打好NLP算法的基础十分重要。我总觉得机器人时代离我们不远了。认知智能在我们这一代一定会实现。另一方面，我选择的领域，让我感觉很有社会价值和意义，这和我建立博客以来就写在副标题上的“为中华之崛起而读书”似乎又贴近了一步。<br>　　即将步入工作岗位，希望在工作中能更好地打磨自己的技术水平，提高编码效率。</p><h1 id="自律"><a href="#自律" class="headerlink" title="自律"></a>自律</h1><p>　　关于自律这件事，远远比不上当年了。即使挂着“自律使人自由”的标语催促着自己上进，也无法再像原来一样。同样都是我，现在的我卷不过以前的我（哈哈哈哈人老了）。我为此不断地想这到底是为什么。是我现在的学习态度不够端正了吗？应该不是。是我变得更加懒惰了吗？好像也不是。后来我可能知道了答案：自律只是一种实现目标的手段，当缺乏必须高度自律才能做到的目标时，自律本身失去了意义。这可能也和我的学习方法改变有关。</p><p>　　三年里，我尝试了一些新的学习方法。以前我按照要趁早的学习模式，干什么都会在一开始就行动。但是久而久之，我不仅有了些强迫症（比如会反复检查作业、格式、要求之类），这无意义地耗费了大量宝贵的时间，心情也不悦（明知没必要但依然会反复检查），而且由于完成的时间很充沛，慢慢地形成了拖延症，而我在很长的时间里将这归结于我是一个慢性子。所以，我开始尝试要求自己不要一开始就行动，而是要在最短的时间内以最高的效率去学习（俗称DDL学习法，滑稽.jpg）。我知道这样是有风险的，毕竟学渣基本上都是DDL学习法的忠实拥护者。但是幸好，我带着意识地去主动尝试，目的是追求高效率，而不是拖到最后不得不做的被动，两者的自我驱动是不一样的。比如，以前上学七点一刻到校，我总是六点就被闹钟叫醒。可是同学们大都六点半才起。这是我第一次意识到自己效率低下或者说为了保证不迟到过于苛刻了。而这损失的半小时睡眠可能使白天的工作效率更低了。读研时，我经常要求自己不要在规定时间提前太多，我要看看我是否拥有这种弹性。事实证明，这些提前的时间就是没必要的，在现有时间内，我也能有条不紊地完成计划。DDL很好地克制住了我反复检查的毛病，而这多出来的时间，应该去做其它更有意义的事情。当然，这个度是难以把握的，一不小心就堕入学渣之境。我现在更加认可两种方法的结合，根据任务的紧急程度和优先级来合理使用这些方法。<br>　　自律应该是自然而然的，当心中有执念时，会主动地做到。如果自律成为一种负担，也许已经是本末倒置了，倒不妨想想自己到底要干什么。</p><p>　　（更新插一句）比如，明天又是周一了，为了早上精力充沛地去上班，我现在就关掉电脑去睡觉了！上班让我作息自律了哈哈哈哈哈哈。</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;一篇六月底就写好、八月中才发布的毕业总结。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="杂谈" scheme="https://hubojing.github.io/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="毕业" scheme="https://hubojing.github.io/tags/%E6%AF%95%E4%B8%9A/"/>
    
      <category term="总结" scheme="https://hubojing.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>《推荐系统：原理与实践》笔记</title>
    <link href="https://hubojing.github.io/2022/06/15/%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%9A%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    <id>https://hubojing.github.io/2022/06/15/《推荐系统：原理与实践》笔记/</id>
    <published>2022-06-15T10:41:55.000Z</published>
    <updated>2024-02-12T13:30:22.754Z</updated>
    
    <content type="html"><![CDATA[<div align="left"><br><img src="\images\假装有图片.jpg" width="300" height="180" style="float:right;"><br><br><br>　　<strong>砖头书笔记（自用）</strong><br><br><br> </div><a id="more"></a><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>　　有几本砖头书在图书馆里我不断续借，网上又没有PDF，现在要毕业了，只有勉强把它看完了……</p><h1 id="高级论问题和应用"><a href="#高级论问题和应用" class="headerlink" title="高级论问题和应用"></a>高级论问题和应用</h1><ul><li>推荐系统中的冷启动问题<br>太常见不说了。</li><li>抗攻击推荐系统<br>只要指恶意评论。</li><li>组推荐系统<br>针对一组用户推荐，而不是单一用户。</li><li>多标准推荐系统<br>如，用户可以给予情节、音乐、特效等对电影进行评分。在多标准推荐系统中，用户可能根本没有给出整体评分。</li><li>推荐系统中的主动学习<br>鼓励用户输入评分以完善系统的机制。例如，用户可能会为某些物品评分获得奖励。因此，必须明智地选择由特定用户进行评分的物品。如，某用户已评价大量动作片，那么要求该用户去评价另一部动作电影对预测其他的动作电影评分帮助不大，并且对预测属于无关种类的电影评分的帮助甚至更少。另一方面，要求用户评价不太热门种类的电影将对预测这种类型的电影评分有显著帮助。当然，如果用户被要求评价无关的电影，他不一定能够提供反馈，因为他可能根本没有看过那部电影。<strong>（此处举例我存疑）</strong>因此，在推荐系统的主动学习问题中有许多在其他问题领域（如分类问题）没有遇到的有趣权衡问题。</li><li>推荐系统中的隐私问题</li><li>保护隐私的推荐算法。</li><li>应用领域</li></ul><h1 id="推荐系统评估"><a href="#推荐系统评估" class="headerlink" title="推荐系统评估"></a>推荐系统评估</h1><h2 id="评估设计的总体目标"><a href="#评估设计的总体目标" class="headerlink" title="评估设计的总体目标"></a>评估设计的总体目标</h2><ul><li>精确性</li><li>覆盖率</li><li>置信度和信任度</li><li>新颖度</li><li>惊喜度</li><li>多样性</li><li>健壮性和稳定性</li><li>可扩展性<h2 id="离线评估的精确性指标"><a href="#离线评估的精确性指标" class="headerlink" title="离线评估的精确性指标"></a>离线评估的精确性指标</h2><h3 id="独立预测评分的精确性"><a href="#独立预测评分的精确性" class="headerlink" title="独立预测评分的精确性"></a>独立预测评分的精确性</h3>　　RMSE, MAE<br>　　RMSE计算时用的是误差的平方，所以它更加显著地被大的误差值或者异常值所影响。一些被预测失败的评分会显著地破坏RMSE方法。在各种评分的预测健壮性非常重要的应用中，RMSE可能是一个更加合适的方法。另一方面，当评估的异常值有限时，MAE能更好地反映精确性。RMSE主要的问题是它不是平均误差的真实反映，而且它又是会导致有误导的结果。<h3 id="通过相关性评估排名"><a href="#通过相关性评估排名" class="headerlink" title="通过相关性评估排名"></a>通过相关性评估排名</h3></li><li>Spearman等级相关系数</li><li>肯德尔等级相关系数<h3 id="通过效用评估排名"><a href="#通过效用评估排名" class="headerlink" title="通过效用评估排名"></a>通过效用评估排名</h3>　　基于效用方法的总体目标就是给出用户可能找到推荐系统排名的有用程度的简单量化。这种方法下隐含的一个重要准则就是相对于物品的总量而言，推荐列表是简短的。因此一个具体评分的效用大部分情况下应该基于在推荐列表中相关性高的物品。这种情况下，RMSE指标有一个缺点，因为它对低排名物品和那些高排名物品赋予了同样的权重。</li></ul><p>　　NDCG, ARHR（平均逆命中率）<br>　　ARHR也被称作是平均倒数排名（MRR）</p><h3 id="通过ROC曲线评估排名"><a href="#通过ROC曲线评估排名" class="headerlink" title="通过ROC曲线评估排名"></a>通过ROC曲线评估排名</h3><h1 id="抵抗攻击的推荐系统"><a href="#抵抗攻击的推荐系统" class="headerlink" title="抵抗攻击的推荐系统"></a>抵抗攻击的推荐系统</h1><h2 id="攻击类型"><a href="#攻击类型" class="headerlink" title="攻击类型"></a>攻击类型</h2><ul><li>随机攻击</li><li>均值攻击</li><li>bandwagon攻击</li><li>流行攻击</li><li>爱/憎攻击</li><li>反向bandwagon攻击</li><li>探测攻击</li><li>分段攻击<h2 id="健壮推荐设计策略"><a href="#健壮推荐设计策略" class="headerlink" title="健壮推荐设计策略"></a>健壮推荐设计策略</h2></li><li>用CAPTCHA防止自动攻击</li><li>使用社会信任</li><li>设计健壮的推荐算法</li></ul><h1 id="排名学习"><a href="#排名学习" class="headerlink" title="排名学习"></a>排名学习</h1><p>　　pointwise<br>　　pairwise: BPR, Eigen Rank, pLPA, CR<br>　　listwise: NDCG, MRR</p><h1 id="多臂赌博机算法"><a href="#多臂赌博机算法" class="headerlink" title="多臂赌博机算法"></a>多臂赌博机算法</h1><h1 id="组推荐系统"><a href="#组推荐系统" class="headerlink" title="组推荐系统"></a>组推荐系统</h1><ul><li>协同和基于内容的系统</li><li>基于知识的系统<h1 id="多标准推荐系统"><a href="#多标准推荐系统" class="headerlink" title="多标准推荐系统"></a>多标准推荐系统</h1></li><li>基于近邻的方法</li><li>基于集成的方法</li><li>无整体评分的多标准系统<h1 id="推荐系统中的主动学习"><a href="#推荐系统中的主动学习" class="headerlink" title="推荐系统中的主动学习"></a>推荐系统中的主动学习</h1></li><li>基于异质性的模型</li><li>基于性能的模型<h1 id="推荐系统中的隐私"><a href="#推荐系统中的隐私" class="headerlink" title="推荐系统中的隐私"></a>推荐系统中的隐私</h1></li><li>基于冷凝的隐私</li><li>高维数据的挑战</li></ul><h1 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h1><ul><li>门户内容个性化</li><li>计算广告与推荐系统</li><li>互惠推荐系统<br>　　基本思想是当考虑多个具有不对称兴趣的利益相关人的推荐的效用时，推荐的任务会发生改变。如在线约会的互惠推荐系统。</li></ul><ol><li>用户意识到交易的成功取决于另一方的许可。另一方是互惠环境中的“物品”。</li><li>用户和物品在系统中可能只出现一次，在一次成功的事物后它们可能永远不会重现。冷启动问题在互惠场景中更加显著。<br>　　方法：</li><li>利用混合方法<br>在这些方法中，两个传统的推荐方法被构造出来，分别对应着两个互惠方的喜好。然后，这两个互惠方的预测被组合起来。</li><li>利用链路预测方法<br>当冷启动问题不是很严重或者可以用来自类似用户和物品的数据来增加评分数据时，可以在系统中采用链路预测方法。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;img src=&quot;\images\假装有图片.jpg&quot; width=&quot;300&quot; height=&quot;180&quot; style=&quot;float:right;&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;　　&lt;strong&gt;砖头书笔记（自用）&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;/div&gt;
    
    </summary>
    
      <category term="推荐系统" scheme="https://hubojing.github.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐算法" scheme="https://hubojing.github.io/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
